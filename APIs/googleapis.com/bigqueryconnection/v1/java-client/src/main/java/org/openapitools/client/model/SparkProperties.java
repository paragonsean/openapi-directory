/*
 * BigQuery Connection API
 * Allows users to manage BigQuery connections to external data sources.
 *
 * The version of the OpenAPI document: v1
 * 
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */


package org.openapitools.client.model;

import java.util.Objects;
import com.google.gson.TypeAdapter;
import com.google.gson.annotations.JsonAdapter;
import com.google.gson.annotations.SerializedName;
import com.google.gson.stream.JsonReader;
import com.google.gson.stream.JsonWriter;
import java.io.IOException;
import java.util.Arrays;
import org.openapitools.client.model.MetastoreServiceConfig;
import org.openapitools.client.model.SparkHistoryServerConfig;

import com.google.gson.Gson;
import com.google.gson.GsonBuilder;
import com.google.gson.JsonArray;
import com.google.gson.JsonDeserializationContext;
import com.google.gson.JsonDeserializer;
import com.google.gson.JsonElement;
import com.google.gson.JsonObject;
import com.google.gson.JsonParseException;
import com.google.gson.TypeAdapterFactory;
import com.google.gson.reflect.TypeToken;
import com.google.gson.TypeAdapter;
import com.google.gson.stream.JsonReader;
import com.google.gson.stream.JsonWriter;
import java.io.IOException;

import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;

import org.openapitools.client.JSON;

/**
 * Container for connection properties to execute stored procedures for Apache Spark.
 */
@javax.annotation.Generated(value = "org.openapitools.codegen.languages.JavaClientCodegen", date = "2024-10-12T11:36:12.015205-04:00[America/New_York]", comments = "Generator version: 7.9.0")
public class SparkProperties {
  public static final String SERIALIZED_NAME_METASTORE_SERVICE_CONFIG = "metastoreServiceConfig";
  @SerializedName(SERIALIZED_NAME_METASTORE_SERVICE_CONFIG)
  private MetastoreServiceConfig metastoreServiceConfig;

  public static final String SERIALIZED_NAME_SERVICE_ACCOUNT_ID = "serviceAccountId";
  @SerializedName(SERIALIZED_NAME_SERVICE_ACCOUNT_ID)
  private String serviceAccountId;

  public static final String SERIALIZED_NAME_SPARK_HISTORY_SERVER_CONFIG = "sparkHistoryServerConfig";
  @SerializedName(SERIALIZED_NAME_SPARK_HISTORY_SERVER_CONFIG)
  private SparkHistoryServerConfig sparkHistoryServerConfig;

  public SparkProperties() {
  }

  public SparkProperties(
     String serviceAccountId
  ) {
    this();
    this.serviceAccountId = serviceAccountId;
  }

  public SparkProperties metastoreServiceConfig(MetastoreServiceConfig metastoreServiceConfig) {
    this.metastoreServiceConfig = metastoreServiceConfig;
    return this;
  }

  /**
   * Get metastoreServiceConfig
   * @return metastoreServiceConfig
   */
  @javax.annotation.Nullable
  public MetastoreServiceConfig getMetastoreServiceConfig() {
    return metastoreServiceConfig;
  }

  public void setMetastoreServiceConfig(MetastoreServiceConfig metastoreServiceConfig) {
    this.metastoreServiceConfig = metastoreServiceConfig;
  }


  /**
   * Output only. The account ID of the service created for the purpose of this connection. The service account does not have any permissions associated with it when it is created. After creation, customers delegate permissions to the service account. When the connection is used in the context of a stored procedure for Apache Spark in BigQuery, the service account is used to connect to the desired resources in Google Cloud. The account ID is in the form of: bqcx--@gcp-sa-bigquery-consp.iam.gserviceaccount.com
   * @return serviceAccountId
   */
  @javax.annotation.Nullable
  public String getServiceAccountId() {
    return serviceAccountId;
  }



  public SparkProperties sparkHistoryServerConfig(SparkHistoryServerConfig sparkHistoryServerConfig) {
    this.sparkHistoryServerConfig = sparkHistoryServerConfig;
    return this;
  }

  /**
   * Get sparkHistoryServerConfig
   * @return sparkHistoryServerConfig
   */
  @javax.annotation.Nullable
  public SparkHistoryServerConfig getSparkHistoryServerConfig() {
    return sparkHistoryServerConfig;
  }

  public void setSparkHistoryServerConfig(SparkHistoryServerConfig sparkHistoryServerConfig) {
    this.sparkHistoryServerConfig = sparkHistoryServerConfig;
  }



  @Override
  public boolean equals(Object o) {
    if (this == o) {
      return true;
    }
    if (o == null || getClass() != o.getClass()) {
      return false;
    }
    SparkProperties sparkProperties = (SparkProperties) o;
    return Objects.equals(this.metastoreServiceConfig, sparkProperties.metastoreServiceConfig) &&
        Objects.equals(this.serviceAccountId, sparkProperties.serviceAccountId) &&
        Objects.equals(this.sparkHistoryServerConfig, sparkProperties.sparkHistoryServerConfig);
  }

  @Override
  public int hashCode() {
    return Objects.hash(metastoreServiceConfig, serviceAccountId, sparkHistoryServerConfig);
  }

  @Override
  public String toString() {
    StringBuilder sb = new StringBuilder();
    sb.append("class SparkProperties {\n");
    sb.append("    metastoreServiceConfig: ").append(toIndentedString(metastoreServiceConfig)).append("\n");
    sb.append("    serviceAccountId: ").append(toIndentedString(serviceAccountId)).append("\n");
    sb.append("    sparkHistoryServerConfig: ").append(toIndentedString(sparkHistoryServerConfig)).append("\n");
    sb.append("}");
    return sb.toString();
  }

  /**
   * Convert the given object to string with each line indented by 4 spaces
   * (except the first line).
   */
  private String toIndentedString(Object o) {
    if (o == null) {
      return "null";
    }
    return o.toString().replace("\n", "\n    ");
  }


  public static HashSet<String> openapiFields;
  public static HashSet<String> openapiRequiredFields;

  static {
    // a set of all properties/fields (JSON key names)
    openapiFields = new HashSet<String>();
    openapiFields.add("metastoreServiceConfig");
    openapiFields.add("serviceAccountId");
    openapiFields.add("sparkHistoryServerConfig");

    // a set of required properties/fields (JSON key names)
    openapiRequiredFields = new HashSet<String>();
  }

  /**
   * Validates the JSON Element and throws an exception if issues found
   *
   * @param jsonElement JSON Element
   * @throws IOException if the JSON Element is invalid with respect to SparkProperties
   */
  public static void validateJsonElement(JsonElement jsonElement) throws IOException {
      if (jsonElement == null) {
        if (!SparkProperties.openapiRequiredFields.isEmpty()) { // has required fields but JSON element is null
          throw new IllegalArgumentException(String.format("The required field(s) %s in SparkProperties is not found in the empty JSON string", SparkProperties.openapiRequiredFields.toString()));
        }
      }

      Set<Map.Entry<String, JsonElement>> entries = jsonElement.getAsJsonObject().entrySet();
      // check to see if the JSON string contains additional fields
      for (Map.Entry<String, JsonElement> entry : entries) {
        if (!SparkProperties.openapiFields.contains(entry.getKey())) {
          throw new IllegalArgumentException(String.format("The field `%s` in the JSON string is not defined in the `SparkProperties` properties. JSON: %s", entry.getKey(), jsonElement.toString()));
        }
      }
        JsonObject jsonObj = jsonElement.getAsJsonObject();
      // validate the optional field `metastoreServiceConfig`
      if (jsonObj.get("metastoreServiceConfig") != null && !jsonObj.get("metastoreServiceConfig").isJsonNull()) {
        MetastoreServiceConfig.validateJsonElement(jsonObj.get("metastoreServiceConfig"));
      }
      if ((jsonObj.get("serviceAccountId") != null && !jsonObj.get("serviceAccountId").isJsonNull()) && !jsonObj.get("serviceAccountId").isJsonPrimitive()) {
        throw new IllegalArgumentException(String.format("Expected the field `serviceAccountId` to be a primitive type in the JSON string but got `%s`", jsonObj.get("serviceAccountId").toString()));
      }
      // validate the optional field `sparkHistoryServerConfig`
      if (jsonObj.get("sparkHistoryServerConfig") != null && !jsonObj.get("sparkHistoryServerConfig").isJsonNull()) {
        SparkHistoryServerConfig.validateJsonElement(jsonObj.get("sparkHistoryServerConfig"));
      }
  }

  public static class CustomTypeAdapterFactory implements TypeAdapterFactory {
    @SuppressWarnings("unchecked")
    @Override
    public <T> TypeAdapter<T> create(Gson gson, TypeToken<T> type) {
       if (!SparkProperties.class.isAssignableFrom(type.getRawType())) {
         return null; // this class only serializes 'SparkProperties' and its subtypes
       }
       final TypeAdapter<JsonElement> elementAdapter = gson.getAdapter(JsonElement.class);
       final TypeAdapter<SparkProperties> thisAdapter
                        = gson.getDelegateAdapter(this, TypeToken.get(SparkProperties.class));

       return (TypeAdapter<T>) new TypeAdapter<SparkProperties>() {
           @Override
           public void write(JsonWriter out, SparkProperties value) throws IOException {
             JsonObject obj = thisAdapter.toJsonTree(value).getAsJsonObject();
             elementAdapter.write(out, obj);
           }

           @Override
           public SparkProperties read(JsonReader in) throws IOException {
             JsonElement jsonElement = elementAdapter.read(in);
             validateJsonElement(jsonElement);
             return thisAdapter.fromJsonTree(jsonElement);
           }

       }.nullSafe();
    }
  }

  /**
   * Create an instance of SparkProperties given an JSON string
   *
   * @param jsonString JSON string
   * @return An instance of SparkProperties
   * @throws IOException if the JSON string is invalid with respect to SparkProperties
   */
  public static SparkProperties fromJson(String jsonString) throws IOException {
    return JSON.getGson().fromJson(jsonString, SparkProperties.class);
  }

  /**
   * Convert an instance of SparkProperties to an JSON string
   *
   * @return JSON string
   */
  public String toJson() {
    return JSON.getGson().toJson(this);
  }
}

