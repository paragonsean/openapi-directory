# VertexAiApi.LearningServingLlmMessageMetadata

## Properties

Name | Type | Description | Notes
------------ | ------------- | ------------- | -------------
**classifierSummary** | [**LearningGenaiRootClassifierOutputSummary**](LearningGenaiRootClassifierOutputSummary.md) |  | [optional] 
**codeyOutput** | [**LearningGenaiRootCodeyOutput**](LearningGenaiRootCodeyOutput.md) |  | [optional] 
**currentStreamTextLength** | **Number** |  | [optional] 
**deleted** | **Boolean** | Whether the corresponding message has been deleted. | [optional] 
**filterMeta** | [**[LearningGenaiRootFilterMetadata]**](LearningGenaiRootFilterMetadata.md) | Metadata for filters that triggered. | [optional] 
**finalMessageScore** | [**LearningGenaiRootScore**](LearningGenaiRootScore.md) |  | [optional] 
**finishReason** | **String** | NOT YET IMPLEMENTED. | [optional] 
**groundingMetadata** | [**LearningGenaiRootGroundingMetadata**](LearningGenaiRootGroundingMetadata.md) |  | [optional] 
**isCode** | **Boolean** | Applies to streaming response message only. Whether the message is a code. | [optional] 
**isFallback** | **Boolean** | Applies to Response message only. Indicates whether the message is a fallback and the response would have otherwise been empty. | [optional] 
**langidResult** | [**NlpSaftLangIdResult**](NlpSaftLangIdResult.md) |  | [optional] 
**language** | **String** | Detected language. | [optional] 
**lmPrefix** | **String** | The LM prefix used to generate this response. | [optional] 
**originalText** | **String** | The original text generated by LLM. This is the raw output for debugging purposes. | [optional] 
**perStreamDecodedTokenCount** | **Number** | NOT YET IMPLEMENTED. Applies to streaming only. Number of tokens decoded / emitted by the model as part of this stream. This may be different from token_count, which contains number of tokens returned in this response after any response rewriting / truncation. | [optional] 
**raiOutputs** | [**[LearningGenaiRootRAIOutput]**](LearningGenaiRootRAIOutput.md) | Results of running RAI on the query or this response candidate. One output per rai_config. It will be populated regardless of whether the threshold is exceeded or not. | [optional] 
**recitationResult** | [**LearningGenaiRecitationRecitationResult**](LearningGenaiRecitationRecitationResult.md) |  | [optional] 
**returnTokenCount** | **Number** | NOT YET IMPLEMENTED. Number of tokens returned as part of this candidate. | [optional] 
**scores** | [**[LearningGenaiRootScore]**](LearningGenaiRootScore.md) | All the different scores for a message are logged here. | [optional] 
**streamTerminated** | **Boolean** | Whether the response is terminated during streaming return. Only used for streaming requests. | [optional] 
**totalDecodedTokenCount** | **Number** | NOT YET IMPLEMENTED. Aggregated number of total tokens decoded so far. For streaming, this is sum of all the tokens decoded so far i.e. aggregated count. | [optional] 
**translatedUserPrompts** | **[String]** | Translated user-prompt used for RAI post processing. This is for internal processing only. We will translate in pre-processor and pass the translated text to the post processor using this field. It will be empty if non of the signals requested need translation. | [optional] 
**vertexRaiResult** | [**CloudAiNlLlmProtoServiceRaiResult**](CloudAiNlLlmProtoServiceRaiResult.md) |  | [optional] 



## Enum: FinishReasonEnum


* `UNSPECIFIED` (value: `"UNSPECIFIED"`)

* `RETURN` (value: `"RETURN"`)

* `STOP` (value: `"STOP"`)

* `MAX_TOKENS` (value: `"MAX_TOKENS"`)

* `FILTER` (value: `"FILTER"`)




