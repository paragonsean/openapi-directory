# coding: utf-8

from datetime import date, datetime

from typing import List, Dict, Type

from openapi_server.models.base_model import Model
from openapi_server.models.cloud_ai_nl_llm_proto_service_rai_result import CloudAiNlLlmProtoServiceRaiResult
from openapi_server.models.learning_genai_recitation_recitation_result import LearningGenaiRecitationRecitationResult
from openapi_server.models.learning_genai_root_classifier_output_summary import LearningGenaiRootClassifierOutputSummary
from openapi_server.models.learning_genai_root_codey_output import LearningGenaiRootCodeyOutput
from openapi_server.models.learning_genai_root_filter_metadata import LearningGenaiRootFilterMetadata
from openapi_server.models.learning_genai_root_grounding_metadata import LearningGenaiRootGroundingMetadata
from openapi_server.models.learning_genai_root_rai_output import LearningGenaiRootRAIOutput
from openapi_server.models.learning_genai_root_score import LearningGenaiRootScore
from openapi_server.models.nlp_saft_lang_id_result import NlpSaftLangIdResult
from openapi_server import util


class LearningServingLlmMessageMetadata(Model):
    """NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).

    Do not edit the class manually.
    """

    def __init__(self, classifier_summary: LearningGenaiRootClassifierOutputSummary=None, codey_output: LearningGenaiRootCodeyOutput=None, current_stream_text_length: int=None, deleted: bool=None, filter_meta: List[LearningGenaiRootFilterMetadata]=None, final_message_score: LearningGenaiRootScore=None, finish_reason: str=None, grounding_metadata: LearningGenaiRootGroundingMetadata=None, is_code: bool=None, is_fallback: bool=None, langid_result: NlpSaftLangIdResult=None, language: str=None, lm_prefix: str=None, original_text: str=None, per_stream_decoded_token_count: int=None, rai_outputs: List[LearningGenaiRootRAIOutput]=None, recitation_result: LearningGenaiRecitationRecitationResult=None, return_token_count: int=None, scores: List[LearningGenaiRootScore]=None, stream_terminated: bool=None, total_decoded_token_count: int=None, translated_user_prompts: List[str]=None, vertex_rai_result: CloudAiNlLlmProtoServiceRaiResult=None):
        """LearningServingLlmMessageMetadata - a model defined in OpenAPI

        :param classifier_summary: The classifier_summary of this LearningServingLlmMessageMetadata.
        :param codey_output: The codey_output of this LearningServingLlmMessageMetadata.
        :param current_stream_text_length: The current_stream_text_length of this LearningServingLlmMessageMetadata.
        :param deleted: The deleted of this LearningServingLlmMessageMetadata.
        :param filter_meta: The filter_meta of this LearningServingLlmMessageMetadata.
        :param final_message_score: The final_message_score of this LearningServingLlmMessageMetadata.
        :param finish_reason: The finish_reason of this LearningServingLlmMessageMetadata.
        :param grounding_metadata: The grounding_metadata of this LearningServingLlmMessageMetadata.
        :param is_code: The is_code of this LearningServingLlmMessageMetadata.
        :param is_fallback: The is_fallback of this LearningServingLlmMessageMetadata.
        :param langid_result: The langid_result of this LearningServingLlmMessageMetadata.
        :param language: The language of this LearningServingLlmMessageMetadata.
        :param lm_prefix: The lm_prefix of this LearningServingLlmMessageMetadata.
        :param original_text: The original_text of this LearningServingLlmMessageMetadata.
        :param per_stream_decoded_token_count: The per_stream_decoded_token_count of this LearningServingLlmMessageMetadata.
        :param rai_outputs: The rai_outputs of this LearningServingLlmMessageMetadata.
        :param recitation_result: The recitation_result of this LearningServingLlmMessageMetadata.
        :param return_token_count: The return_token_count of this LearningServingLlmMessageMetadata.
        :param scores: The scores of this LearningServingLlmMessageMetadata.
        :param stream_terminated: The stream_terminated of this LearningServingLlmMessageMetadata.
        :param total_decoded_token_count: The total_decoded_token_count of this LearningServingLlmMessageMetadata.
        :param translated_user_prompts: The translated_user_prompts of this LearningServingLlmMessageMetadata.
        :param vertex_rai_result: The vertex_rai_result of this LearningServingLlmMessageMetadata.
        """
        self.openapi_types = {
            'classifier_summary': LearningGenaiRootClassifierOutputSummary,
            'codey_output': LearningGenaiRootCodeyOutput,
            'current_stream_text_length': int,
            'deleted': bool,
            'filter_meta': List[LearningGenaiRootFilterMetadata],
            'final_message_score': LearningGenaiRootScore,
            'finish_reason': str,
            'grounding_metadata': LearningGenaiRootGroundingMetadata,
            'is_code': bool,
            'is_fallback': bool,
            'langid_result': NlpSaftLangIdResult,
            'language': str,
            'lm_prefix': str,
            'original_text': str,
            'per_stream_decoded_token_count': int,
            'rai_outputs': List[LearningGenaiRootRAIOutput],
            'recitation_result': LearningGenaiRecitationRecitationResult,
            'return_token_count': int,
            'scores': List[LearningGenaiRootScore],
            'stream_terminated': bool,
            'total_decoded_token_count': int,
            'translated_user_prompts': List[str],
            'vertex_rai_result': CloudAiNlLlmProtoServiceRaiResult
        }

        self.attribute_map = {
            'classifier_summary': 'classifierSummary',
            'codey_output': 'codeyOutput',
            'current_stream_text_length': 'currentStreamTextLength',
            'deleted': 'deleted',
            'filter_meta': 'filterMeta',
            'final_message_score': 'finalMessageScore',
            'finish_reason': 'finishReason',
            'grounding_metadata': 'groundingMetadata',
            'is_code': 'isCode',
            'is_fallback': 'isFallback',
            'langid_result': 'langidResult',
            'language': 'language',
            'lm_prefix': 'lmPrefix',
            'original_text': 'originalText',
            'per_stream_decoded_token_count': 'perStreamDecodedTokenCount',
            'rai_outputs': 'raiOutputs',
            'recitation_result': 'recitationResult',
            'return_token_count': 'returnTokenCount',
            'scores': 'scores',
            'stream_terminated': 'streamTerminated',
            'total_decoded_token_count': 'totalDecodedTokenCount',
            'translated_user_prompts': 'translatedUserPrompts',
            'vertex_rai_result': 'vertexRaiResult'
        }

        self._classifier_summary = classifier_summary
        self._codey_output = codey_output
        self._current_stream_text_length = current_stream_text_length
        self._deleted = deleted
        self._filter_meta = filter_meta
        self._final_message_score = final_message_score
        self._finish_reason = finish_reason
        self._grounding_metadata = grounding_metadata
        self._is_code = is_code
        self._is_fallback = is_fallback
        self._langid_result = langid_result
        self._language = language
        self._lm_prefix = lm_prefix
        self._original_text = original_text
        self._per_stream_decoded_token_count = per_stream_decoded_token_count
        self._rai_outputs = rai_outputs
        self._recitation_result = recitation_result
        self._return_token_count = return_token_count
        self._scores = scores
        self._stream_terminated = stream_terminated
        self._total_decoded_token_count = total_decoded_token_count
        self._translated_user_prompts = translated_user_prompts
        self._vertex_rai_result = vertex_rai_result

    @classmethod
    def from_dict(cls, dikt: dict) -> 'LearningServingLlmMessageMetadata':
        """Returns the dict as a model

        :param dikt: A dict.
        :return: The LearningServingLlmMessageMetadata of this LearningServingLlmMessageMetadata.
        """
        return util.deserialize_model(dikt, cls)

    @property
    def classifier_summary(self):
        """Gets the classifier_summary of this LearningServingLlmMessageMetadata.


        :return: The classifier_summary of this LearningServingLlmMessageMetadata.
        :rtype: LearningGenaiRootClassifierOutputSummary
        """
        return self._classifier_summary

    @classifier_summary.setter
    def classifier_summary(self, classifier_summary):
        """Sets the classifier_summary of this LearningServingLlmMessageMetadata.


        :param classifier_summary: The classifier_summary of this LearningServingLlmMessageMetadata.
        :type classifier_summary: LearningGenaiRootClassifierOutputSummary
        """

        self._classifier_summary = classifier_summary

    @property
    def codey_output(self):
        """Gets the codey_output of this LearningServingLlmMessageMetadata.


        :return: The codey_output of this LearningServingLlmMessageMetadata.
        :rtype: LearningGenaiRootCodeyOutput
        """
        return self._codey_output

    @codey_output.setter
    def codey_output(self, codey_output):
        """Sets the codey_output of this LearningServingLlmMessageMetadata.


        :param codey_output: The codey_output of this LearningServingLlmMessageMetadata.
        :type codey_output: LearningGenaiRootCodeyOutput
        """

        self._codey_output = codey_output

    @property
    def current_stream_text_length(self):
        """Gets the current_stream_text_length of this LearningServingLlmMessageMetadata.


        :return: The current_stream_text_length of this LearningServingLlmMessageMetadata.
        :rtype: int
        """
        return self._current_stream_text_length

    @current_stream_text_length.setter
    def current_stream_text_length(self, current_stream_text_length):
        """Sets the current_stream_text_length of this LearningServingLlmMessageMetadata.


        :param current_stream_text_length: The current_stream_text_length of this LearningServingLlmMessageMetadata.
        :type current_stream_text_length: int
        """

        self._current_stream_text_length = current_stream_text_length

    @property
    def deleted(self):
        """Gets the deleted of this LearningServingLlmMessageMetadata.

        Whether the corresponding message has been deleted.

        :return: The deleted of this LearningServingLlmMessageMetadata.
        :rtype: bool
        """
        return self._deleted

    @deleted.setter
    def deleted(self, deleted):
        """Sets the deleted of this LearningServingLlmMessageMetadata.

        Whether the corresponding message has been deleted.

        :param deleted: The deleted of this LearningServingLlmMessageMetadata.
        :type deleted: bool
        """

        self._deleted = deleted

    @property
    def filter_meta(self):
        """Gets the filter_meta of this LearningServingLlmMessageMetadata.

        Metadata for filters that triggered.

        :return: The filter_meta of this LearningServingLlmMessageMetadata.
        :rtype: List[LearningGenaiRootFilterMetadata]
        """
        return self._filter_meta

    @filter_meta.setter
    def filter_meta(self, filter_meta):
        """Sets the filter_meta of this LearningServingLlmMessageMetadata.

        Metadata for filters that triggered.

        :param filter_meta: The filter_meta of this LearningServingLlmMessageMetadata.
        :type filter_meta: List[LearningGenaiRootFilterMetadata]
        """

        self._filter_meta = filter_meta

    @property
    def final_message_score(self):
        """Gets the final_message_score of this LearningServingLlmMessageMetadata.


        :return: The final_message_score of this LearningServingLlmMessageMetadata.
        :rtype: LearningGenaiRootScore
        """
        return self._final_message_score

    @final_message_score.setter
    def final_message_score(self, final_message_score):
        """Sets the final_message_score of this LearningServingLlmMessageMetadata.


        :param final_message_score: The final_message_score of this LearningServingLlmMessageMetadata.
        :type final_message_score: LearningGenaiRootScore
        """

        self._final_message_score = final_message_score

    @property
    def finish_reason(self):
        """Gets the finish_reason of this LearningServingLlmMessageMetadata.

        NOT YET IMPLEMENTED.

        :return: The finish_reason of this LearningServingLlmMessageMetadata.
        :rtype: str
        """
        return self._finish_reason

    @finish_reason.setter
    def finish_reason(self, finish_reason):
        """Sets the finish_reason of this LearningServingLlmMessageMetadata.

        NOT YET IMPLEMENTED.

        :param finish_reason: The finish_reason of this LearningServingLlmMessageMetadata.
        :type finish_reason: str
        """
        allowed_values = ["UNSPECIFIED", "RETURN", "STOP", "MAX_TOKENS", "FILTER"]  # noqa: E501
        if finish_reason not in allowed_values:
            raise ValueError(
                "Invalid value for `finish_reason` ({0}), must be one of {1}"
                .format(finish_reason, allowed_values)
            )

        self._finish_reason = finish_reason

    @property
    def grounding_metadata(self):
        """Gets the grounding_metadata of this LearningServingLlmMessageMetadata.


        :return: The grounding_metadata of this LearningServingLlmMessageMetadata.
        :rtype: LearningGenaiRootGroundingMetadata
        """
        return self._grounding_metadata

    @grounding_metadata.setter
    def grounding_metadata(self, grounding_metadata):
        """Sets the grounding_metadata of this LearningServingLlmMessageMetadata.


        :param grounding_metadata: The grounding_metadata of this LearningServingLlmMessageMetadata.
        :type grounding_metadata: LearningGenaiRootGroundingMetadata
        """

        self._grounding_metadata = grounding_metadata

    @property
    def is_code(self):
        """Gets the is_code of this LearningServingLlmMessageMetadata.

        Applies to streaming response message only. Whether the message is a code.

        :return: The is_code of this LearningServingLlmMessageMetadata.
        :rtype: bool
        """
        return self._is_code

    @is_code.setter
    def is_code(self, is_code):
        """Sets the is_code of this LearningServingLlmMessageMetadata.

        Applies to streaming response message only. Whether the message is a code.

        :param is_code: The is_code of this LearningServingLlmMessageMetadata.
        :type is_code: bool
        """

        self._is_code = is_code

    @property
    def is_fallback(self):
        """Gets the is_fallback of this LearningServingLlmMessageMetadata.

        Applies to Response message only. Indicates whether the message is a fallback and the response would have otherwise been empty.

        :return: The is_fallback of this LearningServingLlmMessageMetadata.
        :rtype: bool
        """
        return self._is_fallback

    @is_fallback.setter
    def is_fallback(self, is_fallback):
        """Sets the is_fallback of this LearningServingLlmMessageMetadata.

        Applies to Response message only. Indicates whether the message is a fallback and the response would have otherwise been empty.

        :param is_fallback: The is_fallback of this LearningServingLlmMessageMetadata.
        :type is_fallback: bool
        """

        self._is_fallback = is_fallback

    @property
    def langid_result(self):
        """Gets the langid_result of this LearningServingLlmMessageMetadata.


        :return: The langid_result of this LearningServingLlmMessageMetadata.
        :rtype: NlpSaftLangIdResult
        """
        return self._langid_result

    @langid_result.setter
    def langid_result(self, langid_result):
        """Sets the langid_result of this LearningServingLlmMessageMetadata.


        :param langid_result: The langid_result of this LearningServingLlmMessageMetadata.
        :type langid_result: NlpSaftLangIdResult
        """

        self._langid_result = langid_result

    @property
    def language(self):
        """Gets the language of this LearningServingLlmMessageMetadata.

        Detected language.

        :return: The language of this LearningServingLlmMessageMetadata.
        :rtype: str
        """
        return self._language

    @language.setter
    def language(self, language):
        """Sets the language of this LearningServingLlmMessageMetadata.

        Detected language.

        :param language: The language of this LearningServingLlmMessageMetadata.
        :type language: str
        """

        self._language = language

    @property
    def lm_prefix(self):
        """Gets the lm_prefix of this LearningServingLlmMessageMetadata.

        The LM prefix used to generate this response.

        :return: The lm_prefix of this LearningServingLlmMessageMetadata.
        :rtype: str
        """
        return self._lm_prefix

    @lm_prefix.setter
    def lm_prefix(self, lm_prefix):
        """Sets the lm_prefix of this LearningServingLlmMessageMetadata.

        The LM prefix used to generate this response.

        :param lm_prefix: The lm_prefix of this LearningServingLlmMessageMetadata.
        :type lm_prefix: str
        """

        self._lm_prefix = lm_prefix

    @property
    def original_text(self):
        """Gets the original_text of this LearningServingLlmMessageMetadata.

        The original text generated by LLM. This is the raw output for debugging purposes.

        :return: The original_text of this LearningServingLlmMessageMetadata.
        :rtype: str
        """
        return self._original_text

    @original_text.setter
    def original_text(self, original_text):
        """Sets the original_text of this LearningServingLlmMessageMetadata.

        The original text generated by LLM. This is the raw output for debugging purposes.

        :param original_text: The original_text of this LearningServingLlmMessageMetadata.
        :type original_text: str
        """

        self._original_text = original_text

    @property
    def per_stream_decoded_token_count(self):
        """Gets the per_stream_decoded_token_count of this LearningServingLlmMessageMetadata.

        NOT YET IMPLEMENTED. Applies to streaming only. Number of tokens decoded / emitted by the model as part of this stream. This may be different from token_count, which contains number of tokens returned in this response after any response rewriting / truncation.

        :return: The per_stream_decoded_token_count of this LearningServingLlmMessageMetadata.
        :rtype: int
        """
        return self._per_stream_decoded_token_count

    @per_stream_decoded_token_count.setter
    def per_stream_decoded_token_count(self, per_stream_decoded_token_count):
        """Sets the per_stream_decoded_token_count of this LearningServingLlmMessageMetadata.

        NOT YET IMPLEMENTED. Applies to streaming only. Number of tokens decoded / emitted by the model as part of this stream. This may be different from token_count, which contains number of tokens returned in this response after any response rewriting / truncation.

        :param per_stream_decoded_token_count: The per_stream_decoded_token_count of this LearningServingLlmMessageMetadata.
        :type per_stream_decoded_token_count: int
        """

        self._per_stream_decoded_token_count = per_stream_decoded_token_count

    @property
    def rai_outputs(self):
        """Gets the rai_outputs of this LearningServingLlmMessageMetadata.

        Results of running RAI on the query or this response candidate. One output per rai_config. It will be populated regardless of whether the threshold is exceeded or not.

        :return: The rai_outputs of this LearningServingLlmMessageMetadata.
        :rtype: List[LearningGenaiRootRAIOutput]
        """
        return self._rai_outputs

    @rai_outputs.setter
    def rai_outputs(self, rai_outputs):
        """Sets the rai_outputs of this LearningServingLlmMessageMetadata.

        Results of running RAI on the query or this response candidate. One output per rai_config. It will be populated regardless of whether the threshold is exceeded or not.

        :param rai_outputs: The rai_outputs of this LearningServingLlmMessageMetadata.
        :type rai_outputs: List[LearningGenaiRootRAIOutput]
        """

        self._rai_outputs = rai_outputs

    @property
    def recitation_result(self):
        """Gets the recitation_result of this LearningServingLlmMessageMetadata.


        :return: The recitation_result of this LearningServingLlmMessageMetadata.
        :rtype: LearningGenaiRecitationRecitationResult
        """
        return self._recitation_result

    @recitation_result.setter
    def recitation_result(self, recitation_result):
        """Sets the recitation_result of this LearningServingLlmMessageMetadata.


        :param recitation_result: The recitation_result of this LearningServingLlmMessageMetadata.
        :type recitation_result: LearningGenaiRecitationRecitationResult
        """

        self._recitation_result = recitation_result

    @property
    def return_token_count(self):
        """Gets the return_token_count of this LearningServingLlmMessageMetadata.

        NOT YET IMPLEMENTED. Number of tokens returned as part of this candidate.

        :return: The return_token_count of this LearningServingLlmMessageMetadata.
        :rtype: int
        """
        return self._return_token_count

    @return_token_count.setter
    def return_token_count(self, return_token_count):
        """Sets the return_token_count of this LearningServingLlmMessageMetadata.

        NOT YET IMPLEMENTED. Number of tokens returned as part of this candidate.

        :param return_token_count: The return_token_count of this LearningServingLlmMessageMetadata.
        :type return_token_count: int
        """

        self._return_token_count = return_token_count

    @property
    def scores(self):
        """Gets the scores of this LearningServingLlmMessageMetadata.

        All the different scores for a message are logged here.

        :return: The scores of this LearningServingLlmMessageMetadata.
        :rtype: List[LearningGenaiRootScore]
        """
        return self._scores

    @scores.setter
    def scores(self, scores):
        """Sets the scores of this LearningServingLlmMessageMetadata.

        All the different scores for a message are logged here.

        :param scores: The scores of this LearningServingLlmMessageMetadata.
        :type scores: List[LearningGenaiRootScore]
        """

        self._scores = scores

    @property
    def stream_terminated(self):
        """Gets the stream_terminated of this LearningServingLlmMessageMetadata.

        Whether the response is terminated during streaming return. Only used for streaming requests.

        :return: The stream_terminated of this LearningServingLlmMessageMetadata.
        :rtype: bool
        """
        return self._stream_terminated

    @stream_terminated.setter
    def stream_terminated(self, stream_terminated):
        """Sets the stream_terminated of this LearningServingLlmMessageMetadata.

        Whether the response is terminated during streaming return. Only used for streaming requests.

        :param stream_terminated: The stream_terminated of this LearningServingLlmMessageMetadata.
        :type stream_terminated: bool
        """

        self._stream_terminated = stream_terminated

    @property
    def total_decoded_token_count(self):
        """Gets the total_decoded_token_count of this LearningServingLlmMessageMetadata.

        NOT YET IMPLEMENTED. Aggregated number of total tokens decoded so far. For streaming, this is sum of all the tokens decoded so far i.e. aggregated count.

        :return: The total_decoded_token_count of this LearningServingLlmMessageMetadata.
        :rtype: int
        """
        return self._total_decoded_token_count

    @total_decoded_token_count.setter
    def total_decoded_token_count(self, total_decoded_token_count):
        """Sets the total_decoded_token_count of this LearningServingLlmMessageMetadata.

        NOT YET IMPLEMENTED. Aggregated number of total tokens decoded so far. For streaming, this is sum of all the tokens decoded so far i.e. aggregated count.

        :param total_decoded_token_count: The total_decoded_token_count of this LearningServingLlmMessageMetadata.
        :type total_decoded_token_count: int
        """

        self._total_decoded_token_count = total_decoded_token_count

    @property
    def translated_user_prompts(self):
        """Gets the translated_user_prompts of this LearningServingLlmMessageMetadata.

        Translated user-prompt used for RAI post processing. This is for internal processing only. We will translate in pre-processor and pass the translated text to the post processor using this field. It will be empty if non of the signals requested need translation.

        :return: The translated_user_prompts of this LearningServingLlmMessageMetadata.
        :rtype: List[str]
        """
        return self._translated_user_prompts

    @translated_user_prompts.setter
    def translated_user_prompts(self, translated_user_prompts):
        """Sets the translated_user_prompts of this LearningServingLlmMessageMetadata.

        Translated user-prompt used for RAI post processing. This is for internal processing only. We will translate in pre-processor and pass the translated text to the post processor using this field. It will be empty if non of the signals requested need translation.

        :param translated_user_prompts: The translated_user_prompts of this LearningServingLlmMessageMetadata.
        :type translated_user_prompts: List[str]
        """

        self._translated_user_prompts = translated_user_prompts

    @property
    def vertex_rai_result(self):
        """Gets the vertex_rai_result of this LearningServingLlmMessageMetadata.


        :return: The vertex_rai_result of this LearningServingLlmMessageMetadata.
        :rtype: CloudAiNlLlmProtoServiceRaiResult
        """
        return self._vertex_rai_result

    @vertex_rai_result.setter
    def vertex_rai_result(self, vertex_rai_result):
        """Sets the vertex_rai_result of this LearningServingLlmMessageMetadata.


        :param vertex_rai_result: The vertex_rai_result of this LearningServingLlmMessageMetadata.
        :type vertex_rai_result: CloudAiNlLlmProtoServiceRaiResult
        """

        self._vertex_rai_result = vertex_rai_result
