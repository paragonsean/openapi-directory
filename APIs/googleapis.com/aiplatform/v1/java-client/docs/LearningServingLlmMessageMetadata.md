

# LearningServingLlmMessageMetadata

LINT.IfChange This metadata contains additional information required for debugging.

## Properties

| Name | Type | Description | Notes |
|------------ | ------------- | ------------- | -------------|
|**classifierSummary** | [**LearningGenaiRootClassifierOutputSummary**](LearningGenaiRootClassifierOutputSummary.md) |  |  [optional] |
|**codeyOutput** | [**LearningGenaiRootCodeyOutput**](LearningGenaiRootCodeyOutput.md) |  |  [optional] |
|**currentStreamTextLength** | **Integer** |  |  [optional] |
|**deleted** | **Boolean** | Whether the corresponding message has been deleted. |  [optional] |
|**filterMeta** | [**List&lt;LearningGenaiRootFilterMetadata&gt;**](LearningGenaiRootFilterMetadata.md) | Metadata for filters that triggered. |  [optional] |
|**finalMessageScore** | [**LearningGenaiRootScore**](LearningGenaiRootScore.md) |  |  [optional] |
|**finishReason** | [**FinishReasonEnum**](#FinishReasonEnum) | NOT YET IMPLEMENTED. |  [optional] |
|**groundingMetadata** | [**LearningGenaiRootGroundingMetadata**](LearningGenaiRootGroundingMetadata.md) |  |  [optional] |
|**isCode** | **Boolean** | Applies to streaming response message only. Whether the message is a code. |  [optional] |
|**isFallback** | **Boolean** | Applies to Response message only. Indicates whether the message is a fallback and the response would have otherwise been empty. |  [optional] |
|**langidResult** | [**NlpSaftLangIdResult**](NlpSaftLangIdResult.md) |  |  [optional] |
|**language** | **String** | Detected language. |  [optional] |
|**lmPrefix** | **String** | The LM prefix used to generate this response. |  [optional] |
|**originalText** | **String** | The original text generated by LLM. This is the raw output for debugging purposes. |  [optional] |
|**perStreamDecodedTokenCount** | **Integer** | NOT YET IMPLEMENTED. Applies to streaming only. Number of tokens decoded / emitted by the model as part of this stream. This may be different from token_count, which contains number of tokens returned in this response after any response rewriting / truncation. |  [optional] |
|**raiOutputs** | [**List&lt;LearningGenaiRootRAIOutput&gt;**](LearningGenaiRootRAIOutput.md) | Results of running RAI on the query or this response candidate. One output per rai_config. It will be populated regardless of whether the threshold is exceeded or not. |  [optional] |
|**recitationResult** | [**LearningGenaiRecitationRecitationResult**](LearningGenaiRecitationRecitationResult.md) |  |  [optional] |
|**returnTokenCount** | **Integer** | NOT YET IMPLEMENTED. Number of tokens returned as part of this candidate. |  [optional] |
|**scores** | [**List&lt;LearningGenaiRootScore&gt;**](LearningGenaiRootScore.md) | All the different scores for a message are logged here. |  [optional] |
|**streamTerminated** | **Boolean** | Whether the response is terminated during streaming return. Only used for streaming requests. |  [optional] |
|**totalDecodedTokenCount** | **Integer** | NOT YET IMPLEMENTED. Aggregated number of total tokens decoded so far. For streaming, this is sum of all the tokens decoded so far i.e. aggregated count. |  [optional] |
|**translatedUserPrompts** | **List&lt;String&gt;** | Translated user-prompt used for RAI post processing. This is for internal processing only. We will translate in pre-processor and pass the translated text to the post processor using this field. It will be empty if non of the signals requested need translation. |  [optional] |
|**vertexRaiResult** | [**CloudAiNlLlmProtoServiceRaiResult**](CloudAiNlLlmProtoServiceRaiResult.md) |  |  [optional] |



## Enum: FinishReasonEnum

| Name | Value |
|---- | -----|
| UNSPECIFIED | &quot;UNSPECIFIED&quot; |
| RETURN | &quot;RETURN&quot; |
| STOP | &quot;STOP&quot; |
| MAX_TOKENS | &quot;MAX_TOKENS&quot; |
| FILTER | &quot;FILTER&quot; |



