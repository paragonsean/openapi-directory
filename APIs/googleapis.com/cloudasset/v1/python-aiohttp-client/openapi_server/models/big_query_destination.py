# coding: utf-8

from datetime import date, datetime

from typing import List, Dict, Type

from openapi_server.models.base_model import Model
from openapi_server.models.partition_spec import PartitionSpec
from openapi_server import util


class BigQueryDestination(Model):
    """NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).

    Do not edit the class manually.
    """

    def __init__(self, dataset: str=None, force: bool=None, partition_spec: PartitionSpec=None, separate_tables_per_asset_type: bool=None, table: str=None):
        """BigQueryDestination - a model defined in OpenAPI

        :param dataset: The dataset of this BigQueryDestination.
        :param force: The force of this BigQueryDestination.
        :param partition_spec: The partition_spec of this BigQueryDestination.
        :param separate_tables_per_asset_type: The separate_tables_per_asset_type of this BigQueryDestination.
        :param table: The table of this BigQueryDestination.
        """
        self.openapi_types = {
            'dataset': str,
            'force': bool,
            'partition_spec': PartitionSpec,
            'separate_tables_per_asset_type': bool,
            'table': str
        }

        self.attribute_map = {
            'dataset': 'dataset',
            'force': 'force',
            'partition_spec': 'partitionSpec',
            'separate_tables_per_asset_type': 'separateTablesPerAssetType',
            'table': 'table'
        }

        self._dataset = dataset
        self._force = force
        self._partition_spec = partition_spec
        self._separate_tables_per_asset_type = separate_tables_per_asset_type
        self._table = table

    @classmethod
    def from_dict(cls, dikt: dict) -> 'BigQueryDestination':
        """Returns the dict as a model

        :param dikt: A dict.
        :return: The BigQueryDestination of this BigQueryDestination.
        """
        return util.deserialize_model(dikt, cls)

    @property
    def dataset(self):
        """Gets the dataset of this BigQueryDestination.

        Required. The BigQuery dataset in format \"projects/projectId/datasets/datasetId\", to which the snapshot result should be exported. If this dataset does not exist, the export call returns an INVALID_ARGUMENT error. Setting the `contentType` for `exportAssets` determines the [schema](/asset-inventory/docs/exporting-to-bigquery#bigquery-schema) of the BigQuery table. Setting `separateTablesPerAssetType` to `TRUE` also influences the schema.

        :return: The dataset of this BigQueryDestination.
        :rtype: str
        """
        return self._dataset

    @dataset.setter
    def dataset(self, dataset):
        """Sets the dataset of this BigQueryDestination.

        Required. The BigQuery dataset in format \"projects/projectId/datasets/datasetId\", to which the snapshot result should be exported. If this dataset does not exist, the export call returns an INVALID_ARGUMENT error. Setting the `contentType` for `exportAssets` determines the [schema](/asset-inventory/docs/exporting-to-bigquery#bigquery-schema) of the BigQuery table. Setting `separateTablesPerAssetType` to `TRUE` also influences the schema.

        :param dataset: The dataset of this BigQueryDestination.
        :type dataset: str
        """

        self._dataset = dataset

    @property
    def force(self):
        """Gets the force of this BigQueryDestination.

        If the destination table already exists and this flag is `TRUE`, the table will be overwritten by the contents of assets snapshot. If the flag is `FALSE` or unset and the destination table already exists, the export call returns an INVALID_ARGUMEMT error.

        :return: The force of this BigQueryDestination.
        :rtype: bool
        """
        return self._force

    @force.setter
    def force(self, force):
        """Sets the force of this BigQueryDestination.

        If the destination table already exists and this flag is `TRUE`, the table will be overwritten by the contents of assets snapshot. If the flag is `FALSE` or unset and the destination table already exists, the export call returns an INVALID_ARGUMEMT error.

        :param force: The force of this BigQueryDestination.
        :type force: bool
        """

        self._force = force

    @property
    def partition_spec(self):
        """Gets the partition_spec of this BigQueryDestination.


        :return: The partition_spec of this BigQueryDestination.
        :rtype: PartitionSpec
        """
        return self._partition_spec

    @partition_spec.setter
    def partition_spec(self, partition_spec):
        """Sets the partition_spec of this BigQueryDestination.


        :param partition_spec: The partition_spec of this BigQueryDestination.
        :type partition_spec: PartitionSpec
        """

        self._partition_spec = partition_spec

    @property
    def separate_tables_per_asset_type(self):
        """Gets the separate_tables_per_asset_type of this BigQueryDestination.

        If this flag is `TRUE`, the snapshot results will be written to one or multiple tables, each of which contains results of one asset type. The [force] and [partition_spec] fields will apply to each of them. Field [table] will be concatenated with \"_\" and the asset type names (see https://cloud.google.com/asset-inventory/docs/supported-asset-types for supported asset types) to construct per-asset-type table names, in which all non-alphanumeric characters like \".\" and \"/\" will be substituted by \"_\". Example: if field [table] is \"mytable\" and snapshot results contain \"storage.googleapis.com/Bucket\" assets, the corresponding table name will be \"mytable_storage_googleapis_com_Bucket\". If any of these tables does not exist, a new table with the concatenated name will be created. When [content_type] in the ExportAssetsRequest is `RESOURCE`, the schema of each table will include RECORD-type columns mapped to the nested fields in the Asset.resource.data field of that asset type (up to the 15 nested level BigQuery supports (https://cloud.google.com/bigquery/docs/nested-repeated#limitations)). The fields in >15 nested levels will be stored in JSON format string as a child column of its parent RECORD column. If error occurs when exporting to any table, the whole export call will return an error but the export results that already succeed will persist. Example: if exporting to table_type_A succeeds when exporting to table_type_B fails during one export call, the results in table_type_A will persist and there will not be partial results persisting in a table.

        :return: The separate_tables_per_asset_type of this BigQueryDestination.
        :rtype: bool
        """
        return self._separate_tables_per_asset_type

    @separate_tables_per_asset_type.setter
    def separate_tables_per_asset_type(self, separate_tables_per_asset_type):
        """Sets the separate_tables_per_asset_type of this BigQueryDestination.

        If this flag is `TRUE`, the snapshot results will be written to one or multiple tables, each of which contains results of one asset type. The [force] and [partition_spec] fields will apply to each of them. Field [table] will be concatenated with \"_\" and the asset type names (see https://cloud.google.com/asset-inventory/docs/supported-asset-types for supported asset types) to construct per-asset-type table names, in which all non-alphanumeric characters like \".\" and \"/\" will be substituted by \"_\". Example: if field [table] is \"mytable\" and snapshot results contain \"storage.googleapis.com/Bucket\" assets, the corresponding table name will be \"mytable_storage_googleapis_com_Bucket\". If any of these tables does not exist, a new table with the concatenated name will be created. When [content_type] in the ExportAssetsRequest is `RESOURCE`, the schema of each table will include RECORD-type columns mapped to the nested fields in the Asset.resource.data field of that asset type (up to the 15 nested level BigQuery supports (https://cloud.google.com/bigquery/docs/nested-repeated#limitations)). The fields in >15 nested levels will be stored in JSON format string as a child column of its parent RECORD column. If error occurs when exporting to any table, the whole export call will return an error but the export results that already succeed will persist. Example: if exporting to table_type_A succeeds when exporting to table_type_B fails during one export call, the results in table_type_A will persist and there will not be partial results persisting in a table.

        :param separate_tables_per_asset_type: The separate_tables_per_asset_type of this BigQueryDestination.
        :type separate_tables_per_asset_type: bool
        """

        self._separate_tables_per_asset_type = separate_tables_per_asset_type

    @property
    def table(self):
        """Gets the table of this BigQueryDestination.

        Required. The BigQuery table to which the snapshot result should be written. If this table does not exist, a new table with the given name will be created.

        :return: The table of this BigQueryDestination.
        :rtype: str
        """
        return self._table

    @table.setter
    def table(self, table):
        """Sets the table of this BigQueryDestination.

        Required. The BigQuery table to which the snapshot result should be written. If this table does not exist, a new table with the given name will be created.

        :param table: The table of this BigQueryDestination.
        :type table: str
        """

        self._table = table
