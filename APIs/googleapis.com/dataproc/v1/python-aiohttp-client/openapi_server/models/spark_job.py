# coding: utf-8

from datetime import date, datetime

from typing import List, Dict, Type

from openapi_server.models.base_model import Model
from openapi_server.models.logging_config import LoggingConfig
from openapi_server import util


class SparkJob(Model):
    """NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).

    Do not edit the class manually.
    """

    def __init__(self, archive_uris: List[str]=None, args: List[str]=None, file_uris: List[str]=None, jar_file_uris: List[str]=None, logging_config: LoggingConfig=None, main_class: str=None, main_jar_file_uri: str=None, properties: Dict[str, str]=None):
        """SparkJob - a model defined in OpenAPI

        :param archive_uris: The archive_uris of this SparkJob.
        :param args: The args of this SparkJob.
        :param file_uris: The file_uris of this SparkJob.
        :param jar_file_uris: The jar_file_uris of this SparkJob.
        :param logging_config: The logging_config of this SparkJob.
        :param main_class: The main_class of this SparkJob.
        :param main_jar_file_uri: The main_jar_file_uri of this SparkJob.
        :param properties: The properties of this SparkJob.
        """
        self.openapi_types = {
            'archive_uris': List[str],
            'args': List[str],
            'file_uris': List[str],
            'jar_file_uris': List[str],
            'logging_config': LoggingConfig,
            'main_class': str,
            'main_jar_file_uri': str,
            'properties': Dict[str, str]
        }

        self.attribute_map = {
            'archive_uris': 'archiveUris',
            'args': 'args',
            'file_uris': 'fileUris',
            'jar_file_uris': 'jarFileUris',
            'logging_config': 'loggingConfig',
            'main_class': 'mainClass',
            'main_jar_file_uri': 'mainJarFileUri',
            'properties': 'properties'
        }

        self._archive_uris = archive_uris
        self._args = args
        self._file_uris = file_uris
        self._jar_file_uris = jar_file_uris
        self._logging_config = logging_config
        self._main_class = main_class
        self._main_jar_file_uri = main_jar_file_uri
        self._properties = properties

    @classmethod
    def from_dict(cls, dikt: dict) -> 'SparkJob':
        """Returns the dict as a model

        :param dikt: A dict.
        :return: The SparkJob of this SparkJob.
        """
        return util.deserialize_model(dikt, cls)

    @property
    def archive_uris(self):
        """Gets the archive_uris of this SparkJob.

        Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.

        :return: The archive_uris of this SparkJob.
        :rtype: List[str]
        """
        return self._archive_uris

    @archive_uris.setter
    def archive_uris(self, archive_uris):
        """Sets the archive_uris of this SparkJob.

        Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.

        :param archive_uris: The archive_uris of this SparkJob.
        :type archive_uris: List[str]
        """

        self._archive_uris = archive_uris

    @property
    def args(self):
        """Gets the args of this SparkJob.

        Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.

        :return: The args of this SparkJob.
        :rtype: List[str]
        """
        return self._args

    @args.setter
    def args(self, args):
        """Sets the args of this SparkJob.

        Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.

        :param args: The args of this SparkJob.
        :type args: List[str]
        """

        self._args = args

    @property
    def file_uris(self):
        """Gets the file_uris of this SparkJob.

        Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.

        :return: The file_uris of this SparkJob.
        :rtype: List[str]
        """
        return self._file_uris

    @file_uris.setter
    def file_uris(self, file_uris):
        """Sets the file_uris of this SparkJob.

        Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.

        :param file_uris: The file_uris of this SparkJob.
        :type file_uris: List[str]
        """

        self._file_uris = file_uris

    @property
    def jar_file_uris(self):
        """Gets the jar_file_uris of this SparkJob.

        Optional. HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.

        :return: The jar_file_uris of this SparkJob.
        :rtype: List[str]
        """
        return self._jar_file_uris

    @jar_file_uris.setter
    def jar_file_uris(self, jar_file_uris):
        """Sets the jar_file_uris of this SparkJob.

        Optional. HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.

        :param jar_file_uris: The jar_file_uris of this SparkJob.
        :type jar_file_uris: List[str]
        """

        self._jar_file_uris = jar_file_uris

    @property
    def logging_config(self):
        """Gets the logging_config of this SparkJob.


        :return: The logging_config of this SparkJob.
        :rtype: LoggingConfig
        """
        return self._logging_config

    @logging_config.setter
    def logging_config(self, logging_config):
        """Sets the logging_config of this SparkJob.


        :param logging_config: The logging_config of this SparkJob.
        :type logging_config: LoggingConfig
        """

        self._logging_config = logging_config

    @property
    def main_class(self):
        """Gets the main_class of this SparkJob.

        The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in SparkJob.jar_file_uris.

        :return: The main_class of this SparkJob.
        :rtype: str
        """
        return self._main_class

    @main_class.setter
    def main_class(self, main_class):
        """Sets the main_class of this SparkJob.

        The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in SparkJob.jar_file_uris.

        :param main_class: The main_class of this SparkJob.
        :type main_class: str
        """

        self._main_class = main_class

    @property
    def main_jar_file_uri(self):
        """Gets the main_jar_file_uri of this SparkJob.

        The HCFS URI of the jar file that contains the main class.

        :return: The main_jar_file_uri of this SparkJob.
        :rtype: str
        """
        return self._main_jar_file_uri

    @main_jar_file_uri.setter
    def main_jar_file_uri(self, main_jar_file_uri):
        """Sets the main_jar_file_uri of this SparkJob.

        The HCFS URI of the jar file that contains the main class.

        :param main_jar_file_uri: The main_jar_file_uri of this SparkJob.
        :type main_jar_file_uri: str
        """

        self._main_jar_file_uri = main_jar_file_uri

    @property
    def properties(self):
        """Gets the properties of this SparkJob.

        Optional. A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Dataproc API might be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.

        :return: The properties of this SparkJob.
        :rtype: Dict[str, str]
        """
        return self._properties

    @properties.setter
    def properties(self, properties):
        """Sets the properties of this SparkJob.

        Optional. A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Dataproc API might be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.

        :param properties: The properties of this SparkJob.
        :type properties: Dict[str, str]
        """

        self._properties = properties
