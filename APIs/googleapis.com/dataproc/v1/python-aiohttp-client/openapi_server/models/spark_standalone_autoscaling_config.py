# coding: utf-8

from datetime import date, datetime

from typing import List, Dict, Type

from openapi_server.models.base_model import Model
from openapi_server import util


class SparkStandaloneAutoscalingConfig(Model):
    """NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).

    Do not edit the class manually.
    """

    def __init__(self, graceful_decommission_timeout: str=None, remove_only_idle_workers: bool=None, scale_down_factor: float=None, scale_down_min_worker_fraction: float=None, scale_up_factor: float=None, scale_up_min_worker_fraction: float=None):
        """SparkStandaloneAutoscalingConfig - a model defined in OpenAPI

        :param graceful_decommission_timeout: The graceful_decommission_timeout of this SparkStandaloneAutoscalingConfig.
        :param remove_only_idle_workers: The remove_only_idle_workers of this SparkStandaloneAutoscalingConfig.
        :param scale_down_factor: The scale_down_factor of this SparkStandaloneAutoscalingConfig.
        :param scale_down_min_worker_fraction: The scale_down_min_worker_fraction of this SparkStandaloneAutoscalingConfig.
        :param scale_up_factor: The scale_up_factor of this SparkStandaloneAutoscalingConfig.
        :param scale_up_min_worker_fraction: The scale_up_min_worker_fraction of this SparkStandaloneAutoscalingConfig.
        """
        self.openapi_types = {
            'graceful_decommission_timeout': str,
            'remove_only_idle_workers': bool,
            'scale_down_factor': float,
            'scale_down_min_worker_fraction': float,
            'scale_up_factor': float,
            'scale_up_min_worker_fraction': float
        }

        self.attribute_map = {
            'graceful_decommission_timeout': 'gracefulDecommissionTimeout',
            'remove_only_idle_workers': 'removeOnlyIdleWorkers',
            'scale_down_factor': 'scaleDownFactor',
            'scale_down_min_worker_fraction': 'scaleDownMinWorkerFraction',
            'scale_up_factor': 'scaleUpFactor',
            'scale_up_min_worker_fraction': 'scaleUpMinWorkerFraction'
        }

        self._graceful_decommission_timeout = graceful_decommission_timeout
        self._remove_only_idle_workers = remove_only_idle_workers
        self._scale_down_factor = scale_down_factor
        self._scale_down_min_worker_fraction = scale_down_min_worker_fraction
        self._scale_up_factor = scale_up_factor
        self._scale_up_min_worker_fraction = scale_up_min_worker_fraction

    @classmethod
    def from_dict(cls, dikt: dict) -> 'SparkStandaloneAutoscalingConfig':
        """Returns the dict as a model

        :param dikt: A dict.
        :return: The SparkStandaloneAutoscalingConfig of this SparkStandaloneAutoscalingConfig.
        """
        return util.deserialize_model(dikt, cls)

    @property
    def graceful_decommission_timeout(self):
        """Gets the graceful_decommission_timeout of this SparkStandaloneAutoscalingConfig.

        Required. Timeout for Spark graceful decommissioning of spark workers. Specifies the duration to wait for spark worker to complete spark decommissioning tasks before forcefully removing workers. Only applicable to downscaling operations.Bounds: 0s, 1d.

        :return: The graceful_decommission_timeout of this SparkStandaloneAutoscalingConfig.
        :rtype: str
        """
        return self._graceful_decommission_timeout

    @graceful_decommission_timeout.setter
    def graceful_decommission_timeout(self, graceful_decommission_timeout):
        """Sets the graceful_decommission_timeout of this SparkStandaloneAutoscalingConfig.

        Required. Timeout for Spark graceful decommissioning of spark workers. Specifies the duration to wait for spark worker to complete spark decommissioning tasks before forcefully removing workers. Only applicable to downscaling operations.Bounds: 0s, 1d.

        :param graceful_decommission_timeout: The graceful_decommission_timeout of this SparkStandaloneAutoscalingConfig.
        :type graceful_decommission_timeout: str
        """

        self._graceful_decommission_timeout = graceful_decommission_timeout

    @property
    def remove_only_idle_workers(self):
        """Gets the remove_only_idle_workers of this SparkStandaloneAutoscalingConfig.

        Optional. Remove only idle workers when scaling down cluster

        :return: The remove_only_idle_workers of this SparkStandaloneAutoscalingConfig.
        :rtype: bool
        """
        return self._remove_only_idle_workers

    @remove_only_idle_workers.setter
    def remove_only_idle_workers(self, remove_only_idle_workers):
        """Sets the remove_only_idle_workers of this SparkStandaloneAutoscalingConfig.

        Optional. Remove only idle workers when scaling down cluster

        :param remove_only_idle_workers: The remove_only_idle_workers of this SparkStandaloneAutoscalingConfig.
        :type remove_only_idle_workers: bool
        """

        self._remove_only_idle_workers = remove_only_idle_workers

    @property
    def scale_down_factor(self):
        """Gets the scale_down_factor of this SparkStandaloneAutoscalingConfig.

        Required. Fraction of required executors to remove from Spark Serverless clusters. A scale-down factor of 1.0 will result in scaling down so that there are no more executors for the Spark Job.(more aggressive scaling). A scale-down factor closer to 0 will result in a smaller magnitude of scaling donw (less aggressive scaling).Bounds: 0.0, 1.0.

        :return: The scale_down_factor of this SparkStandaloneAutoscalingConfig.
        :rtype: float
        """
        return self._scale_down_factor

    @scale_down_factor.setter
    def scale_down_factor(self, scale_down_factor):
        """Sets the scale_down_factor of this SparkStandaloneAutoscalingConfig.

        Required. Fraction of required executors to remove from Spark Serverless clusters. A scale-down factor of 1.0 will result in scaling down so that there are no more executors for the Spark Job.(more aggressive scaling). A scale-down factor closer to 0 will result in a smaller magnitude of scaling donw (less aggressive scaling).Bounds: 0.0, 1.0.

        :param scale_down_factor: The scale_down_factor of this SparkStandaloneAutoscalingConfig.
        :type scale_down_factor: float
        """

        self._scale_down_factor = scale_down_factor

    @property
    def scale_down_min_worker_fraction(self):
        """Gets the scale_down_min_worker_fraction of this SparkStandaloneAutoscalingConfig.

        Optional. Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.

        :return: The scale_down_min_worker_fraction of this SparkStandaloneAutoscalingConfig.
        :rtype: float
        """
        return self._scale_down_min_worker_fraction

    @scale_down_min_worker_fraction.setter
    def scale_down_min_worker_fraction(self, scale_down_min_worker_fraction):
        """Sets the scale_down_min_worker_fraction of this SparkStandaloneAutoscalingConfig.

        Optional. Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.

        :param scale_down_min_worker_fraction: The scale_down_min_worker_fraction of this SparkStandaloneAutoscalingConfig.
        :type scale_down_min_worker_fraction: float
        """

        self._scale_down_min_worker_fraction = scale_down_min_worker_fraction

    @property
    def scale_up_factor(self):
        """Gets the scale_up_factor of this SparkStandaloneAutoscalingConfig.

        Required. Fraction of required workers to add to Spark Standalone clusters. A scale-up factor of 1.0 will result in scaling up so that there are no more required workers for the Spark Job (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling).Bounds: 0.0, 1.0.

        :return: The scale_up_factor of this SparkStandaloneAutoscalingConfig.
        :rtype: float
        """
        return self._scale_up_factor

    @scale_up_factor.setter
    def scale_up_factor(self, scale_up_factor):
        """Sets the scale_up_factor of this SparkStandaloneAutoscalingConfig.

        Required. Fraction of required workers to add to Spark Standalone clusters. A scale-up factor of 1.0 will result in scaling up so that there are no more required workers for the Spark Job (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling).Bounds: 0.0, 1.0.

        :param scale_up_factor: The scale_up_factor of this SparkStandaloneAutoscalingConfig.
        :type scale_up_factor: float
        """

        self._scale_up_factor = scale_up_factor

    @property
    def scale_up_min_worker_fraction(self):
        """Gets the scale_up_min_worker_fraction of this SparkStandaloneAutoscalingConfig.

        Optional. Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.

        :return: The scale_up_min_worker_fraction of this SparkStandaloneAutoscalingConfig.
        :rtype: float
        """
        return self._scale_up_min_worker_fraction

    @scale_up_min_worker_fraction.setter
    def scale_up_min_worker_fraction(self, scale_up_min_worker_fraction):
        """Sets the scale_up_min_worker_fraction of this SparkStandaloneAutoscalingConfig.

        Optional. Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.

        :param scale_up_min_worker_fraction: The scale_up_min_worker_fraction of this SparkStandaloneAutoscalingConfig.
        :type scale_up_min_worker_fraction: float
        """

        self._scale_up_min_worker_fraction = scale_up_min_worker_fraction
