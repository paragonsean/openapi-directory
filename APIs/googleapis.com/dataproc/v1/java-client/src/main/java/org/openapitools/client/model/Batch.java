/*
 * Cloud Dataproc API
 * Manages Hadoop-based clusters and jobs on Google Cloud Platform.
 *
 * The version of the OpenAPI document: v1
 * 
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */


package org.openapitools.client.model;

import java.util.Objects;
import com.google.gson.TypeAdapter;
import com.google.gson.annotations.JsonAdapter;
import com.google.gson.annotations.SerializedName;
import com.google.gson.stream.JsonReader;
import com.google.gson.stream.JsonWriter;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import org.openapitools.client.model.EnvironmentConfig;
import org.openapitools.client.model.PySparkBatch;
import org.openapitools.client.model.RuntimeConfig;
import org.openapitools.client.model.RuntimeInfo;
import org.openapitools.client.model.SparkBatch;
import org.openapitools.client.model.SparkRBatch;
import org.openapitools.client.model.SparkSqlBatch;
import org.openapitools.client.model.StateHistory;

import com.google.gson.Gson;
import com.google.gson.GsonBuilder;
import com.google.gson.JsonArray;
import com.google.gson.JsonDeserializationContext;
import com.google.gson.JsonDeserializer;
import com.google.gson.JsonElement;
import com.google.gson.JsonObject;
import com.google.gson.JsonParseException;
import com.google.gson.TypeAdapterFactory;
import com.google.gson.reflect.TypeToken;
import com.google.gson.TypeAdapter;
import com.google.gson.stream.JsonReader;
import com.google.gson.stream.JsonWriter;
import java.io.IOException;

import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;

import org.openapitools.client.JSON;

/**
 * A representation of a batch workload in the service.
 */
@javax.annotation.Generated(value = "org.openapitools.codegen.languages.JavaClientCodegen", date = "2024-10-12T11:49:50.925918-04:00[America/New_York]", comments = "Generator version: 7.9.0")
public class Batch {
  public static final String SERIALIZED_NAME_CREATE_TIME = "createTime";
  @SerializedName(SERIALIZED_NAME_CREATE_TIME)
  private String createTime;

  public static final String SERIALIZED_NAME_CREATOR = "creator";
  @SerializedName(SERIALIZED_NAME_CREATOR)
  private String creator;

  public static final String SERIALIZED_NAME_ENVIRONMENT_CONFIG = "environmentConfig";
  @SerializedName(SERIALIZED_NAME_ENVIRONMENT_CONFIG)
  private EnvironmentConfig environmentConfig;

  public static final String SERIALIZED_NAME_LABELS = "labels";
  @SerializedName(SERIALIZED_NAME_LABELS)
  private Map<String, String> labels = new HashMap<>();

  public static final String SERIALIZED_NAME_NAME = "name";
  @SerializedName(SERIALIZED_NAME_NAME)
  private String name;

  public static final String SERIALIZED_NAME_OPERATION = "operation";
  @SerializedName(SERIALIZED_NAME_OPERATION)
  private String operation;

  public static final String SERIALIZED_NAME_PYSPARK_BATCH = "pysparkBatch";
  @SerializedName(SERIALIZED_NAME_PYSPARK_BATCH)
  private PySparkBatch pysparkBatch;

  public static final String SERIALIZED_NAME_RUNTIME_CONFIG = "runtimeConfig";
  @SerializedName(SERIALIZED_NAME_RUNTIME_CONFIG)
  private RuntimeConfig runtimeConfig;

  public static final String SERIALIZED_NAME_RUNTIME_INFO = "runtimeInfo";
  @SerializedName(SERIALIZED_NAME_RUNTIME_INFO)
  private RuntimeInfo runtimeInfo;

  public static final String SERIALIZED_NAME_SPARK_BATCH = "sparkBatch";
  @SerializedName(SERIALIZED_NAME_SPARK_BATCH)
  private SparkBatch sparkBatch;

  public static final String SERIALIZED_NAME_SPARK_R_BATCH = "sparkRBatch";
  @SerializedName(SERIALIZED_NAME_SPARK_R_BATCH)
  private SparkRBatch sparkRBatch;

  public static final String SERIALIZED_NAME_SPARK_SQL_BATCH = "sparkSqlBatch";
  @SerializedName(SERIALIZED_NAME_SPARK_SQL_BATCH)
  private SparkSqlBatch sparkSqlBatch;

  /**
   * Output only. The state of the batch.
   */
  @JsonAdapter(StateEnum.Adapter.class)
  public enum StateEnum {
    STATE_UNSPECIFIED("STATE_UNSPECIFIED"),
    
    PENDING("PENDING"),
    
    RUNNING("RUNNING"),
    
    CANCELLING("CANCELLING"),
    
    CANCELLED("CANCELLED"),
    
    SUCCEEDED("SUCCEEDED"),
    
    FAILED("FAILED");

    private String value;

    StateEnum(String value) {
      this.value = value;
    }

    public String getValue() {
      return value;
    }

    @Override
    public String toString() {
      return String.valueOf(value);
    }

    public static StateEnum fromValue(String value) {
      for (StateEnum b : StateEnum.values()) {
        if (b.value.equals(value)) {
          return b;
        }
      }
      throw new IllegalArgumentException("Unexpected value '" + value + "'");
    }

    public static class Adapter extends TypeAdapter<StateEnum> {
      @Override
      public void write(final JsonWriter jsonWriter, final StateEnum enumeration) throws IOException {
        jsonWriter.value(enumeration.getValue());
      }

      @Override
      public StateEnum read(final JsonReader jsonReader) throws IOException {
        String value =  jsonReader.nextString();
        return StateEnum.fromValue(value);
      }
    }

    public static void validateJsonElement(JsonElement jsonElement) throws IOException {
      String value = jsonElement.getAsString();
      StateEnum.fromValue(value);
    }
  }

  public static final String SERIALIZED_NAME_STATE = "state";
  @SerializedName(SERIALIZED_NAME_STATE)
  private StateEnum state;

  public static final String SERIALIZED_NAME_STATE_HISTORY = "stateHistory";
  @SerializedName(SERIALIZED_NAME_STATE_HISTORY)
  private List<StateHistory> stateHistory = new ArrayList<>();

  public static final String SERIALIZED_NAME_STATE_MESSAGE = "stateMessage";
  @SerializedName(SERIALIZED_NAME_STATE_MESSAGE)
  private String stateMessage;

  public static final String SERIALIZED_NAME_STATE_TIME = "stateTime";
  @SerializedName(SERIALIZED_NAME_STATE_TIME)
  private String stateTime;

  public static final String SERIALIZED_NAME_UUID = "uuid";
  @SerializedName(SERIALIZED_NAME_UUID)
  private String uuid;

  public Batch() {
  }

  public Batch(
     String createTime, 
     String creator, 
     String name, 
     String operation, 
     StateEnum state, 
     List<StateHistory> stateHistory, 
     String stateMessage, 
     String stateTime, 
     String uuid
  ) {
    this();
    this.createTime = createTime;
    this.creator = creator;
    this.name = name;
    this.operation = operation;
    this.state = state;
    this.stateHistory = stateHistory;
    this.stateMessage = stateMessage;
    this.stateTime = stateTime;
    this.uuid = uuid;
  }

  /**
   * Output only. The time when the batch was created.
   * @return createTime
   */
  @javax.annotation.Nullable
  public String getCreateTime() {
    return createTime;
  }



  /**
   * Output only. The email address of the user who created the batch.
   * @return creator
   */
  @javax.annotation.Nullable
  public String getCreator() {
    return creator;
  }



  public Batch environmentConfig(EnvironmentConfig environmentConfig) {
    this.environmentConfig = environmentConfig;
    return this;
  }

  /**
   * Get environmentConfig
   * @return environmentConfig
   */
  @javax.annotation.Nullable
  public EnvironmentConfig getEnvironmentConfig() {
    return environmentConfig;
  }

  public void setEnvironmentConfig(EnvironmentConfig environmentConfig) {
    this.environmentConfig = environmentConfig;
  }


  public Batch labels(Map<String, String> labels) {
    this.labels = labels;
    return this;
  }

  public Batch putLabelsItem(String key, String labelsItem) {
    if (this.labels == null) {
      this.labels = new HashMap<>();
    }
    this.labels.put(key, labelsItem);
    return this;
  }

  /**
   * Optional. The labels to associate with this batch. Label keys must contain 1 to 63 characters, and must conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). Label values may be empty, but, if present, must contain 1 to 63 characters, and must conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). No more than 32 labels can be associated with a batch.
   * @return labels
   */
  @javax.annotation.Nullable
  public Map<String, String> getLabels() {
    return labels;
  }

  public void setLabels(Map<String, String> labels) {
    this.labels = labels;
  }


  /**
   * Output only. The resource name of the batch.
   * @return name
   */
  @javax.annotation.Nullable
  public String getName() {
    return name;
  }



  /**
   * Output only. The resource name of the operation associated with this batch.
   * @return operation
   */
  @javax.annotation.Nullable
  public String getOperation() {
    return operation;
  }



  public Batch pysparkBatch(PySparkBatch pysparkBatch) {
    this.pysparkBatch = pysparkBatch;
    return this;
  }

  /**
   * Get pysparkBatch
   * @return pysparkBatch
   */
  @javax.annotation.Nullable
  public PySparkBatch getPysparkBatch() {
    return pysparkBatch;
  }

  public void setPysparkBatch(PySparkBatch pysparkBatch) {
    this.pysparkBatch = pysparkBatch;
  }


  public Batch runtimeConfig(RuntimeConfig runtimeConfig) {
    this.runtimeConfig = runtimeConfig;
    return this;
  }

  /**
   * Get runtimeConfig
   * @return runtimeConfig
   */
  @javax.annotation.Nullable
  public RuntimeConfig getRuntimeConfig() {
    return runtimeConfig;
  }

  public void setRuntimeConfig(RuntimeConfig runtimeConfig) {
    this.runtimeConfig = runtimeConfig;
  }


  public Batch runtimeInfo(RuntimeInfo runtimeInfo) {
    this.runtimeInfo = runtimeInfo;
    return this;
  }

  /**
   * Get runtimeInfo
   * @return runtimeInfo
   */
  @javax.annotation.Nullable
  public RuntimeInfo getRuntimeInfo() {
    return runtimeInfo;
  }

  public void setRuntimeInfo(RuntimeInfo runtimeInfo) {
    this.runtimeInfo = runtimeInfo;
  }


  public Batch sparkBatch(SparkBatch sparkBatch) {
    this.sparkBatch = sparkBatch;
    return this;
  }

  /**
   * Get sparkBatch
   * @return sparkBatch
   */
  @javax.annotation.Nullable
  public SparkBatch getSparkBatch() {
    return sparkBatch;
  }

  public void setSparkBatch(SparkBatch sparkBatch) {
    this.sparkBatch = sparkBatch;
  }


  public Batch sparkRBatch(SparkRBatch sparkRBatch) {
    this.sparkRBatch = sparkRBatch;
    return this;
  }

  /**
   * Get sparkRBatch
   * @return sparkRBatch
   */
  @javax.annotation.Nullable
  public SparkRBatch getSparkRBatch() {
    return sparkRBatch;
  }

  public void setSparkRBatch(SparkRBatch sparkRBatch) {
    this.sparkRBatch = sparkRBatch;
  }


  public Batch sparkSqlBatch(SparkSqlBatch sparkSqlBatch) {
    this.sparkSqlBatch = sparkSqlBatch;
    return this;
  }

  /**
   * Get sparkSqlBatch
   * @return sparkSqlBatch
   */
  @javax.annotation.Nullable
  public SparkSqlBatch getSparkSqlBatch() {
    return sparkSqlBatch;
  }

  public void setSparkSqlBatch(SparkSqlBatch sparkSqlBatch) {
    this.sparkSqlBatch = sparkSqlBatch;
  }


  /**
   * Output only. The state of the batch.
   * @return state
   */
  @javax.annotation.Nullable
  public StateEnum getState() {
    return state;
  }



  /**
   * Output only. Historical state information for the batch.
   * @return stateHistory
   */
  @javax.annotation.Nullable
  public List<StateHistory> getStateHistory() {
    return stateHistory;
  }



  /**
   * Output only. Batch state details, such as a failure description if the state is FAILED.
   * @return stateMessage
   */
  @javax.annotation.Nullable
  public String getStateMessage() {
    return stateMessage;
  }



  /**
   * Output only. The time when the batch entered a current state.
   * @return stateTime
   */
  @javax.annotation.Nullable
  public String getStateTime() {
    return stateTime;
  }



  /**
   * Output only. A batch UUID (Unique Universal Identifier). The service generates this value when it creates the batch.
   * @return uuid
   */
  @javax.annotation.Nullable
  public String getUuid() {
    return uuid;
  }




  @Override
  public boolean equals(Object o) {
    if (this == o) {
      return true;
    }
    if (o == null || getClass() != o.getClass()) {
      return false;
    }
    Batch batch = (Batch) o;
    return Objects.equals(this.createTime, batch.createTime) &&
        Objects.equals(this.creator, batch.creator) &&
        Objects.equals(this.environmentConfig, batch.environmentConfig) &&
        Objects.equals(this.labels, batch.labels) &&
        Objects.equals(this.name, batch.name) &&
        Objects.equals(this.operation, batch.operation) &&
        Objects.equals(this.pysparkBatch, batch.pysparkBatch) &&
        Objects.equals(this.runtimeConfig, batch.runtimeConfig) &&
        Objects.equals(this.runtimeInfo, batch.runtimeInfo) &&
        Objects.equals(this.sparkBatch, batch.sparkBatch) &&
        Objects.equals(this.sparkRBatch, batch.sparkRBatch) &&
        Objects.equals(this.sparkSqlBatch, batch.sparkSqlBatch) &&
        Objects.equals(this.state, batch.state) &&
        Objects.equals(this.stateHistory, batch.stateHistory) &&
        Objects.equals(this.stateMessage, batch.stateMessage) &&
        Objects.equals(this.stateTime, batch.stateTime) &&
        Objects.equals(this.uuid, batch.uuid);
  }

  @Override
  public int hashCode() {
    return Objects.hash(createTime, creator, environmentConfig, labels, name, operation, pysparkBatch, runtimeConfig, runtimeInfo, sparkBatch, sparkRBatch, sparkSqlBatch, state, stateHistory, stateMessage, stateTime, uuid);
  }

  @Override
  public String toString() {
    StringBuilder sb = new StringBuilder();
    sb.append("class Batch {\n");
    sb.append("    createTime: ").append(toIndentedString(createTime)).append("\n");
    sb.append("    creator: ").append(toIndentedString(creator)).append("\n");
    sb.append("    environmentConfig: ").append(toIndentedString(environmentConfig)).append("\n");
    sb.append("    labels: ").append(toIndentedString(labels)).append("\n");
    sb.append("    name: ").append(toIndentedString(name)).append("\n");
    sb.append("    operation: ").append(toIndentedString(operation)).append("\n");
    sb.append("    pysparkBatch: ").append(toIndentedString(pysparkBatch)).append("\n");
    sb.append("    runtimeConfig: ").append(toIndentedString(runtimeConfig)).append("\n");
    sb.append("    runtimeInfo: ").append(toIndentedString(runtimeInfo)).append("\n");
    sb.append("    sparkBatch: ").append(toIndentedString(sparkBatch)).append("\n");
    sb.append("    sparkRBatch: ").append(toIndentedString(sparkRBatch)).append("\n");
    sb.append("    sparkSqlBatch: ").append(toIndentedString(sparkSqlBatch)).append("\n");
    sb.append("    state: ").append(toIndentedString(state)).append("\n");
    sb.append("    stateHistory: ").append(toIndentedString(stateHistory)).append("\n");
    sb.append("    stateMessage: ").append(toIndentedString(stateMessage)).append("\n");
    sb.append("    stateTime: ").append(toIndentedString(stateTime)).append("\n");
    sb.append("    uuid: ").append(toIndentedString(uuid)).append("\n");
    sb.append("}");
    return sb.toString();
  }

  /**
   * Convert the given object to string with each line indented by 4 spaces
   * (except the first line).
   */
  private String toIndentedString(Object o) {
    if (o == null) {
      return "null";
    }
    return o.toString().replace("\n", "\n    ");
  }


  public static HashSet<String> openapiFields;
  public static HashSet<String> openapiRequiredFields;

  static {
    // a set of all properties/fields (JSON key names)
    openapiFields = new HashSet<String>();
    openapiFields.add("createTime");
    openapiFields.add("creator");
    openapiFields.add("environmentConfig");
    openapiFields.add("labels");
    openapiFields.add("name");
    openapiFields.add("operation");
    openapiFields.add("pysparkBatch");
    openapiFields.add("runtimeConfig");
    openapiFields.add("runtimeInfo");
    openapiFields.add("sparkBatch");
    openapiFields.add("sparkRBatch");
    openapiFields.add("sparkSqlBatch");
    openapiFields.add("state");
    openapiFields.add("stateHistory");
    openapiFields.add("stateMessage");
    openapiFields.add("stateTime");
    openapiFields.add("uuid");

    // a set of required properties/fields (JSON key names)
    openapiRequiredFields = new HashSet<String>();
  }

  /**
   * Validates the JSON Element and throws an exception if issues found
   *
   * @param jsonElement JSON Element
   * @throws IOException if the JSON Element is invalid with respect to Batch
   */
  public static void validateJsonElement(JsonElement jsonElement) throws IOException {
      if (jsonElement == null) {
        if (!Batch.openapiRequiredFields.isEmpty()) { // has required fields but JSON element is null
          throw new IllegalArgumentException(String.format("The required field(s) %s in Batch is not found in the empty JSON string", Batch.openapiRequiredFields.toString()));
        }
      }

      Set<Map.Entry<String, JsonElement>> entries = jsonElement.getAsJsonObject().entrySet();
      // check to see if the JSON string contains additional fields
      for (Map.Entry<String, JsonElement> entry : entries) {
        if (!Batch.openapiFields.contains(entry.getKey())) {
          throw new IllegalArgumentException(String.format("The field `%s` in the JSON string is not defined in the `Batch` properties. JSON: %s", entry.getKey(), jsonElement.toString()));
        }
      }
        JsonObject jsonObj = jsonElement.getAsJsonObject();
      if ((jsonObj.get("createTime") != null && !jsonObj.get("createTime").isJsonNull()) && !jsonObj.get("createTime").isJsonPrimitive()) {
        throw new IllegalArgumentException(String.format("Expected the field `createTime` to be a primitive type in the JSON string but got `%s`", jsonObj.get("createTime").toString()));
      }
      if ((jsonObj.get("creator") != null && !jsonObj.get("creator").isJsonNull()) && !jsonObj.get("creator").isJsonPrimitive()) {
        throw new IllegalArgumentException(String.format("Expected the field `creator` to be a primitive type in the JSON string but got `%s`", jsonObj.get("creator").toString()));
      }
      // validate the optional field `environmentConfig`
      if (jsonObj.get("environmentConfig") != null && !jsonObj.get("environmentConfig").isJsonNull()) {
        EnvironmentConfig.validateJsonElement(jsonObj.get("environmentConfig"));
      }
      if ((jsonObj.get("name") != null && !jsonObj.get("name").isJsonNull()) && !jsonObj.get("name").isJsonPrimitive()) {
        throw new IllegalArgumentException(String.format("Expected the field `name` to be a primitive type in the JSON string but got `%s`", jsonObj.get("name").toString()));
      }
      if ((jsonObj.get("operation") != null && !jsonObj.get("operation").isJsonNull()) && !jsonObj.get("operation").isJsonPrimitive()) {
        throw new IllegalArgumentException(String.format("Expected the field `operation` to be a primitive type in the JSON string but got `%s`", jsonObj.get("operation").toString()));
      }
      // validate the optional field `pysparkBatch`
      if (jsonObj.get("pysparkBatch") != null && !jsonObj.get("pysparkBatch").isJsonNull()) {
        PySparkBatch.validateJsonElement(jsonObj.get("pysparkBatch"));
      }
      // validate the optional field `runtimeConfig`
      if (jsonObj.get("runtimeConfig") != null && !jsonObj.get("runtimeConfig").isJsonNull()) {
        RuntimeConfig.validateJsonElement(jsonObj.get("runtimeConfig"));
      }
      // validate the optional field `runtimeInfo`
      if (jsonObj.get("runtimeInfo") != null && !jsonObj.get("runtimeInfo").isJsonNull()) {
        RuntimeInfo.validateJsonElement(jsonObj.get("runtimeInfo"));
      }
      // validate the optional field `sparkBatch`
      if (jsonObj.get("sparkBatch") != null && !jsonObj.get("sparkBatch").isJsonNull()) {
        SparkBatch.validateJsonElement(jsonObj.get("sparkBatch"));
      }
      // validate the optional field `sparkRBatch`
      if (jsonObj.get("sparkRBatch") != null && !jsonObj.get("sparkRBatch").isJsonNull()) {
        SparkRBatch.validateJsonElement(jsonObj.get("sparkRBatch"));
      }
      // validate the optional field `sparkSqlBatch`
      if (jsonObj.get("sparkSqlBatch") != null && !jsonObj.get("sparkSqlBatch").isJsonNull()) {
        SparkSqlBatch.validateJsonElement(jsonObj.get("sparkSqlBatch"));
      }
      if ((jsonObj.get("state") != null && !jsonObj.get("state").isJsonNull()) && !jsonObj.get("state").isJsonPrimitive()) {
        throw new IllegalArgumentException(String.format("Expected the field `state` to be a primitive type in the JSON string but got `%s`", jsonObj.get("state").toString()));
      }
      // validate the optional field `state`
      if (jsonObj.get("state") != null && !jsonObj.get("state").isJsonNull()) {
        StateEnum.validateJsonElement(jsonObj.get("state"));
      }
      if (jsonObj.get("stateHistory") != null && !jsonObj.get("stateHistory").isJsonNull()) {
        JsonArray jsonArraystateHistory = jsonObj.getAsJsonArray("stateHistory");
        if (jsonArraystateHistory != null) {
          // ensure the json data is an array
          if (!jsonObj.get("stateHistory").isJsonArray()) {
            throw new IllegalArgumentException(String.format("Expected the field `stateHistory` to be an array in the JSON string but got `%s`", jsonObj.get("stateHistory").toString()));
          }

          // validate the optional field `stateHistory` (array)
          for (int i = 0; i < jsonArraystateHistory.size(); i++) {
            StateHistory.validateJsonElement(jsonArraystateHistory.get(i));
          };
        }
      }
      if ((jsonObj.get("stateMessage") != null && !jsonObj.get("stateMessage").isJsonNull()) && !jsonObj.get("stateMessage").isJsonPrimitive()) {
        throw new IllegalArgumentException(String.format("Expected the field `stateMessage` to be a primitive type in the JSON string but got `%s`", jsonObj.get("stateMessage").toString()));
      }
      if ((jsonObj.get("stateTime") != null && !jsonObj.get("stateTime").isJsonNull()) && !jsonObj.get("stateTime").isJsonPrimitive()) {
        throw new IllegalArgumentException(String.format("Expected the field `stateTime` to be a primitive type in the JSON string but got `%s`", jsonObj.get("stateTime").toString()));
      }
      if ((jsonObj.get("uuid") != null && !jsonObj.get("uuid").isJsonNull()) && !jsonObj.get("uuid").isJsonPrimitive()) {
        throw new IllegalArgumentException(String.format("Expected the field `uuid` to be a primitive type in the JSON string but got `%s`", jsonObj.get("uuid").toString()));
      }
  }

  public static class CustomTypeAdapterFactory implements TypeAdapterFactory {
    @SuppressWarnings("unchecked")
    @Override
    public <T> TypeAdapter<T> create(Gson gson, TypeToken<T> type) {
       if (!Batch.class.isAssignableFrom(type.getRawType())) {
         return null; // this class only serializes 'Batch' and its subtypes
       }
       final TypeAdapter<JsonElement> elementAdapter = gson.getAdapter(JsonElement.class);
       final TypeAdapter<Batch> thisAdapter
                        = gson.getDelegateAdapter(this, TypeToken.get(Batch.class));

       return (TypeAdapter<T>) new TypeAdapter<Batch>() {
           @Override
           public void write(JsonWriter out, Batch value) throws IOException {
             JsonObject obj = thisAdapter.toJsonTree(value).getAsJsonObject();
             elementAdapter.write(out, obj);
           }

           @Override
           public Batch read(JsonReader in) throws IOException {
             JsonElement jsonElement = elementAdapter.read(in);
             validateJsonElement(jsonElement);
             return thisAdapter.fromJsonTree(jsonElement);
           }

       }.nullSafe();
    }
  }

  /**
   * Create an instance of Batch given an JSON string
   *
   * @param jsonString JSON string
   * @return An instance of Batch
   * @throws IOException if the JSON string is invalid with respect to Batch
   */
  public static Batch fromJson(String jsonString) throws IOException {
    return JSON.getGson().fromJson(jsonString, Batch.class);
  }

  /**
   * Convert an instance of Batch to an JSON string
   *
   * @return JSON string
   */
  public String toJson() {
    return JSON.getGson().toJson(this);
  }
}

