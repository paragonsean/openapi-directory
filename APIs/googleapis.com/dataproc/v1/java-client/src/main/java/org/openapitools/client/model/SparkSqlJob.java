/*
 * Cloud Dataproc API
 * Manages Hadoop-based clusters and jobs on Google Cloud Platform.
 *
 * The version of the OpenAPI document: v1
 * 
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */


package org.openapitools.client.model;

import java.util.Objects;
import com.google.gson.TypeAdapter;
import com.google.gson.annotations.JsonAdapter;
import com.google.gson.annotations.SerializedName;
import com.google.gson.stream.JsonReader;
import com.google.gson.stream.JsonWriter;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import org.openapitools.client.model.LoggingConfig;
import org.openapitools.client.model.QueryList;

import com.google.gson.Gson;
import com.google.gson.GsonBuilder;
import com.google.gson.JsonArray;
import com.google.gson.JsonDeserializationContext;
import com.google.gson.JsonDeserializer;
import com.google.gson.JsonElement;
import com.google.gson.JsonObject;
import com.google.gson.JsonParseException;
import com.google.gson.TypeAdapterFactory;
import com.google.gson.reflect.TypeToken;
import com.google.gson.TypeAdapter;
import com.google.gson.stream.JsonReader;
import com.google.gson.stream.JsonWriter;
import java.io.IOException;

import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;

import org.openapitools.client.JSON;

/**
 * A Dataproc job for running Apache Spark SQL (https://spark.apache.org/sql/) queries.
 */
@javax.annotation.Generated(value = "org.openapitools.codegen.languages.JavaClientCodegen", date = "2024-10-12T11:49:50.925918-04:00[America/New_York]", comments = "Generator version: 7.9.0")
public class SparkSqlJob {
  public static final String SERIALIZED_NAME_JAR_FILE_URIS = "jarFileUris";
  @SerializedName(SERIALIZED_NAME_JAR_FILE_URIS)
  private List<String> jarFileUris = new ArrayList<>();

  public static final String SERIALIZED_NAME_LOGGING_CONFIG = "loggingConfig";
  @SerializedName(SERIALIZED_NAME_LOGGING_CONFIG)
  private LoggingConfig loggingConfig;

  public static final String SERIALIZED_NAME_PROPERTIES = "properties";
  @SerializedName(SERIALIZED_NAME_PROPERTIES)
  private Map<String, String> properties = new HashMap<>();

  public static final String SERIALIZED_NAME_QUERY_FILE_URI = "queryFileUri";
  @SerializedName(SERIALIZED_NAME_QUERY_FILE_URI)
  private String queryFileUri;

  public static final String SERIALIZED_NAME_QUERY_LIST = "queryList";
  @SerializedName(SERIALIZED_NAME_QUERY_LIST)
  private QueryList queryList;

  public static final String SERIALIZED_NAME_SCRIPT_VARIABLES = "scriptVariables";
  @SerializedName(SERIALIZED_NAME_SCRIPT_VARIABLES)
  private Map<String, String> scriptVariables = new HashMap<>();

  public SparkSqlJob() {
  }

  public SparkSqlJob jarFileUris(List<String> jarFileUris) {
    this.jarFileUris = jarFileUris;
    return this;
  }

  public SparkSqlJob addJarFileUrisItem(String jarFileUrisItem) {
    if (this.jarFileUris == null) {
      this.jarFileUris = new ArrayList<>();
    }
    this.jarFileUris.add(jarFileUrisItem);
    return this;
  }

  /**
   * Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
   * @return jarFileUris
   */
  @javax.annotation.Nullable
  public List<String> getJarFileUris() {
    return jarFileUris;
  }

  public void setJarFileUris(List<String> jarFileUris) {
    this.jarFileUris = jarFileUris;
  }


  public SparkSqlJob loggingConfig(LoggingConfig loggingConfig) {
    this.loggingConfig = loggingConfig;
    return this;
  }

  /**
   * Get loggingConfig
   * @return loggingConfig
   */
  @javax.annotation.Nullable
  public LoggingConfig getLoggingConfig() {
    return loggingConfig;
  }

  public void setLoggingConfig(LoggingConfig loggingConfig) {
    this.loggingConfig = loggingConfig;
  }


  public SparkSqlJob properties(Map<String, String> properties) {
    this.properties = properties;
    return this;
  }

  public SparkSqlJob putPropertiesItem(String key, String propertiesItem) {
    if (this.properties == null) {
      this.properties = new HashMap<>();
    }
    this.properties.put(key, propertiesItem);
    return this;
  }

  /**
   * Optional. A mapping of property names to values, used to configure Spark SQL&#39;s SparkConf. Properties that conflict with values set by the Dataproc API might be overwritten.
   * @return properties
   */
  @javax.annotation.Nullable
  public Map<String, String> getProperties() {
    return properties;
  }

  public void setProperties(Map<String, String> properties) {
    this.properties = properties;
  }


  public SparkSqlJob queryFileUri(String queryFileUri) {
    this.queryFileUri = queryFileUri;
    return this;
  }

  /**
   * The HCFS URI of the script that contains SQL queries.
   * @return queryFileUri
   */
  @javax.annotation.Nullable
  public String getQueryFileUri() {
    return queryFileUri;
  }

  public void setQueryFileUri(String queryFileUri) {
    this.queryFileUri = queryFileUri;
  }


  public SparkSqlJob queryList(QueryList queryList) {
    this.queryList = queryList;
    return this;
  }

  /**
   * Get queryList
   * @return queryList
   */
  @javax.annotation.Nullable
  public QueryList getQueryList() {
    return queryList;
  }

  public void setQueryList(QueryList queryList) {
    this.queryList = queryList;
  }


  public SparkSqlJob scriptVariables(Map<String, String> scriptVariables) {
    this.scriptVariables = scriptVariables;
    return this;
  }

  public SparkSqlJob putScriptVariablesItem(String key, String scriptVariablesItem) {
    if (this.scriptVariables == null) {
      this.scriptVariables = new HashMap<>();
    }
    this.scriptVariables.put(key, scriptVariablesItem);
    return this;
  }

  /**
   * Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: SET name&#x3D;\&quot;value\&quot;;).
   * @return scriptVariables
   */
  @javax.annotation.Nullable
  public Map<String, String> getScriptVariables() {
    return scriptVariables;
  }

  public void setScriptVariables(Map<String, String> scriptVariables) {
    this.scriptVariables = scriptVariables;
  }



  @Override
  public boolean equals(Object o) {
    if (this == o) {
      return true;
    }
    if (o == null || getClass() != o.getClass()) {
      return false;
    }
    SparkSqlJob sparkSqlJob = (SparkSqlJob) o;
    return Objects.equals(this.jarFileUris, sparkSqlJob.jarFileUris) &&
        Objects.equals(this.loggingConfig, sparkSqlJob.loggingConfig) &&
        Objects.equals(this.properties, sparkSqlJob.properties) &&
        Objects.equals(this.queryFileUri, sparkSqlJob.queryFileUri) &&
        Objects.equals(this.queryList, sparkSqlJob.queryList) &&
        Objects.equals(this.scriptVariables, sparkSqlJob.scriptVariables);
  }

  @Override
  public int hashCode() {
    return Objects.hash(jarFileUris, loggingConfig, properties, queryFileUri, queryList, scriptVariables);
  }

  @Override
  public String toString() {
    StringBuilder sb = new StringBuilder();
    sb.append("class SparkSqlJob {\n");
    sb.append("    jarFileUris: ").append(toIndentedString(jarFileUris)).append("\n");
    sb.append("    loggingConfig: ").append(toIndentedString(loggingConfig)).append("\n");
    sb.append("    properties: ").append(toIndentedString(properties)).append("\n");
    sb.append("    queryFileUri: ").append(toIndentedString(queryFileUri)).append("\n");
    sb.append("    queryList: ").append(toIndentedString(queryList)).append("\n");
    sb.append("    scriptVariables: ").append(toIndentedString(scriptVariables)).append("\n");
    sb.append("}");
    return sb.toString();
  }

  /**
   * Convert the given object to string with each line indented by 4 spaces
   * (except the first line).
   */
  private String toIndentedString(Object o) {
    if (o == null) {
      return "null";
    }
    return o.toString().replace("\n", "\n    ");
  }


  public static HashSet<String> openapiFields;
  public static HashSet<String> openapiRequiredFields;

  static {
    // a set of all properties/fields (JSON key names)
    openapiFields = new HashSet<String>();
    openapiFields.add("jarFileUris");
    openapiFields.add("loggingConfig");
    openapiFields.add("properties");
    openapiFields.add("queryFileUri");
    openapiFields.add("queryList");
    openapiFields.add("scriptVariables");

    // a set of required properties/fields (JSON key names)
    openapiRequiredFields = new HashSet<String>();
  }

  /**
   * Validates the JSON Element and throws an exception if issues found
   *
   * @param jsonElement JSON Element
   * @throws IOException if the JSON Element is invalid with respect to SparkSqlJob
   */
  public static void validateJsonElement(JsonElement jsonElement) throws IOException {
      if (jsonElement == null) {
        if (!SparkSqlJob.openapiRequiredFields.isEmpty()) { // has required fields but JSON element is null
          throw new IllegalArgumentException(String.format("The required field(s) %s in SparkSqlJob is not found in the empty JSON string", SparkSqlJob.openapiRequiredFields.toString()));
        }
      }

      Set<Map.Entry<String, JsonElement>> entries = jsonElement.getAsJsonObject().entrySet();
      // check to see if the JSON string contains additional fields
      for (Map.Entry<String, JsonElement> entry : entries) {
        if (!SparkSqlJob.openapiFields.contains(entry.getKey())) {
          throw new IllegalArgumentException(String.format("The field `%s` in the JSON string is not defined in the `SparkSqlJob` properties. JSON: %s", entry.getKey(), jsonElement.toString()));
        }
      }
        JsonObject jsonObj = jsonElement.getAsJsonObject();
      // ensure the optional json data is an array if present
      if (jsonObj.get("jarFileUris") != null && !jsonObj.get("jarFileUris").isJsonNull() && !jsonObj.get("jarFileUris").isJsonArray()) {
        throw new IllegalArgumentException(String.format("Expected the field `jarFileUris` to be an array in the JSON string but got `%s`", jsonObj.get("jarFileUris").toString()));
      }
      // validate the optional field `loggingConfig`
      if (jsonObj.get("loggingConfig") != null && !jsonObj.get("loggingConfig").isJsonNull()) {
        LoggingConfig.validateJsonElement(jsonObj.get("loggingConfig"));
      }
      if ((jsonObj.get("queryFileUri") != null && !jsonObj.get("queryFileUri").isJsonNull()) && !jsonObj.get("queryFileUri").isJsonPrimitive()) {
        throw new IllegalArgumentException(String.format("Expected the field `queryFileUri` to be a primitive type in the JSON string but got `%s`", jsonObj.get("queryFileUri").toString()));
      }
      // validate the optional field `queryList`
      if (jsonObj.get("queryList") != null && !jsonObj.get("queryList").isJsonNull()) {
        QueryList.validateJsonElement(jsonObj.get("queryList"));
      }
  }

  public static class CustomTypeAdapterFactory implements TypeAdapterFactory {
    @SuppressWarnings("unchecked")
    @Override
    public <T> TypeAdapter<T> create(Gson gson, TypeToken<T> type) {
       if (!SparkSqlJob.class.isAssignableFrom(type.getRawType())) {
         return null; // this class only serializes 'SparkSqlJob' and its subtypes
       }
       final TypeAdapter<JsonElement> elementAdapter = gson.getAdapter(JsonElement.class);
       final TypeAdapter<SparkSqlJob> thisAdapter
                        = gson.getDelegateAdapter(this, TypeToken.get(SparkSqlJob.class));

       return (TypeAdapter<T>) new TypeAdapter<SparkSqlJob>() {
           @Override
           public void write(JsonWriter out, SparkSqlJob value) throws IOException {
             JsonObject obj = thisAdapter.toJsonTree(value).getAsJsonObject();
             elementAdapter.write(out, obj);
           }

           @Override
           public SparkSqlJob read(JsonReader in) throws IOException {
             JsonElement jsonElement = elementAdapter.read(in);
             validateJsonElement(jsonElement);
             return thisAdapter.fromJsonTree(jsonElement);
           }

       }.nullSafe();
    }
  }

  /**
   * Create an instance of SparkSqlJob given an JSON string
   *
   * @param jsonString JSON string
   * @return An instance of SparkSqlJob
   * @throws IOException if the JSON string is invalid with respect to SparkSqlJob
   */
  public static SparkSqlJob fromJson(String jsonString) throws IOException {
    return JSON.getGson().fromJson(jsonString, SparkSqlJob.class);
  }

  /**
   * Convert an instance of SparkSqlJob to an JSON string
   *
   * @return JSON string
   */
  public String toJson() {
    return JSON.getGson().toJson(this);
  }
}

