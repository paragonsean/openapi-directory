/*
 * Cloud Dataproc API
 * Manages Hadoop-based clusters and jobs on Google Cloud Platform.
 *
 * The version of the OpenAPI document: v1
 * 
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */


package org.openapitools.client.model;

import java.util.Objects;
import com.google.gson.TypeAdapter;
import com.google.gson.annotations.JsonAdapter;
import com.google.gson.annotations.SerializedName;
import com.google.gson.stream.JsonReader;
import com.google.gson.stream.JsonWriter;
import java.io.IOException;
import java.util.Arrays;

import com.google.gson.Gson;
import com.google.gson.GsonBuilder;
import com.google.gson.JsonArray;
import com.google.gson.JsonDeserializationContext;
import com.google.gson.JsonDeserializer;
import com.google.gson.JsonElement;
import com.google.gson.JsonObject;
import com.google.gson.JsonParseException;
import com.google.gson.TypeAdapterFactory;
import com.google.gson.reflect.TypeToken;
import com.google.gson.TypeAdapter;
import com.google.gson.stream.JsonReader;
import com.google.gson.stream.JsonWriter;
import java.io.IOException;

import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;

import org.openapitools.client.JSON;

/**
 * Basic autoscaling configurations for Spark Standalone.
 */
@javax.annotation.Generated(value = "org.openapitools.codegen.languages.JavaClientCodegen", date = "2024-10-12T11:49:50.925918-04:00[America/New_York]", comments = "Generator version: 7.9.0")
public class SparkStandaloneAutoscalingConfig {
  public static final String SERIALIZED_NAME_GRACEFUL_DECOMMISSION_TIMEOUT = "gracefulDecommissionTimeout";
  @SerializedName(SERIALIZED_NAME_GRACEFUL_DECOMMISSION_TIMEOUT)
  private String gracefulDecommissionTimeout;

  public static final String SERIALIZED_NAME_REMOVE_ONLY_IDLE_WORKERS = "removeOnlyIdleWorkers";
  @SerializedName(SERIALIZED_NAME_REMOVE_ONLY_IDLE_WORKERS)
  private Boolean removeOnlyIdleWorkers;

  public static final String SERIALIZED_NAME_SCALE_DOWN_FACTOR = "scaleDownFactor";
  @SerializedName(SERIALIZED_NAME_SCALE_DOWN_FACTOR)
  private Double scaleDownFactor;

  public static final String SERIALIZED_NAME_SCALE_DOWN_MIN_WORKER_FRACTION = "scaleDownMinWorkerFraction";
  @SerializedName(SERIALIZED_NAME_SCALE_DOWN_MIN_WORKER_FRACTION)
  private Double scaleDownMinWorkerFraction;

  public static final String SERIALIZED_NAME_SCALE_UP_FACTOR = "scaleUpFactor";
  @SerializedName(SERIALIZED_NAME_SCALE_UP_FACTOR)
  private Double scaleUpFactor;

  public static final String SERIALIZED_NAME_SCALE_UP_MIN_WORKER_FRACTION = "scaleUpMinWorkerFraction";
  @SerializedName(SERIALIZED_NAME_SCALE_UP_MIN_WORKER_FRACTION)
  private Double scaleUpMinWorkerFraction;

  public SparkStandaloneAutoscalingConfig() {
  }

  public SparkStandaloneAutoscalingConfig gracefulDecommissionTimeout(String gracefulDecommissionTimeout) {
    this.gracefulDecommissionTimeout = gracefulDecommissionTimeout;
    return this;
  }

  /**
   * Required. Timeout for Spark graceful decommissioning of spark workers. Specifies the duration to wait for spark worker to complete spark decommissioning tasks before forcefully removing workers. Only applicable to downscaling operations.Bounds: 0s, 1d.
   * @return gracefulDecommissionTimeout
   */
  @javax.annotation.Nullable
  public String getGracefulDecommissionTimeout() {
    return gracefulDecommissionTimeout;
  }

  public void setGracefulDecommissionTimeout(String gracefulDecommissionTimeout) {
    this.gracefulDecommissionTimeout = gracefulDecommissionTimeout;
  }


  public SparkStandaloneAutoscalingConfig removeOnlyIdleWorkers(Boolean removeOnlyIdleWorkers) {
    this.removeOnlyIdleWorkers = removeOnlyIdleWorkers;
    return this;
  }

  /**
   * Optional. Remove only idle workers when scaling down cluster
   * @return removeOnlyIdleWorkers
   */
  @javax.annotation.Nullable
  public Boolean getRemoveOnlyIdleWorkers() {
    return removeOnlyIdleWorkers;
  }

  public void setRemoveOnlyIdleWorkers(Boolean removeOnlyIdleWorkers) {
    this.removeOnlyIdleWorkers = removeOnlyIdleWorkers;
  }


  public SparkStandaloneAutoscalingConfig scaleDownFactor(Double scaleDownFactor) {
    this.scaleDownFactor = scaleDownFactor;
    return this;
  }

  /**
   * Required. Fraction of required executors to remove from Spark Serverless clusters. A scale-down factor of 1.0 will result in scaling down so that there are no more executors for the Spark Job.(more aggressive scaling). A scale-down factor closer to 0 will result in a smaller magnitude of scaling donw (less aggressive scaling).Bounds: 0.0, 1.0.
   * @return scaleDownFactor
   */
  @javax.annotation.Nullable
  public Double getScaleDownFactor() {
    return scaleDownFactor;
  }

  public void setScaleDownFactor(Double scaleDownFactor) {
    this.scaleDownFactor = scaleDownFactor;
  }


  public SparkStandaloneAutoscalingConfig scaleDownMinWorkerFraction(Double scaleDownMinWorkerFraction) {
    this.scaleDownMinWorkerFraction = scaleDownMinWorkerFraction;
    return this;
  }

  /**
   * Optional. Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
   * @return scaleDownMinWorkerFraction
   */
  @javax.annotation.Nullable
  public Double getScaleDownMinWorkerFraction() {
    return scaleDownMinWorkerFraction;
  }

  public void setScaleDownMinWorkerFraction(Double scaleDownMinWorkerFraction) {
    this.scaleDownMinWorkerFraction = scaleDownMinWorkerFraction;
  }


  public SparkStandaloneAutoscalingConfig scaleUpFactor(Double scaleUpFactor) {
    this.scaleUpFactor = scaleUpFactor;
    return this;
  }

  /**
   * Required. Fraction of required workers to add to Spark Standalone clusters. A scale-up factor of 1.0 will result in scaling up so that there are no more required workers for the Spark Job (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling).Bounds: 0.0, 1.0.
   * @return scaleUpFactor
   */
  @javax.annotation.Nullable
  public Double getScaleUpFactor() {
    return scaleUpFactor;
  }

  public void setScaleUpFactor(Double scaleUpFactor) {
    this.scaleUpFactor = scaleUpFactor;
  }


  public SparkStandaloneAutoscalingConfig scaleUpMinWorkerFraction(Double scaleUpMinWorkerFraction) {
    this.scaleUpMinWorkerFraction = scaleUpMinWorkerFraction;
    return this;
  }

  /**
   * Optional. Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
   * @return scaleUpMinWorkerFraction
   */
  @javax.annotation.Nullable
  public Double getScaleUpMinWorkerFraction() {
    return scaleUpMinWorkerFraction;
  }

  public void setScaleUpMinWorkerFraction(Double scaleUpMinWorkerFraction) {
    this.scaleUpMinWorkerFraction = scaleUpMinWorkerFraction;
  }



  @Override
  public boolean equals(Object o) {
    if (this == o) {
      return true;
    }
    if (o == null || getClass() != o.getClass()) {
      return false;
    }
    SparkStandaloneAutoscalingConfig sparkStandaloneAutoscalingConfig = (SparkStandaloneAutoscalingConfig) o;
    return Objects.equals(this.gracefulDecommissionTimeout, sparkStandaloneAutoscalingConfig.gracefulDecommissionTimeout) &&
        Objects.equals(this.removeOnlyIdleWorkers, sparkStandaloneAutoscalingConfig.removeOnlyIdleWorkers) &&
        Objects.equals(this.scaleDownFactor, sparkStandaloneAutoscalingConfig.scaleDownFactor) &&
        Objects.equals(this.scaleDownMinWorkerFraction, sparkStandaloneAutoscalingConfig.scaleDownMinWorkerFraction) &&
        Objects.equals(this.scaleUpFactor, sparkStandaloneAutoscalingConfig.scaleUpFactor) &&
        Objects.equals(this.scaleUpMinWorkerFraction, sparkStandaloneAutoscalingConfig.scaleUpMinWorkerFraction);
  }

  @Override
  public int hashCode() {
    return Objects.hash(gracefulDecommissionTimeout, removeOnlyIdleWorkers, scaleDownFactor, scaleDownMinWorkerFraction, scaleUpFactor, scaleUpMinWorkerFraction);
  }

  @Override
  public String toString() {
    StringBuilder sb = new StringBuilder();
    sb.append("class SparkStandaloneAutoscalingConfig {\n");
    sb.append("    gracefulDecommissionTimeout: ").append(toIndentedString(gracefulDecommissionTimeout)).append("\n");
    sb.append("    removeOnlyIdleWorkers: ").append(toIndentedString(removeOnlyIdleWorkers)).append("\n");
    sb.append("    scaleDownFactor: ").append(toIndentedString(scaleDownFactor)).append("\n");
    sb.append("    scaleDownMinWorkerFraction: ").append(toIndentedString(scaleDownMinWorkerFraction)).append("\n");
    sb.append("    scaleUpFactor: ").append(toIndentedString(scaleUpFactor)).append("\n");
    sb.append("    scaleUpMinWorkerFraction: ").append(toIndentedString(scaleUpMinWorkerFraction)).append("\n");
    sb.append("}");
    return sb.toString();
  }

  /**
   * Convert the given object to string with each line indented by 4 spaces
   * (except the first line).
   */
  private String toIndentedString(Object o) {
    if (o == null) {
      return "null";
    }
    return o.toString().replace("\n", "\n    ");
  }


  public static HashSet<String> openapiFields;
  public static HashSet<String> openapiRequiredFields;

  static {
    // a set of all properties/fields (JSON key names)
    openapiFields = new HashSet<String>();
    openapiFields.add("gracefulDecommissionTimeout");
    openapiFields.add("removeOnlyIdleWorkers");
    openapiFields.add("scaleDownFactor");
    openapiFields.add("scaleDownMinWorkerFraction");
    openapiFields.add("scaleUpFactor");
    openapiFields.add("scaleUpMinWorkerFraction");

    // a set of required properties/fields (JSON key names)
    openapiRequiredFields = new HashSet<String>();
  }

  /**
   * Validates the JSON Element and throws an exception if issues found
   *
   * @param jsonElement JSON Element
   * @throws IOException if the JSON Element is invalid with respect to SparkStandaloneAutoscalingConfig
   */
  public static void validateJsonElement(JsonElement jsonElement) throws IOException {
      if (jsonElement == null) {
        if (!SparkStandaloneAutoscalingConfig.openapiRequiredFields.isEmpty()) { // has required fields but JSON element is null
          throw new IllegalArgumentException(String.format("The required field(s) %s in SparkStandaloneAutoscalingConfig is not found in the empty JSON string", SparkStandaloneAutoscalingConfig.openapiRequiredFields.toString()));
        }
      }

      Set<Map.Entry<String, JsonElement>> entries = jsonElement.getAsJsonObject().entrySet();
      // check to see if the JSON string contains additional fields
      for (Map.Entry<String, JsonElement> entry : entries) {
        if (!SparkStandaloneAutoscalingConfig.openapiFields.contains(entry.getKey())) {
          throw new IllegalArgumentException(String.format("The field `%s` in the JSON string is not defined in the `SparkStandaloneAutoscalingConfig` properties. JSON: %s", entry.getKey(), jsonElement.toString()));
        }
      }
        JsonObject jsonObj = jsonElement.getAsJsonObject();
      if ((jsonObj.get("gracefulDecommissionTimeout") != null && !jsonObj.get("gracefulDecommissionTimeout").isJsonNull()) && !jsonObj.get("gracefulDecommissionTimeout").isJsonPrimitive()) {
        throw new IllegalArgumentException(String.format("Expected the field `gracefulDecommissionTimeout` to be a primitive type in the JSON string but got `%s`", jsonObj.get("gracefulDecommissionTimeout").toString()));
      }
  }

  public static class CustomTypeAdapterFactory implements TypeAdapterFactory {
    @SuppressWarnings("unchecked")
    @Override
    public <T> TypeAdapter<T> create(Gson gson, TypeToken<T> type) {
       if (!SparkStandaloneAutoscalingConfig.class.isAssignableFrom(type.getRawType())) {
         return null; // this class only serializes 'SparkStandaloneAutoscalingConfig' and its subtypes
       }
       final TypeAdapter<JsonElement> elementAdapter = gson.getAdapter(JsonElement.class);
       final TypeAdapter<SparkStandaloneAutoscalingConfig> thisAdapter
                        = gson.getDelegateAdapter(this, TypeToken.get(SparkStandaloneAutoscalingConfig.class));

       return (TypeAdapter<T>) new TypeAdapter<SparkStandaloneAutoscalingConfig>() {
           @Override
           public void write(JsonWriter out, SparkStandaloneAutoscalingConfig value) throws IOException {
             JsonObject obj = thisAdapter.toJsonTree(value).getAsJsonObject();
             elementAdapter.write(out, obj);
           }

           @Override
           public SparkStandaloneAutoscalingConfig read(JsonReader in) throws IOException {
             JsonElement jsonElement = elementAdapter.read(in);
             validateJsonElement(jsonElement);
             return thisAdapter.fromJsonTree(jsonElement);
           }

       }.nullSafe();
    }
  }

  /**
   * Create an instance of SparkStandaloneAutoscalingConfig given an JSON string
   *
   * @param jsonString JSON string
   * @return An instance of SparkStandaloneAutoscalingConfig
   * @throws IOException if the JSON string is invalid with respect to SparkStandaloneAutoscalingConfig
   */
  public static SparkStandaloneAutoscalingConfig fromJson(String jsonString) throws IOException {
    return JSON.getGson().fromJson(jsonString, SparkStandaloneAutoscalingConfig.class);
  }

  /**
   * Convert an instance of SparkStandaloneAutoscalingConfig to an JSON string
   *
   * @return JSON string
   */
  public String toJson() {
    return JSON.getGson().toJson(this);
  }
}

