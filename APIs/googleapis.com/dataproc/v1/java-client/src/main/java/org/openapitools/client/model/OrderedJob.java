/*
 * Cloud Dataproc API
 * Manages Hadoop-based clusters and jobs on Google Cloud Platform.
 *
 * The version of the OpenAPI document: v1
 * 
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */


package org.openapitools.client.model;

import java.util.Objects;
import com.google.gson.TypeAdapter;
import com.google.gson.annotations.JsonAdapter;
import com.google.gson.annotations.SerializedName;
import com.google.gson.stream.JsonReader;
import com.google.gson.stream.JsonWriter;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import org.openapitools.client.model.FlinkJob;
import org.openapitools.client.model.HadoopJob;
import org.openapitools.client.model.HiveJob;
import org.openapitools.client.model.JobScheduling;
import org.openapitools.client.model.PigJob;
import org.openapitools.client.model.PrestoJob;
import org.openapitools.client.model.PySparkJob;
import org.openapitools.client.model.SparkJob;
import org.openapitools.client.model.SparkRJob;
import org.openapitools.client.model.SparkSqlJob;
import org.openapitools.client.model.TrinoJob;

import com.google.gson.Gson;
import com.google.gson.GsonBuilder;
import com.google.gson.JsonArray;
import com.google.gson.JsonDeserializationContext;
import com.google.gson.JsonDeserializer;
import com.google.gson.JsonElement;
import com.google.gson.JsonObject;
import com.google.gson.JsonParseException;
import com.google.gson.TypeAdapterFactory;
import com.google.gson.reflect.TypeToken;
import com.google.gson.TypeAdapter;
import com.google.gson.stream.JsonReader;
import com.google.gson.stream.JsonWriter;
import java.io.IOException;

import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;

import org.openapitools.client.JSON;

/**
 * A job executed by the workflow.
 */
@javax.annotation.Generated(value = "org.openapitools.codegen.languages.JavaClientCodegen", date = "2024-10-12T11:49:50.925918-04:00[America/New_York]", comments = "Generator version: 7.9.0")
public class OrderedJob {
  public static final String SERIALIZED_NAME_FLINK_JOB = "flinkJob";
  @SerializedName(SERIALIZED_NAME_FLINK_JOB)
  private FlinkJob flinkJob;

  public static final String SERIALIZED_NAME_HADOOP_JOB = "hadoopJob";
  @SerializedName(SERIALIZED_NAME_HADOOP_JOB)
  private HadoopJob hadoopJob;

  public static final String SERIALIZED_NAME_HIVE_JOB = "hiveJob";
  @SerializedName(SERIALIZED_NAME_HIVE_JOB)
  private HiveJob hiveJob;

  public static final String SERIALIZED_NAME_LABELS = "labels";
  @SerializedName(SERIALIZED_NAME_LABELS)
  private Map<String, String> labels = new HashMap<>();

  public static final String SERIALIZED_NAME_PIG_JOB = "pigJob";
  @SerializedName(SERIALIZED_NAME_PIG_JOB)
  private PigJob pigJob;

  public static final String SERIALIZED_NAME_PREREQUISITE_STEP_IDS = "prerequisiteStepIds";
  @SerializedName(SERIALIZED_NAME_PREREQUISITE_STEP_IDS)
  private List<String> prerequisiteStepIds = new ArrayList<>();

  public static final String SERIALIZED_NAME_PRESTO_JOB = "prestoJob";
  @SerializedName(SERIALIZED_NAME_PRESTO_JOB)
  private PrestoJob prestoJob;

  public static final String SERIALIZED_NAME_PYSPARK_JOB = "pysparkJob";
  @SerializedName(SERIALIZED_NAME_PYSPARK_JOB)
  private PySparkJob pysparkJob;

  public static final String SERIALIZED_NAME_SCHEDULING = "scheduling";
  @SerializedName(SERIALIZED_NAME_SCHEDULING)
  private JobScheduling scheduling;

  public static final String SERIALIZED_NAME_SPARK_JOB = "sparkJob";
  @SerializedName(SERIALIZED_NAME_SPARK_JOB)
  private SparkJob sparkJob;

  public static final String SERIALIZED_NAME_SPARK_R_JOB = "sparkRJob";
  @SerializedName(SERIALIZED_NAME_SPARK_R_JOB)
  private SparkRJob sparkRJob;

  public static final String SERIALIZED_NAME_SPARK_SQL_JOB = "sparkSqlJob";
  @SerializedName(SERIALIZED_NAME_SPARK_SQL_JOB)
  private SparkSqlJob sparkSqlJob;

  public static final String SERIALIZED_NAME_STEP_ID = "stepId";
  @SerializedName(SERIALIZED_NAME_STEP_ID)
  private String stepId;

  public static final String SERIALIZED_NAME_TRINO_JOB = "trinoJob";
  @SerializedName(SERIALIZED_NAME_TRINO_JOB)
  private TrinoJob trinoJob;

  public OrderedJob() {
  }

  public OrderedJob flinkJob(FlinkJob flinkJob) {
    this.flinkJob = flinkJob;
    return this;
  }

  /**
   * Get flinkJob
   * @return flinkJob
   */
  @javax.annotation.Nullable
  public FlinkJob getFlinkJob() {
    return flinkJob;
  }

  public void setFlinkJob(FlinkJob flinkJob) {
    this.flinkJob = flinkJob;
  }


  public OrderedJob hadoopJob(HadoopJob hadoopJob) {
    this.hadoopJob = hadoopJob;
    return this;
  }

  /**
   * Get hadoopJob
   * @return hadoopJob
   */
  @javax.annotation.Nullable
  public HadoopJob getHadoopJob() {
    return hadoopJob;
  }

  public void setHadoopJob(HadoopJob hadoopJob) {
    this.hadoopJob = hadoopJob;
  }


  public OrderedJob hiveJob(HiveJob hiveJob) {
    this.hiveJob = hiveJob;
    return this;
  }

  /**
   * Get hiveJob
   * @return hiveJob
   */
  @javax.annotation.Nullable
  public HiveJob getHiveJob() {
    return hiveJob;
  }

  public void setHiveJob(HiveJob hiveJob) {
    this.hiveJob = hiveJob;
  }


  public OrderedJob labels(Map<String, String> labels) {
    this.labels = labels;
    return this;
  }

  public OrderedJob putLabelsItem(String key, String labelsItem) {
    if (this.labels == null) {
      this.labels = new HashMap<>();
    }
    this.labels.put(key, labelsItem);
    return this;
  }

  /**
   * Optional. The labels to associate with this job.Label keys must be between 1 and 63 characters long, and must conform to the following regular expression: \\p{Ll}\\p{Lo}{0,62}Label values must be between 1 and 63 characters long, and must conform to the following regular expression: \\p{Ll}\\p{Lo}\\p{N}_-{0,63}No more than 32 labels can be associated with a given job.
   * @return labels
   */
  @javax.annotation.Nullable
  public Map<String, String> getLabels() {
    return labels;
  }

  public void setLabels(Map<String, String> labels) {
    this.labels = labels;
  }


  public OrderedJob pigJob(PigJob pigJob) {
    this.pigJob = pigJob;
    return this;
  }

  /**
   * Get pigJob
   * @return pigJob
   */
  @javax.annotation.Nullable
  public PigJob getPigJob() {
    return pigJob;
  }

  public void setPigJob(PigJob pigJob) {
    this.pigJob = pigJob;
  }


  public OrderedJob prerequisiteStepIds(List<String> prerequisiteStepIds) {
    this.prerequisiteStepIds = prerequisiteStepIds;
    return this;
  }

  public OrderedJob addPrerequisiteStepIdsItem(String prerequisiteStepIdsItem) {
    if (this.prerequisiteStepIds == null) {
      this.prerequisiteStepIds = new ArrayList<>();
    }
    this.prerequisiteStepIds.add(prerequisiteStepIdsItem);
    return this;
  }

  /**
   * Optional. The optional list of prerequisite job step_ids. If not specified, the job will start at the beginning of workflow.
   * @return prerequisiteStepIds
   */
  @javax.annotation.Nullable
  public List<String> getPrerequisiteStepIds() {
    return prerequisiteStepIds;
  }

  public void setPrerequisiteStepIds(List<String> prerequisiteStepIds) {
    this.prerequisiteStepIds = prerequisiteStepIds;
  }


  public OrderedJob prestoJob(PrestoJob prestoJob) {
    this.prestoJob = prestoJob;
    return this;
  }

  /**
   * Get prestoJob
   * @return prestoJob
   */
  @javax.annotation.Nullable
  public PrestoJob getPrestoJob() {
    return prestoJob;
  }

  public void setPrestoJob(PrestoJob prestoJob) {
    this.prestoJob = prestoJob;
  }


  public OrderedJob pysparkJob(PySparkJob pysparkJob) {
    this.pysparkJob = pysparkJob;
    return this;
  }

  /**
   * Get pysparkJob
   * @return pysparkJob
   */
  @javax.annotation.Nullable
  public PySparkJob getPysparkJob() {
    return pysparkJob;
  }

  public void setPysparkJob(PySparkJob pysparkJob) {
    this.pysparkJob = pysparkJob;
  }


  public OrderedJob scheduling(JobScheduling scheduling) {
    this.scheduling = scheduling;
    return this;
  }

  /**
   * Get scheduling
   * @return scheduling
   */
  @javax.annotation.Nullable
  public JobScheduling getScheduling() {
    return scheduling;
  }

  public void setScheduling(JobScheduling scheduling) {
    this.scheduling = scheduling;
  }


  public OrderedJob sparkJob(SparkJob sparkJob) {
    this.sparkJob = sparkJob;
    return this;
  }

  /**
   * Get sparkJob
   * @return sparkJob
   */
  @javax.annotation.Nullable
  public SparkJob getSparkJob() {
    return sparkJob;
  }

  public void setSparkJob(SparkJob sparkJob) {
    this.sparkJob = sparkJob;
  }


  public OrderedJob sparkRJob(SparkRJob sparkRJob) {
    this.sparkRJob = sparkRJob;
    return this;
  }

  /**
   * Get sparkRJob
   * @return sparkRJob
   */
  @javax.annotation.Nullable
  public SparkRJob getSparkRJob() {
    return sparkRJob;
  }

  public void setSparkRJob(SparkRJob sparkRJob) {
    this.sparkRJob = sparkRJob;
  }


  public OrderedJob sparkSqlJob(SparkSqlJob sparkSqlJob) {
    this.sparkSqlJob = sparkSqlJob;
    return this;
  }

  /**
   * Get sparkSqlJob
   * @return sparkSqlJob
   */
  @javax.annotation.Nullable
  public SparkSqlJob getSparkSqlJob() {
    return sparkSqlJob;
  }

  public void setSparkSqlJob(SparkSqlJob sparkSqlJob) {
    this.sparkSqlJob = sparkSqlJob;
  }


  public OrderedJob stepId(String stepId) {
    this.stepId = stepId;
    return this;
  }

  /**
   * Required. The step id. The id must be unique among all jobs within the template.The step id is used as prefix for job id, as job goog-dataproc-workflow-step-id label, and in prerequisiteStepIds field from other steps.The id must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of between 3 and 50 characters.
   * @return stepId
   */
  @javax.annotation.Nullable
  public String getStepId() {
    return stepId;
  }

  public void setStepId(String stepId) {
    this.stepId = stepId;
  }


  public OrderedJob trinoJob(TrinoJob trinoJob) {
    this.trinoJob = trinoJob;
    return this;
  }

  /**
   * Get trinoJob
   * @return trinoJob
   */
  @javax.annotation.Nullable
  public TrinoJob getTrinoJob() {
    return trinoJob;
  }

  public void setTrinoJob(TrinoJob trinoJob) {
    this.trinoJob = trinoJob;
  }



  @Override
  public boolean equals(Object o) {
    if (this == o) {
      return true;
    }
    if (o == null || getClass() != o.getClass()) {
      return false;
    }
    OrderedJob orderedJob = (OrderedJob) o;
    return Objects.equals(this.flinkJob, orderedJob.flinkJob) &&
        Objects.equals(this.hadoopJob, orderedJob.hadoopJob) &&
        Objects.equals(this.hiveJob, orderedJob.hiveJob) &&
        Objects.equals(this.labels, orderedJob.labels) &&
        Objects.equals(this.pigJob, orderedJob.pigJob) &&
        Objects.equals(this.prerequisiteStepIds, orderedJob.prerequisiteStepIds) &&
        Objects.equals(this.prestoJob, orderedJob.prestoJob) &&
        Objects.equals(this.pysparkJob, orderedJob.pysparkJob) &&
        Objects.equals(this.scheduling, orderedJob.scheduling) &&
        Objects.equals(this.sparkJob, orderedJob.sparkJob) &&
        Objects.equals(this.sparkRJob, orderedJob.sparkRJob) &&
        Objects.equals(this.sparkSqlJob, orderedJob.sparkSqlJob) &&
        Objects.equals(this.stepId, orderedJob.stepId) &&
        Objects.equals(this.trinoJob, orderedJob.trinoJob);
  }

  @Override
  public int hashCode() {
    return Objects.hash(flinkJob, hadoopJob, hiveJob, labels, pigJob, prerequisiteStepIds, prestoJob, pysparkJob, scheduling, sparkJob, sparkRJob, sparkSqlJob, stepId, trinoJob);
  }

  @Override
  public String toString() {
    StringBuilder sb = new StringBuilder();
    sb.append("class OrderedJob {\n");
    sb.append("    flinkJob: ").append(toIndentedString(flinkJob)).append("\n");
    sb.append("    hadoopJob: ").append(toIndentedString(hadoopJob)).append("\n");
    sb.append("    hiveJob: ").append(toIndentedString(hiveJob)).append("\n");
    sb.append("    labels: ").append(toIndentedString(labels)).append("\n");
    sb.append("    pigJob: ").append(toIndentedString(pigJob)).append("\n");
    sb.append("    prerequisiteStepIds: ").append(toIndentedString(prerequisiteStepIds)).append("\n");
    sb.append("    prestoJob: ").append(toIndentedString(prestoJob)).append("\n");
    sb.append("    pysparkJob: ").append(toIndentedString(pysparkJob)).append("\n");
    sb.append("    scheduling: ").append(toIndentedString(scheduling)).append("\n");
    sb.append("    sparkJob: ").append(toIndentedString(sparkJob)).append("\n");
    sb.append("    sparkRJob: ").append(toIndentedString(sparkRJob)).append("\n");
    sb.append("    sparkSqlJob: ").append(toIndentedString(sparkSqlJob)).append("\n");
    sb.append("    stepId: ").append(toIndentedString(stepId)).append("\n");
    sb.append("    trinoJob: ").append(toIndentedString(trinoJob)).append("\n");
    sb.append("}");
    return sb.toString();
  }

  /**
   * Convert the given object to string with each line indented by 4 spaces
   * (except the first line).
   */
  private String toIndentedString(Object o) {
    if (o == null) {
      return "null";
    }
    return o.toString().replace("\n", "\n    ");
  }


  public static HashSet<String> openapiFields;
  public static HashSet<String> openapiRequiredFields;

  static {
    // a set of all properties/fields (JSON key names)
    openapiFields = new HashSet<String>();
    openapiFields.add("flinkJob");
    openapiFields.add("hadoopJob");
    openapiFields.add("hiveJob");
    openapiFields.add("labels");
    openapiFields.add("pigJob");
    openapiFields.add("prerequisiteStepIds");
    openapiFields.add("prestoJob");
    openapiFields.add("pysparkJob");
    openapiFields.add("scheduling");
    openapiFields.add("sparkJob");
    openapiFields.add("sparkRJob");
    openapiFields.add("sparkSqlJob");
    openapiFields.add("stepId");
    openapiFields.add("trinoJob");

    // a set of required properties/fields (JSON key names)
    openapiRequiredFields = new HashSet<String>();
  }

  /**
   * Validates the JSON Element and throws an exception if issues found
   *
   * @param jsonElement JSON Element
   * @throws IOException if the JSON Element is invalid with respect to OrderedJob
   */
  public static void validateJsonElement(JsonElement jsonElement) throws IOException {
      if (jsonElement == null) {
        if (!OrderedJob.openapiRequiredFields.isEmpty()) { // has required fields but JSON element is null
          throw new IllegalArgumentException(String.format("The required field(s) %s in OrderedJob is not found in the empty JSON string", OrderedJob.openapiRequiredFields.toString()));
        }
      }

      Set<Map.Entry<String, JsonElement>> entries = jsonElement.getAsJsonObject().entrySet();
      // check to see if the JSON string contains additional fields
      for (Map.Entry<String, JsonElement> entry : entries) {
        if (!OrderedJob.openapiFields.contains(entry.getKey())) {
          throw new IllegalArgumentException(String.format("The field `%s` in the JSON string is not defined in the `OrderedJob` properties. JSON: %s", entry.getKey(), jsonElement.toString()));
        }
      }
        JsonObject jsonObj = jsonElement.getAsJsonObject();
      // validate the optional field `flinkJob`
      if (jsonObj.get("flinkJob") != null && !jsonObj.get("flinkJob").isJsonNull()) {
        FlinkJob.validateJsonElement(jsonObj.get("flinkJob"));
      }
      // validate the optional field `hadoopJob`
      if (jsonObj.get("hadoopJob") != null && !jsonObj.get("hadoopJob").isJsonNull()) {
        HadoopJob.validateJsonElement(jsonObj.get("hadoopJob"));
      }
      // validate the optional field `hiveJob`
      if (jsonObj.get("hiveJob") != null && !jsonObj.get("hiveJob").isJsonNull()) {
        HiveJob.validateJsonElement(jsonObj.get("hiveJob"));
      }
      // validate the optional field `pigJob`
      if (jsonObj.get("pigJob") != null && !jsonObj.get("pigJob").isJsonNull()) {
        PigJob.validateJsonElement(jsonObj.get("pigJob"));
      }
      // ensure the optional json data is an array if present
      if (jsonObj.get("prerequisiteStepIds") != null && !jsonObj.get("prerequisiteStepIds").isJsonNull() && !jsonObj.get("prerequisiteStepIds").isJsonArray()) {
        throw new IllegalArgumentException(String.format("Expected the field `prerequisiteStepIds` to be an array in the JSON string but got `%s`", jsonObj.get("prerequisiteStepIds").toString()));
      }
      // validate the optional field `prestoJob`
      if (jsonObj.get("prestoJob") != null && !jsonObj.get("prestoJob").isJsonNull()) {
        PrestoJob.validateJsonElement(jsonObj.get("prestoJob"));
      }
      // validate the optional field `pysparkJob`
      if (jsonObj.get("pysparkJob") != null && !jsonObj.get("pysparkJob").isJsonNull()) {
        PySparkJob.validateJsonElement(jsonObj.get("pysparkJob"));
      }
      // validate the optional field `scheduling`
      if (jsonObj.get("scheduling") != null && !jsonObj.get("scheduling").isJsonNull()) {
        JobScheduling.validateJsonElement(jsonObj.get("scheduling"));
      }
      // validate the optional field `sparkJob`
      if (jsonObj.get("sparkJob") != null && !jsonObj.get("sparkJob").isJsonNull()) {
        SparkJob.validateJsonElement(jsonObj.get("sparkJob"));
      }
      // validate the optional field `sparkRJob`
      if (jsonObj.get("sparkRJob") != null && !jsonObj.get("sparkRJob").isJsonNull()) {
        SparkRJob.validateJsonElement(jsonObj.get("sparkRJob"));
      }
      // validate the optional field `sparkSqlJob`
      if (jsonObj.get("sparkSqlJob") != null && !jsonObj.get("sparkSqlJob").isJsonNull()) {
        SparkSqlJob.validateJsonElement(jsonObj.get("sparkSqlJob"));
      }
      if ((jsonObj.get("stepId") != null && !jsonObj.get("stepId").isJsonNull()) && !jsonObj.get("stepId").isJsonPrimitive()) {
        throw new IllegalArgumentException(String.format("Expected the field `stepId` to be a primitive type in the JSON string but got `%s`", jsonObj.get("stepId").toString()));
      }
      // validate the optional field `trinoJob`
      if (jsonObj.get("trinoJob") != null && !jsonObj.get("trinoJob").isJsonNull()) {
        TrinoJob.validateJsonElement(jsonObj.get("trinoJob"));
      }
  }

  public static class CustomTypeAdapterFactory implements TypeAdapterFactory {
    @SuppressWarnings("unchecked")
    @Override
    public <T> TypeAdapter<T> create(Gson gson, TypeToken<T> type) {
       if (!OrderedJob.class.isAssignableFrom(type.getRawType())) {
         return null; // this class only serializes 'OrderedJob' and its subtypes
       }
       final TypeAdapter<JsonElement> elementAdapter = gson.getAdapter(JsonElement.class);
       final TypeAdapter<OrderedJob> thisAdapter
                        = gson.getDelegateAdapter(this, TypeToken.get(OrderedJob.class));

       return (TypeAdapter<T>) new TypeAdapter<OrderedJob>() {
           @Override
           public void write(JsonWriter out, OrderedJob value) throws IOException {
             JsonObject obj = thisAdapter.toJsonTree(value).getAsJsonObject();
             elementAdapter.write(out, obj);
           }

           @Override
           public OrderedJob read(JsonReader in) throws IOException {
             JsonElement jsonElement = elementAdapter.read(in);
             validateJsonElement(jsonElement);
             return thisAdapter.fromJsonTree(jsonElement);
           }

       }.nullSafe();
    }
  }

  /**
   * Create an instance of OrderedJob given an JSON string
   *
   * @param jsonString JSON string
   * @return An instance of OrderedJob
   * @throws IOException if the JSON string is invalid with respect to OrderedJob
   */
  public static OrderedJob fromJson(String jsonString) throws IOException {
    return JSON.getGson().fromJson(jsonString, OrderedJob.class);
  }

  /**
   * Convert an instance of OrderedJob to an JSON string
   *
   * @return JSON string
   */
  public String toJson() {
    return JSON.getGson().toJson(this);
  }
}

