/*
 * Cloud Dataproc API
 * Manages Hadoop-based clusters and jobs on Google Cloud Platform.
 *
 * The version of the OpenAPI document: v1
 * 
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */


package org.openapitools.client.model;

import java.util.Objects;
import com.google.gson.TypeAdapter;
import com.google.gson.annotations.JsonAdapter;
import com.google.gson.annotations.SerializedName;
import com.google.gson.stream.JsonReader;
import com.google.gson.stream.JsonWriter;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import org.openapitools.client.model.DriverSchedulingConfig;
import org.openapitools.client.model.FlinkJob;
import org.openapitools.client.model.HadoopJob;
import org.openapitools.client.model.HiveJob;
import org.openapitools.client.model.JobPlacement;
import org.openapitools.client.model.JobReference;
import org.openapitools.client.model.JobScheduling;
import org.openapitools.client.model.JobStatus;
import org.openapitools.client.model.PigJob;
import org.openapitools.client.model.PrestoJob;
import org.openapitools.client.model.PySparkJob;
import org.openapitools.client.model.SparkJob;
import org.openapitools.client.model.SparkRJob;
import org.openapitools.client.model.SparkSqlJob;
import org.openapitools.client.model.TrinoJob;
import org.openapitools.client.model.YarnApplication;

import com.google.gson.Gson;
import com.google.gson.GsonBuilder;
import com.google.gson.JsonArray;
import com.google.gson.JsonDeserializationContext;
import com.google.gson.JsonDeserializer;
import com.google.gson.JsonElement;
import com.google.gson.JsonObject;
import com.google.gson.JsonParseException;
import com.google.gson.TypeAdapterFactory;
import com.google.gson.reflect.TypeToken;
import com.google.gson.TypeAdapter;
import com.google.gson.stream.JsonReader;
import com.google.gson.stream.JsonWriter;
import java.io.IOException;

import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;

import org.openapitools.client.JSON;

/**
 * A Dataproc job resource.
 */
@javax.annotation.Generated(value = "org.openapitools.codegen.languages.JavaClientCodegen", date = "2024-10-12T11:49:50.925918-04:00[America/New_York]", comments = "Generator version: 7.9.0")
public class Job {
  public static final String SERIALIZED_NAME_DONE = "done";
  @SerializedName(SERIALIZED_NAME_DONE)
  private Boolean done;

  public static final String SERIALIZED_NAME_DRIVER_CONTROL_FILES_URI = "driverControlFilesUri";
  @SerializedName(SERIALIZED_NAME_DRIVER_CONTROL_FILES_URI)
  private String driverControlFilesUri;

  public static final String SERIALIZED_NAME_DRIVER_OUTPUT_RESOURCE_URI = "driverOutputResourceUri";
  @SerializedName(SERIALIZED_NAME_DRIVER_OUTPUT_RESOURCE_URI)
  private String driverOutputResourceUri;

  public static final String SERIALIZED_NAME_DRIVER_SCHEDULING_CONFIG = "driverSchedulingConfig";
  @SerializedName(SERIALIZED_NAME_DRIVER_SCHEDULING_CONFIG)
  private DriverSchedulingConfig driverSchedulingConfig;

  public static final String SERIALIZED_NAME_FLINK_JOB = "flinkJob";
  @SerializedName(SERIALIZED_NAME_FLINK_JOB)
  private FlinkJob flinkJob;

  public static final String SERIALIZED_NAME_HADOOP_JOB = "hadoopJob";
  @SerializedName(SERIALIZED_NAME_HADOOP_JOB)
  private HadoopJob hadoopJob;

  public static final String SERIALIZED_NAME_HIVE_JOB = "hiveJob";
  @SerializedName(SERIALIZED_NAME_HIVE_JOB)
  private HiveJob hiveJob;

  public static final String SERIALIZED_NAME_JOB_UUID = "jobUuid";
  @SerializedName(SERIALIZED_NAME_JOB_UUID)
  private String jobUuid;

  public static final String SERIALIZED_NAME_LABELS = "labels";
  @SerializedName(SERIALIZED_NAME_LABELS)
  private Map<String, String> labels = new HashMap<>();

  public static final String SERIALIZED_NAME_PIG_JOB = "pigJob";
  @SerializedName(SERIALIZED_NAME_PIG_JOB)
  private PigJob pigJob;

  public static final String SERIALIZED_NAME_PLACEMENT = "placement";
  @SerializedName(SERIALIZED_NAME_PLACEMENT)
  private JobPlacement placement;

  public static final String SERIALIZED_NAME_PRESTO_JOB = "prestoJob";
  @SerializedName(SERIALIZED_NAME_PRESTO_JOB)
  private PrestoJob prestoJob;

  public static final String SERIALIZED_NAME_PYSPARK_JOB = "pysparkJob";
  @SerializedName(SERIALIZED_NAME_PYSPARK_JOB)
  private PySparkJob pysparkJob;

  public static final String SERIALIZED_NAME_REFERENCE = "reference";
  @SerializedName(SERIALIZED_NAME_REFERENCE)
  private JobReference reference;

  public static final String SERIALIZED_NAME_SCHEDULING = "scheduling";
  @SerializedName(SERIALIZED_NAME_SCHEDULING)
  private JobScheduling scheduling;

  public static final String SERIALIZED_NAME_SPARK_JOB = "sparkJob";
  @SerializedName(SERIALIZED_NAME_SPARK_JOB)
  private SparkJob sparkJob;

  public static final String SERIALIZED_NAME_SPARK_R_JOB = "sparkRJob";
  @SerializedName(SERIALIZED_NAME_SPARK_R_JOB)
  private SparkRJob sparkRJob;

  public static final String SERIALIZED_NAME_SPARK_SQL_JOB = "sparkSqlJob";
  @SerializedName(SERIALIZED_NAME_SPARK_SQL_JOB)
  private SparkSqlJob sparkSqlJob;

  public static final String SERIALIZED_NAME_STATUS = "status";
  @SerializedName(SERIALIZED_NAME_STATUS)
  private JobStatus status;

  public static final String SERIALIZED_NAME_STATUS_HISTORY = "statusHistory";
  @SerializedName(SERIALIZED_NAME_STATUS_HISTORY)
  private List<JobStatus> statusHistory = new ArrayList<>();

  public static final String SERIALIZED_NAME_TRINO_JOB = "trinoJob";
  @SerializedName(SERIALIZED_NAME_TRINO_JOB)
  private TrinoJob trinoJob;

  public static final String SERIALIZED_NAME_YARN_APPLICATIONS = "yarnApplications";
  @SerializedName(SERIALIZED_NAME_YARN_APPLICATIONS)
  private List<YarnApplication> yarnApplications = new ArrayList<>();

  public Job() {
  }

  public Job(
     Boolean done, 
     String driverControlFilesUri, 
     String driverOutputResourceUri, 
     String jobUuid, 
     List<JobStatus> statusHistory, 
     List<YarnApplication> yarnApplications
  ) {
    this();
    this.done = done;
    this.driverControlFilesUri = driverControlFilesUri;
    this.driverOutputResourceUri = driverOutputResourceUri;
    this.jobUuid = jobUuid;
    this.statusHistory = statusHistory;
    this.yarnApplications = yarnApplications;
  }

  /**
   * Output only. Indicates whether the job is completed. If the value is false, the job is still in progress. If true, the job is completed, and status.state field will indicate if it was successful, failed, or cancelled.
   * @return done
   */
  @javax.annotation.Nullable
  public Boolean getDone() {
    return done;
  }



  /**
   * Output only. If present, the location of miscellaneous control files which can be used as part of job setup and handling. If not present, control files might be placed in the same location as driver_output_uri.
   * @return driverControlFilesUri
   */
  @javax.annotation.Nullable
  public String getDriverControlFilesUri() {
    return driverControlFilesUri;
  }



  /**
   * Output only. A URI pointing to the location of the stdout of the job&#39;s driver program.
   * @return driverOutputResourceUri
   */
  @javax.annotation.Nullable
  public String getDriverOutputResourceUri() {
    return driverOutputResourceUri;
  }



  public Job driverSchedulingConfig(DriverSchedulingConfig driverSchedulingConfig) {
    this.driverSchedulingConfig = driverSchedulingConfig;
    return this;
  }

  /**
   * Get driverSchedulingConfig
   * @return driverSchedulingConfig
   */
  @javax.annotation.Nullable
  public DriverSchedulingConfig getDriverSchedulingConfig() {
    return driverSchedulingConfig;
  }

  public void setDriverSchedulingConfig(DriverSchedulingConfig driverSchedulingConfig) {
    this.driverSchedulingConfig = driverSchedulingConfig;
  }


  public Job flinkJob(FlinkJob flinkJob) {
    this.flinkJob = flinkJob;
    return this;
  }

  /**
   * Get flinkJob
   * @return flinkJob
   */
  @javax.annotation.Nullable
  public FlinkJob getFlinkJob() {
    return flinkJob;
  }

  public void setFlinkJob(FlinkJob flinkJob) {
    this.flinkJob = flinkJob;
  }


  public Job hadoopJob(HadoopJob hadoopJob) {
    this.hadoopJob = hadoopJob;
    return this;
  }

  /**
   * Get hadoopJob
   * @return hadoopJob
   */
  @javax.annotation.Nullable
  public HadoopJob getHadoopJob() {
    return hadoopJob;
  }

  public void setHadoopJob(HadoopJob hadoopJob) {
    this.hadoopJob = hadoopJob;
  }


  public Job hiveJob(HiveJob hiveJob) {
    this.hiveJob = hiveJob;
    return this;
  }

  /**
   * Get hiveJob
   * @return hiveJob
   */
  @javax.annotation.Nullable
  public HiveJob getHiveJob() {
    return hiveJob;
  }

  public void setHiveJob(HiveJob hiveJob) {
    this.hiveJob = hiveJob;
  }


  /**
   * Output only. A UUID that uniquely identifies a job within the project over time. This is in contrast to a user-settable reference.job_id that might be reused over time.
   * @return jobUuid
   */
  @javax.annotation.Nullable
  public String getJobUuid() {
    return jobUuid;
  }



  public Job labels(Map<String, String> labels) {
    this.labels = labels;
    return this;
  }

  public Job putLabelsItem(String key, String labelsItem) {
    if (this.labels == null) {
      this.labels = new HashMap<>();
    }
    this.labels.put(key, labelsItem);
    return this;
  }

  /**
   * Optional. The labels to associate with this job. Label keys must contain 1 to 63 characters, and must conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). Label values can be empty, but, if present, must contain 1 to 63 characters, and must conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). No more than 32 labels can be associated with a job.
   * @return labels
   */
  @javax.annotation.Nullable
  public Map<String, String> getLabels() {
    return labels;
  }

  public void setLabels(Map<String, String> labels) {
    this.labels = labels;
  }


  public Job pigJob(PigJob pigJob) {
    this.pigJob = pigJob;
    return this;
  }

  /**
   * Get pigJob
   * @return pigJob
   */
  @javax.annotation.Nullable
  public PigJob getPigJob() {
    return pigJob;
  }

  public void setPigJob(PigJob pigJob) {
    this.pigJob = pigJob;
  }


  public Job placement(JobPlacement placement) {
    this.placement = placement;
    return this;
  }

  /**
   * Get placement
   * @return placement
   */
  @javax.annotation.Nullable
  public JobPlacement getPlacement() {
    return placement;
  }

  public void setPlacement(JobPlacement placement) {
    this.placement = placement;
  }


  public Job prestoJob(PrestoJob prestoJob) {
    this.prestoJob = prestoJob;
    return this;
  }

  /**
   * Get prestoJob
   * @return prestoJob
   */
  @javax.annotation.Nullable
  public PrestoJob getPrestoJob() {
    return prestoJob;
  }

  public void setPrestoJob(PrestoJob prestoJob) {
    this.prestoJob = prestoJob;
  }


  public Job pysparkJob(PySparkJob pysparkJob) {
    this.pysparkJob = pysparkJob;
    return this;
  }

  /**
   * Get pysparkJob
   * @return pysparkJob
   */
  @javax.annotation.Nullable
  public PySparkJob getPysparkJob() {
    return pysparkJob;
  }

  public void setPysparkJob(PySparkJob pysparkJob) {
    this.pysparkJob = pysparkJob;
  }


  public Job reference(JobReference reference) {
    this.reference = reference;
    return this;
  }

  /**
   * Get reference
   * @return reference
   */
  @javax.annotation.Nullable
  public JobReference getReference() {
    return reference;
  }

  public void setReference(JobReference reference) {
    this.reference = reference;
  }


  public Job scheduling(JobScheduling scheduling) {
    this.scheduling = scheduling;
    return this;
  }

  /**
   * Get scheduling
   * @return scheduling
   */
  @javax.annotation.Nullable
  public JobScheduling getScheduling() {
    return scheduling;
  }

  public void setScheduling(JobScheduling scheduling) {
    this.scheduling = scheduling;
  }


  public Job sparkJob(SparkJob sparkJob) {
    this.sparkJob = sparkJob;
    return this;
  }

  /**
   * Get sparkJob
   * @return sparkJob
   */
  @javax.annotation.Nullable
  public SparkJob getSparkJob() {
    return sparkJob;
  }

  public void setSparkJob(SparkJob sparkJob) {
    this.sparkJob = sparkJob;
  }


  public Job sparkRJob(SparkRJob sparkRJob) {
    this.sparkRJob = sparkRJob;
    return this;
  }

  /**
   * Get sparkRJob
   * @return sparkRJob
   */
  @javax.annotation.Nullable
  public SparkRJob getSparkRJob() {
    return sparkRJob;
  }

  public void setSparkRJob(SparkRJob sparkRJob) {
    this.sparkRJob = sparkRJob;
  }


  public Job sparkSqlJob(SparkSqlJob sparkSqlJob) {
    this.sparkSqlJob = sparkSqlJob;
    return this;
  }

  /**
   * Get sparkSqlJob
   * @return sparkSqlJob
   */
  @javax.annotation.Nullable
  public SparkSqlJob getSparkSqlJob() {
    return sparkSqlJob;
  }

  public void setSparkSqlJob(SparkSqlJob sparkSqlJob) {
    this.sparkSqlJob = sparkSqlJob;
  }


  public Job status(JobStatus status) {
    this.status = status;
    return this;
  }

  /**
   * Get status
   * @return status
   */
  @javax.annotation.Nullable
  public JobStatus getStatus() {
    return status;
  }

  public void setStatus(JobStatus status) {
    this.status = status;
  }


  /**
   * Output only. The previous job status.
   * @return statusHistory
   */
  @javax.annotation.Nullable
  public List<JobStatus> getStatusHistory() {
    return statusHistory;
  }



  public Job trinoJob(TrinoJob trinoJob) {
    this.trinoJob = trinoJob;
    return this;
  }

  /**
   * Get trinoJob
   * @return trinoJob
   */
  @javax.annotation.Nullable
  public TrinoJob getTrinoJob() {
    return trinoJob;
  }

  public void setTrinoJob(TrinoJob trinoJob) {
    this.trinoJob = trinoJob;
  }


  /**
   * Output only. The collection of YARN applications spun up by this job.Beta Feature: This report is available for testing purposes only. It might be changed before final release.
   * @return yarnApplications
   */
  @javax.annotation.Nullable
  public List<YarnApplication> getYarnApplications() {
    return yarnApplications;
  }




  @Override
  public boolean equals(Object o) {
    if (this == o) {
      return true;
    }
    if (o == null || getClass() != o.getClass()) {
      return false;
    }
    Job job = (Job) o;
    return Objects.equals(this.done, job.done) &&
        Objects.equals(this.driverControlFilesUri, job.driverControlFilesUri) &&
        Objects.equals(this.driverOutputResourceUri, job.driverOutputResourceUri) &&
        Objects.equals(this.driverSchedulingConfig, job.driverSchedulingConfig) &&
        Objects.equals(this.flinkJob, job.flinkJob) &&
        Objects.equals(this.hadoopJob, job.hadoopJob) &&
        Objects.equals(this.hiveJob, job.hiveJob) &&
        Objects.equals(this.jobUuid, job.jobUuid) &&
        Objects.equals(this.labels, job.labels) &&
        Objects.equals(this.pigJob, job.pigJob) &&
        Objects.equals(this.placement, job.placement) &&
        Objects.equals(this.prestoJob, job.prestoJob) &&
        Objects.equals(this.pysparkJob, job.pysparkJob) &&
        Objects.equals(this.reference, job.reference) &&
        Objects.equals(this.scheduling, job.scheduling) &&
        Objects.equals(this.sparkJob, job.sparkJob) &&
        Objects.equals(this.sparkRJob, job.sparkRJob) &&
        Objects.equals(this.sparkSqlJob, job.sparkSqlJob) &&
        Objects.equals(this.status, job.status) &&
        Objects.equals(this.statusHistory, job.statusHistory) &&
        Objects.equals(this.trinoJob, job.trinoJob) &&
        Objects.equals(this.yarnApplications, job.yarnApplications);
  }

  @Override
  public int hashCode() {
    return Objects.hash(done, driverControlFilesUri, driverOutputResourceUri, driverSchedulingConfig, flinkJob, hadoopJob, hiveJob, jobUuid, labels, pigJob, placement, prestoJob, pysparkJob, reference, scheduling, sparkJob, sparkRJob, sparkSqlJob, status, statusHistory, trinoJob, yarnApplications);
  }

  @Override
  public String toString() {
    StringBuilder sb = new StringBuilder();
    sb.append("class Job {\n");
    sb.append("    done: ").append(toIndentedString(done)).append("\n");
    sb.append("    driverControlFilesUri: ").append(toIndentedString(driverControlFilesUri)).append("\n");
    sb.append("    driverOutputResourceUri: ").append(toIndentedString(driverOutputResourceUri)).append("\n");
    sb.append("    driverSchedulingConfig: ").append(toIndentedString(driverSchedulingConfig)).append("\n");
    sb.append("    flinkJob: ").append(toIndentedString(flinkJob)).append("\n");
    sb.append("    hadoopJob: ").append(toIndentedString(hadoopJob)).append("\n");
    sb.append("    hiveJob: ").append(toIndentedString(hiveJob)).append("\n");
    sb.append("    jobUuid: ").append(toIndentedString(jobUuid)).append("\n");
    sb.append("    labels: ").append(toIndentedString(labels)).append("\n");
    sb.append("    pigJob: ").append(toIndentedString(pigJob)).append("\n");
    sb.append("    placement: ").append(toIndentedString(placement)).append("\n");
    sb.append("    prestoJob: ").append(toIndentedString(prestoJob)).append("\n");
    sb.append("    pysparkJob: ").append(toIndentedString(pysparkJob)).append("\n");
    sb.append("    reference: ").append(toIndentedString(reference)).append("\n");
    sb.append("    scheduling: ").append(toIndentedString(scheduling)).append("\n");
    sb.append("    sparkJob: ").append(toIndentedString(sparkJob)).append("\n");
    sb.append("    sparkRJob: ").append(toIndentedString(sparkRJob)).append("\n");
    sb.append("    sparkSqlJob: ").append(toIndentedString(sparkSqlJob)).append("\n");
    sb.append("    status: ").append(toIndentedString(status)).append("\n");
    sb.append("    statusHistory: ").append(toIndentedString(statusHistory)).append("\n");
    sb.append("    trinoJob: ").append(toIndentedString(trinoJob)).append("\n");
    sb.append("    yarnApplications: ").append(toIndentedString(yarnApplications)).append("\n");
    sb.append("}");
    return sb.toString();
  }

  /**
   * Convert the given object to string with each line indented by 4 spaces
   * (except the first line).
   */
  private String toIndentedString(Object o) {
    if (o == null) {
      return "null";
    }
    return o.toString().replace("\n", "\n    ");
  }


  public static HashSet<String> openapiFields;
  public static HashSet<String> openapiRequiredFields;

  static {
    // a set of all properties/fields (JSON key names)
    openapiFields = new HashSet<String>();
    openapiFields.add("done");
    openapiFields.add("driverControlFilesUri");
    openapiFields.add("driverOutputResourceUri");
    openapiFields.add("driverSchedulingConfig");
    openapiFields.add("flinkJob");
    openapiFields.add("hadoopJob");
    openapiFields.add("hiveJob");
    openapiFields.add("jobUuid");
    openapiFields.add("labels");
    openapiFields.add("pigJob");
    openapiFields.add("placement");
    openapiFields.add("prestoJob");
    openapiFields.add("pysparkJob");
    openapiFields.add("reference");
    openapiFields.add("scheduling");
    openapiFields.add("sparkJob");
    openapiFields.add("sparkRJob");
    openapiFields.add("sparkSqlJob");
    openapiFields.add("status");
    openapiFields.add("statusHistory");
    openapiFields.add("trinoJob");
    openapiFields.add("yarnApplications");

    // a set of required properties/fields (JSON key names)
    openapiRequiredFields = new HashSet<String>();
  }

  /**
   * Validates the JSON Element and throws an exception if issues found
   *
   * @param jsonElement JSON Element
   * @throws IOException if the JSON Element is invalid with respect to Job
   */
  public static void validateJsonElement(JsonElement jsonElement) throws IOException {
      if (jsonElement == null) {
        if (!Job.openapiRequiredFields.isEmpty()) { // has required fields but JSON element is null
          throw new IllegalArgumentException(String.format("The required field(s) %s in Job is not found in the empty JSON string", Job.openapiRequiredFields.toString()));
        }
      }

      Set<Map.Entry<String, JsonElement>> entries = jsonElement.getAsJsonObject().entrySet();
      // check to see if the JSON string contains additional fields
      for (Map.Entry<String, JsonElement> entry : entries) {
        if (!Job.openapiFields.contains(entry.getKey())) {
          throw new IllegalArgumentException(String.format("The field `%s` in the JSON string is not defined in the `Job` properties. JSON: %s", entry.getKey(), jsonElement.toString()));
        }
      }
        JsonObject jsonObj = jsonElement.getAsJsonObject();
      if ((jsonObj.get("driverControlFilesUri") != null && !jsonObj.get("driverControlFilesUri").isJsonNull()) && !jsonObj.get("driverControlFilesUri").isJsonPrimitive()) {
        throw new IllegalArgumentException(String.format("Expected the field `driverControlFilesUri` to be a primitive type in the JSON string but got `%s`", jsonObj.get("driverControlFilesUri").toString()));
      }
      if ((jsonObj.get("driverOutputResourceUri") != null && !jsonObj.get("driverOutputResourceUri").isJsonNull()) && !jsonObj.get("driverOutputResourceUri").isJsonPrimitive()) {
        throw new IllegalArgumentException(String.format("Expected the field `driverOutputResourceUri` to be a primitive type in the JSON string but got `%s`", jsonObj.get("driverOutputResourceUri").toString()));
      }
      // validate the optional field `driverSchedulingConfig`
      if (jsonObj.get("driverSchedulingConfig") != null && !jsonObj.get("driverSchedulingConfig").isJsonNull()) {
        DriverSchedulingConfig.validateJsonElement(jsonObj.get("driverSchedulingConfig"));
      }
      // validate the optional field `flinkJob`
      if (jsonObj.get("flinkJob") != null && !jsonObj.get("flinkJob").isJsonNull()) {
        FlinkJob.validateJsonElement(jsonObj.get("flinkJob"));
      }
      // validate the optional field `hadoopJob`
      if (jsonObj.get("hadoopJob") != null && !jsonObj.get("hadoopJob").isJsonNull()) {
        HadoopJob.validateJsonElement(jsonObj.get("hadoopJob"));
      }
      // validate the optional field `hiveJob`
      if (jsonObj.get("hiveJob") != null && !jsonObj.get("hiveJob").isJsonNull()) {
        HiveJob.validateJsonElement(jsonObj.get("hiveJob"));
      }
      if ((jsonObj.get("jobUuid") != null && !jsonObj.get("jobUuid").isJsonNull()) && !jsonObj.get("jobUuid").isJsonPrimitive()) {
        throw new IllegalArgumentException(String.format("Expected the field `jobUuid` to be a primitive type in the JSON string but got `%s`", jsonObj.get("jobUuid").toString()));
      }
      // validate the optional field `pigJob`
      if (jsonObj.get("pigJob") != null && !jsonObj.get("pigJob").isJsonNull()) {
        PigJob.validateJsonElement(jsonObj.get("pigJob"));
      }
      // validate the optional field `placement`
      if (jsonObj.get("placement") != null && !jsonObj.get("placement").isJsonNull()) {
        JobPlacement.validateJsonElement(jsonObj.get("placement"));
      }
      // validate the optional field `prestoJob`
      if (jsonObj.get("prestoJob") != null && !jsonObj.get("prestoJob").isJsonNull()) {
        PrestoJob.validateJsonElement(jsonObj.get("prestoJob"));
      }
      // validate the optional field `pysparkJob`
      if (jsonObj.get("pysparkJob") != null && !jsonObj.get("pysparkJob").isJsonNull()) {
        PySparkJob.validateJsonElement(jsonObj.get("pysparkJob"));
      }
      // validate the optional field `reference`
      if (jsonObj.get("reference") != null && !jsonObj.get("reference").isJsonNull()) {
        JobReference.validateJsonElement(jsonObj.get("reference"));
      }
      // validate the optional field `scheduling`
      if (jsonObj.get("scheduling") != null && !jsonObj.get("scheduling").isJsonNull()) {
        JobScheduling.validateJsonElement(jsonObj.get("scheduling"));
      }
      // validate the optional field `sparkJob`
      if (jsonObj.get("sparkJob") != null && !jsonObj.get("sparkJob").isJsonNull()) {
        SparkJob.validateJsonElement(jsonObj.get("sparkJob"));
      }
      // validate the optional field `sparkRJob`
      if (jsonObj.get("sparkRJob") != null && !jsonObj.get("sparkRJob").isJsonNull()) {
        SparkRJob.validateJsonElement(jsonObj.get("sparkRJob"));
      }
      // validate the optional field `sparkSqlJob`
      if (jsonObj.get("sparkSqlJob") != null && !jsonObj.get("sparkSqlJob").isJsonNull()) {
        SparkSqlJob.validateJsonElement(jsonObj.get("sparkSqlJob"));
      }
      // validate the optional field `status`
      if (jsonObj.get("status") != null && !jsonObj.get("status").isJsonNull()) {
        JobStatus.validateJsonElement(jsonObj.get("status"));
      }
      if (jsonObj.get("statusHistory") != null && !jsonObj.get("statusHistory").isJsonNull()) {
        JsonArray jsonArraystatusHistory = jsonObj.getAsJsonArray("statusHistory");
        if (jsonArraystatusHistory != null) {
          // ensure the json data is an array
          if (!jsonObj.get("statusHistory").isJsonArray()) {
            throw new IllegalArgumentException(String.format("Expected the field `statusHistory` to be an array in the JSON string but got `%s`", jsonObj.get("statusHistory").toString()));
          }

          // validate the optional field `statusHistory` (array)
          for (int i = 0; i < jsonArraystatusHistory.size(); i++) {
            JobStatus.validateJsonElement(jsonArraystatusHistory.get(i));
          };
        }
      }
      // validate the optional field `trinoJob`
      if (jsonObj.get("trinoJob") != null && !jsonObj.get("trinoJob").isJsonNull()) {
        TrinoJob.validateJsonElement(jsonObj.get("trinoJob"));
      }
      if (jsonObj.get("yarnApplications") != null && !jsonObj.get("yarnApplications").isJsonNull()) {
        JsonArray jsonArrayyarnApplications = jsonObj.getAsJsonArray("yarnApplications");
        if (jsonArrayyarnApplications != null) {
          // ensure the json data is an array
          if (!jsonObj.get("yarnApplications").isJsonArray()) {
            throw new IllegalArgumentException(String.format("Expected the field `yarnApplications` to be an array in the JSON string but got `%s`", jsonObj.get("yarnApplications").toString()));
          }

          // validate the optional field `yarnApplications` (array)
          for (int i = 0; i < jsonArrayyarnApplications.size(); i++) {
            YarnApplication.validateJsonElement(jsonArrayyarnApplications.get(i));
          };
        }
      }
  }

  public static class CustomTypeAdapterFactory implements TypeAdapterFactory {
    @SuppressWarnings("unchecked")
    @Override
    public <T> TypeAdapter<T> create(Gson gson, TypeToken<T> type) {
       if (!Job.class.isAssignableFrom(type.getRawType())) {
         return null; // this class only serializes 'Job' and its subtypes
       }
       final TypeAdapter<JsonElement> elementAdapter = gson.getAdapter(JsonElement.class);
       final TypeAdapter<Job> thisAdapter
                        = gson.getDelegateAdapter(this, TypeToken.get(Job.class));

       return (TypeAdapter<T>) new TypeAdapter<Job>() {
           @Override
           public void write(JsonWriter out, Job value) throws IOException {
             JsonObject obj = thisAdapter.toJsonTree(value).getAsJsonObject();
             elementAdapter.write(out, obj);
           }

           @Override
           public Job read(JsonReader in) throws IOException {
             JsonElement jsonElement = elementAdapter.read(in);
             validateJsonElement(jsonElement);
             return thisAdapter.fromJsonTree(jsonElement);
           }

       }.nullSafe();
    }
  }

  /**
   * Create an instance of Job given an JSON string
   *
   * @param jsonString JSON string
   * @return An instance of Job
   * @throws IOException if the JSON string is invalid with respect to Job
   */
  public static Job fromJson(String jsonString) throws IOException {
    return JSON.getGson().fromJson(jsonString, Job.class);
  }

  /**
   * Convert an instance of Job to an JSON string
   *
   * @return JSON string
   */
  public String toJson() {
    return JSON.getGson().toJson(this);
  }
}

