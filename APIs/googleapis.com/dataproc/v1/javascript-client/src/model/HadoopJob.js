/**
 * Cloud Dataproc API
 * Manages Hadoop-based clusters and jobs on Google Cloud Platform.
 *
 * The version of the OpenAPI document: v1
 * 
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 *
 */

import ApiClient from '../ApiClient';
import LoggingConfig from './LoggingConfig';

/**
 * The HadoopJob model module.
 * @module model/HadoopJob
 * @version v1
 */
class HadoopJob {
    /**
     * Constructs a new <code>HadoopJob</code>.
     * A Dataproc job for running Apache Hadoop MapReduce (https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html) jobs on Apache Hadoop YARN (https://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-site/YARN.html).
     * @alias module:model/HadoopJob
     */
    constructor() { 
        
        HadoopJob.initialize(this);
    }

    /**
     * Initializes the fields of this object.
     * This method is used by the constructors of any subclasses, in order to implement multiple inheritance (mix-ins).
     * Only for internal use.
     */
    static initialize(obj) { 
    }

    /**
     * Constructs a <code>HadoopJob</code> from a plain JavaScript object, optionally creating a new instance.
     * Copies all relevant properties from <code>data</code> to <code>obj</code> if supplied or a new instance if not.
     * @param {Object} data The plain JavaScript object bearing properties of interest.
     * @param {module:model/HadoopJob} obj Optional instance to populate.
     * @return {module:model/HadoopJob} The populated <code>HadoopJob</code> instance.
     */
    static constructFromObject(data, obj) {
        if (data) {
            obj = obj || new HadoopJob();

            if (data.hasOwnProperty('archiveUris')) {
                obj['archiveUris'] = ApiClient.convertToType(data['archiveUris'], ['String']);
            }
            if (data.hasOwnProperty('args')) {
                obj['args'] = ApiClient.convertToType(data['args'], ['String']);
            }
            if (data.hasOwnProperty('fileUris')) {
                obj['fileUris'] = ApiClient.convertToType(data['fileUris'], ['String']);
            }
            if (data.hasOwnProperty('jarFileUris')) {
                obj['jarFileUris'] = ApiClient.convertToType(data['jarFileUris'], ['String']);
            }
            if (data.hasOwnProperty('loggingConfig')) {
                obj['loggingConfig'] = LoggingConfig.constructFromObject(data['loggingConfig']);
            }
            if (data.hasOwnProperty('mainClass')) {
                obj['mainClass'] = ApiClient.convertToType(data['mainClass'], 'String');
            }
            if (data.hasOwnProperty('mainJarFileUri')) {
                obj['mainJarFileUri'] = ApiClient.convertToType(data['mainJarFileUri'], 'String');
            }
            if (data.hasOwnProperty('properties')) {
                obj['properties'] = ApiClient.convertToType(data['properties'], {'String': 'String'});
            }
        }
        return obj;
    }

    /**
     * Validates the JSON data with respect to <code>HadoopJob</code>.
     * @param {Object} data The plain JavaScript object bearing properties of interest.
     * @return {boolean} to indicate whether the JSON data is valid with respect to <code>HadoopJob</code>.
     */
    static validateJSON(data) {
        // ensure the json data is an array
        if (!Array.isArray(data['archiveUris'])) {
            throw new Error("Expected the field `archiveUris` to be an array in the JSON data but got " + data['archiveUris']);
        }
        // ensure the json data is an array
        if (!Array.isArray(data['args'])) {
            throw new Error("Expected the field `args` to be an array in the JSON data but got " + data['args']);
        }
        // ensure the json data is an array
        if (!Array.isArray(data['fileUris'])) {
            throw new Error("Expected the field `fileUris` to be an array in the JSON data but got " + data['fileUris']);
        }
        // ensure the json data is an array
        if (!Array.isArray(data['jarFileUris'])) {
            throw new Error("Expected the field `jarFileUris` to be an array in the JSON data but got " + data['jarFileUris']);
        }
        // validate the optional field `loggingConfig`
        if (data['loggingConfig']) { // data not null
          LoggingConfig.validateJSON(data['loggingConfig']);
        }
        // ensure the json data is a string
        if (data['mainClass'] && !(typeof data['mainClass'] === 'string' || data['mainClass'] instanceof String)) {
            throw new Error("Expected the field `mainClass` to be a primitive type in the JSON string but got " + data['mainClass']);
        }
        // ensure the json data is a string
        if (data['mainJarFileUri'] && !(typeof data['mainJarFileUri'] === 'string' || data['mainJarFileUri'] instanceof String)) {
            throw new Error("Expected the field `mainJarFileUri` to be a primitive type in the JSON string but got " + data['mainJarFileUri']);
        }

        return true;
    }


}



/**
 * Optional. HCFS URIs of archives to be extracted in the working directory of Hadoop drivers and tasks. Supported file types: .jar, .tar, .tar.gz, .tgz, or .zip.
 * @member {Array.<String>} archiveUris
 */
HadoopJob.prototype['archiveUris'] = undefined;

/**
 * Optional. The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision might occur that causes an incorrect job submission.
 * @member {Array.<String>} args
 */
HadoopJob.prototype['args'] = undefined;

/**
 * Optional. HCFS (Hadoop Compatible Filesystem) URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
 * @member {Array.<String>} fileUris
 */
HadoopJob.prototype['fileUris'] = undefined;

/**
 * Optional. Jar file URIs to add to the CLASSPATHs of the Hadoop driver and tasks.
 * @member {Array.<String>} jarFileUris
 */
HadoopJob.prototype['jarFileUris'] = undefined;

/**
 * @member {module:model/LoggingConfig} loggingConfig
 */
HadoopJob.prototype['loggingConfig'] = undefined;

/**
 * The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in jar_file_uris.
 * @member {String} mainClass
 */
HadoopJob.prototype['mainClass'] = undefined;

/**
 * The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'
 * @member {String} mainJarFileUri
 */
HadoopJob.prototype['mainJarFileUri'] = undefined;

/**
 * Optional. A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Dataproc API might be overwritten. Can include properties set in /etc/hadoop/conf/_*-site and classes in user code.
 * @member {Object.<String, String>} properties
 */
HadoopJob.prototype['properties'] = undefined;






export default HadoopJob;

