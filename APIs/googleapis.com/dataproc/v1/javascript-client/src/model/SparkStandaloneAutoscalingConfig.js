/**
 * Cloud Dataproc API
 * Manages Hadoop-based clusters and jobs on Google Cloud Platform.
 *
 * The version of the OpenAPI document: v1
 * 
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 *
 */

import ApiClient from '../ApiClient';

/**
 * The SparkStandaloneAutoscalingConfig model module.
 * @module model/SparkStandaloneAutoscalingConfig
 * @version v1
 */
class SparkStandaloneAutoscalingConfig {
    /**
     * Constructs a new <code>SparkStandaloneAutoscalingConfig</code>.
     * Basic autoscaling configurations for Spark Standalone.
     * @alias module:model/SparkStandaloneAutoscalingConfig
     */
    constructor() { 
        
        SparkStandaloneAutoscalingConfig.initialize(this);
    }

    /**
     * Initializes the fields of this object.
     * This method is used by the constructors of any subclasses, in order to implement multiple inheritance (mix-ins).
     * Only for internal use.
     */
    static initialize(obj) { 
    }

    /**
     * Constructs a <code>SparkStandaloneAutoscalingConfig</code> from a plain JavaScript object, optionally creating a new instance.
     * Copies all relevant properties from <code>data</code> to <code>obj</code> if supplied or a new instance if not.
     * @param {Object} data The plain JavaScript object bearing properties of interest.
     * @param {module:model/SparkStandaloneAutoscalingConfig} obj Optional instance to populate.
     * @return {module:model/SparkStandaloneAutoscalingConfig} The populated <code>SparkStandaloneAutoscalingConfig</code> instance.
     */
    static constructFromObject(data, obj) {
        if (data) {
            obj = obj || new SparkStandaloneAutoscalingConfig();

            if (data.hasOwnProperty('gracefulDecommissionTimeout')) {
                obj['gracefulDecommissionTimeout'] = ApiClient.convertToType(data['gracefulDecommissionTimeout'], 'String');
            }
            if (data.hasOwnProperty('removeOnlyIdleWorkers')) {
                obj['removeOnlyIdleWorkers'] = ApiClient.convertToType(data['removeOnlyIdleWorkers'], 'Boolean');
            }
            if (data.hasOwnProperty('scaleDownFactor')) {
                obj['scaleDownFactor'] = ApiClient.convertToType(data['scaleDownFactor'], 'Number');
            }
            if (data.hasOwnProperty('scaleDownMinWorkerFraction')) {
                obj['scaleDownMinWorkerFraction'] = ApiClient.convertToType(data['scaleDownMinWorkerFraction'], 'Number');
            }
            if (data.hasOwnProperty('scaleUpFactor')) {
                obj['scaleUpFactor'] = ApiClient.convertToType(data['scaleUpFactor'], 'Number');
            }
            if (data.hasOwnProperty('scaleUpMinWorkerFraction')) {
                obj['scaleUpMinWorkerFraction'] = ApiClient.convertToType(data['scaleUpMinWorkerFraction'], 'Number');
            }
        }
        return obj;
    }

    /**
     * Validates the JSON data with respect to <code>SparkStandaloneAutoscalingConfig</code>.
     * @param {Object} data The plain JavaScript object bearing properties of interest.
     * @return {boolean} to indicate whether the JSON data is valid with respect to <code>SparkStandaloneAutoscalingConfig</code>.
     */
    static validateJSON(data) {
        // ensure the json data is a string
        if (data['gracefulDecommissionTimeout'] && !(typeof data['gracefulDecommissionTimeout'] === 'string' || data['gracefulDecommissionTimeout'] instanceof String)) {
            throw new Error("Expected the field `gracefulDecommissionTimeout` to be a primitive type in the JSON string but got " + data['gracefulDecommissionTimeout']);
        }

        return true;
    }


}



/**
 * Required. Timeout for Spark graceful decommissioning of spark workers. Specifies the duration to wait for spark worker to complete spark decommissioning tasks before forcefully removing workers. Only applicable to downscaling operations.Bounds: 0s, 1d.
 * @member {String} gracefulDecommissionTimeout
 */
SparkStandaloneAutoscalingConfig.prototype['gracefulDecommissionTimeout'] = undefined;

/**
 * Optional. Remove only idle workers when scaling down cluster
 * @member {Boolean} removeOnlyIdleWorkers
 */
SparkStandaloneAutoscalingConfig.prototype['removeOnlyIdleWorkers'] = undefined;

/**
 * Required. Fraction of required executors to remove from Spark Serverless clusters. A scale-down factor of 1.0 will result in scaling down so that there are no more executors for the Spark Job.(more aggressive scaling). A scale-down factor closer to 0 will result in a smaller magnitude of scaling donw (less aggressive scaling).Bounds: 0.0, 1.0.
 * @member {Number} scaleDownFactor
 */
SparkStandaloneAutoscalingConfig.prototype['scaleDownFactor'] = undefined;

/**
 * Optional. Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
 * @member {Number} scaleDownMinWorkerFraction
 */
SparkStandaloneAutoscalingConfig.prototype['scaleDownMinWorkerFraction'] = undefined;

/**
 * Required. Fraction of required workers to add to Spark Standalone clusters. A scale-up factor of 1.0 will result in scaling up so that there are no more required workers for the Spark Job (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling).Bounds: 0.0, 1.0.
 * @member {Number} scaleUpFactor
 */
SparkStandaloneAutoscalingConfig.prototype['scaleUpFactor'] = undefined;

/**
 * Optional. Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
 * @member {Number} scaleUpMinWorkerFraction
 */
SparkStandaloneAutoscalingConfig.prototype['scaleUpMinWorkerFraction'] = undefined;






export default SparkStandaloneAutoscalingConfig;

