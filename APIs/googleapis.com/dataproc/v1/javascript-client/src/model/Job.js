/**
 * Cloud Dataproc API
 * Manages Hadoop-based clusters and jobs on Google Cloud Platform.
 *
 * The version of the OpenAPI document: v1
 * 
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 *
 */

import ApiClient from '../ApiClient';
import DriverSchedulingConfig from './DriverSchedulingConfig';
import FlinkJob from './FlinkJob';
import HadoopJob from './HadoopJob';
import HiveJob from './HiveJob';
import JobPlacement from './JobPlacement';
import JobReference from './JobReference';
import JobScheduling from './JobScheduling';
import JobStatus from './JobStatus';
import PigJob from './PigJob';
import PrestoJob from './PrestoJob';
import PySparkJob from './PySparkJob';
import SparkJob from './SparkJob';
import SparkRJob from './SparkRJob';
import SparkSqlJob from './SparkSqlJob';
import TrinoJob from './TrinoJob';
import YarnApplication from './YarnApplication';

/**
 * The Job model module.
 * @module model/Job
 * @version v1
 */
class Job {
    /**
     * Constructs a new <code>Job</code>.
     * A Dataproc job resource.
     * @alias module:model/Job
     */
    constructor() { 
        
        Job.initialize(this);
    }

    /**
     * Initializes the fields of this object.
     * This method is used by the constructors of any subclasses, in order to implement multiple inheritance (mix-ins).
     * Only for internal use.
     */
    static initialize(obj) { 
    }

    /**
     * Constructs a <code>Job</code> from a plain JavaScript object, optionally creating a new instance.
     * Copies all relevant properties from <code>data</code> to <code>obj</code> if supplied or a new instance if not.
     * @param {Object} data The plain JavaScript object bearing properties of interest.
     * @param {module:model/Job} obj Optional instance to populate.
     * @return {module:model/Job} The populated <code>Job</code> instance.
     */
    static constructFromObject(data, obj) {
        if (data) {
            obj = obj || new Job();

            if (data.hasOwnProperty('done')) {
                obj['done'] = ApiClient.convertToType(data['done'], 'Boolean');
            }
            if (data.hasOwnProperty('driverControlFilesUri')) {
                obj['driverControlFilesUri'] = ApiClient.convertToType(data['driverControlFilesUri'], 'String');
            }
            if (data.hasOwnProperty('driverOutputResourceUri')) {
                obj['driverOutputResourceUri'] = ApiClient.convertToType(data['driverOutputResourceUri'], 'String');
            }
            if (data.hasOwnProperty('driverSchedulingConfig')) {
                obj['driverSchedulingConfig'] = DriverSchedulingConfig.constructFromObject(data['driverSchedulingConfig']);
            }
            if (data.hasOwnProperty('flinkJob')) {
                obj['flinkJob'] = FlinkJob.constructFromObject(data['flinkJob']);
            }
            if (data.hasOwnProperty('hadoopJob')) {
                obj['hadoopJob'] = HadoopJob.constructFromObject(data['hadoopJob']);
            }
            if (data.hasOwnProperty('hiveJob')) {
                obj['hiveJob'] = HiveJob.constructFromObject(data['hiveJob']);
            }
            if (data.hasOwnProperty('jobUuid')) {
                obj['jobUuid'] = ApiClient.convertToType(data['jobUuid'], 'String');
            }
            if (data.hasOwnProperty('labels')) {
                obj['labels'] = ApiClient.convertToType(data['labels'], {'String': 'String'});
            }
            if (data.hasOwnProperty('pigJob')) {
                obj['pigJob'] = PigJob.constructFromObject(data['pigJob']);
            }
            if (data.hasOwnProperty('placement')) {
                obj['placement'] = JobPlacement.constructFromObject(data['placement']);
            }
            if (data.hasOwnProperty('prestoJob')) {
                obj['prestoJob'] = PrestoJob.constructFromObject(data['prestoJob']);
            }
            if (data.hasOwnProperty('pysparkJob')) {
                obj['pysparkJob'] = PySparkJob.constructFromObject(data['pysparkJob']);
            }
            if (data.hasOwnProperty('reference')) {
                obj['reference'] = JobReference.constructFromObject(data['reference']);
            }
            if (data.hasOwnProperty('scheduling')) {
                obj['scheduling'] = JobScheduling.constructFromObject(data['scheduling']);
            }
            if (data.hasOwnProperty('sparkJob')) {
                obj['sparkJob'] = SparkJob.constructFromObject(data['sparkJob']);
            }
            if (data.hasOwnProperty('sparkRJob')) {
                obj['sparkRJob'] = SparkRJob.constructFromObject(data['sparkRJob']);
            }
            if (data.hasOwnProperty('sparkSqlJob')) {
                obj['sparkSqlJob'] = SparkSqlJob.constructFromObject(data['sparkSqlJob']);
            }
            if (data.hasOwnProperty('status')) {
                obj['status'] = JobStatus.constructFromObject(data['status']);
            }
            if (data.hasOwnProperty('statusHistory')) {
                obj['statusHistory'] = ApiClient.convertToType(data['statusHistory'], [JobStatus]);
            }
            if (data.hasOwnProperty('trinoJob')) {
                obj['trinoJob'] = TrinoJob.constructFromObject(data['trinoJob']);
            }
            if (data.hasOwnProperty('yarnApplications')) {
                obj['yarnApplications'] = ApiClient.convertToType(data['yarnApplications'], [YarnApplication]);
            }
        }
        return obj;
    }

    /**
     * Validates the JSON data with respect to <code>Job</code>.
     * @param {Object} data The plain JavaScript object bearing properties of interest.
     * @return {boolean} to indicate whether the JSON data is valid with respect to <code>Job</code>.
     */
    static validateJSON(data) {
        // ensure the json data is a string
        if (data['driverControlFilesUri'] && !(typeof data['driverControlFilesUri'] === 'string' || data['driverControlFilesUri'] instanceof String)) {
            throw new Error("Expected the field `driverControlFilesUri` to be a primitive type in the JSON string but got " + data['driverControlFilesUri']);
        }
        // ensure the json data is a string
        if (data['driverOutputResourceUri'] && !(typeof data['driverOutputResourceUri'] === 'string' || data['driverOutputResourceUri'] instanceof String)) {
            throw new Error("Expected the field `driverOutputResourceUri` to be a primitive type in the JSON string but got " + data['driverOutputResourceUri']);
        }
        // validate the optional field `driverSchedulingConfig`
        if (data['driverSchedulingConfig']) { // data not null
          DriverSchedulingConfig.validateJSON(data['driverSchedulingConfig']);
        }
        // validate the optional field `flinkJob`
        if (data['flinkJob']) { // data not null
          FlinkJob.validateJSON(data['flinkJob']);
        }
        // validate the optional field `hadoopJob`
        if (data['hadoopJob']) { // data not null
          HadoopJob.validateJSON(data['hadoopJob']);
        }
        // validate the optional field `hiveJob`
        if (data['hiveJob']) { // data not null
          HiveJob.validateJSON(data['hiveJob']);
        }
        // ensure the json data is a string
        if (data['jobUuid'] && !(typeof data['jobUuid'] === 'string' || data['jobUuid'] instanceof String)) {
            throw new Error("Expected the field `jobUuid` to be a primitive type in the JSON string but got " + data['jobUuid']);
        }
        // validate the optional field `pigJob`
        if (data['pigJob']) { // data not null
          PigJob.validateJSON(data['pigJob']);
        }
        // validate the optional field `placement`
        if (data['placement']) { // data not null
          JobPlacement.validateJSON(data['placement']);
        }
        // validate the optional field `prestoJob`
        if (data['prestoJob']) { // data not null
          PrestoJob.validateJSON(data['prestoJob']);
        }
        // validate the optional field `pysparkJob`
        if (data['pysparkJob']) { // data not null
          PySparkJob.validateJSON(data['pysparkJob']);
        }
        // validate the optional field `reference`
        if (data['reference']) { // data not null
          JobReference.validateJSON(data['reference']);
        }
        // validate the optional field `scheduling`
        if (data['scheduling']) { // data not null
          JobScheduling.validateJSON(data['scheduling']);
        }
        // validate the optional field `sparkJob`
        if (data['sparkJob']) { // data not null
          SparkJob.validateJSON(data['sparkJob']);
        }
        // validate the optional field `sparkRJob`
        if (data['sparkRJob']) { // data not null
          SparkRJob.validateJSON(data['sparkRJob']);
        }
        // validate the optional field `sparkSqlJob`
        if (data['sparkSqlJob']) { // data not null
          SparkSqlJob.validateJSON(data['sparkSqlJob']);
        }
        // validate the optional field `status`
        if (data['status']) { // data not null
          JobStatus.validateJSON(data['status']);
        }
        if (data['statusHistory']) { // data not null
            // ensure the json data is an array
            if (!Array.isArray(data['statusHistory'])) {
                throw new Error("Expected the field `statusHistory` to be an array in the JSON data but got " + data['statusHistory']);
            }
            // validate the optional field `statusHistory` (array)
            for (const item of data['statusHistory']) {
                JobStatus.validateJSON(item);
            };
        }
        // validate the optional field `trinoJob`
        if (data['trinoJob']) { // data not null
          TrinoJob.validateJSON(data['trinoJob']);
        }
        if (data['yarnApplications']) { // data not null
            // ensure the json data is an array
            if (!Array.isArray(data['yarnApplications'])) {
                throw new Error("Expected the field `yarnApplications` to be an array in the JSON data but got " + data['yarnApplications']);
            }
            // validate the optional field `yarnApplications` (array)
            for (const item of data['yarnApplications']) {
                YarnApplication.validateJSON(item);
            };
        }

        return true;
    }


}



/**
 * Output only. Indicates whether the job is completed. If the value is false, the job is still in progress. If true, the job is completed, and status.state field will indicate if it was successful, failed, or cancelled.
 * @member {Boolean} done
 */
Job.prototype['done'] = undefined;

/**
 * Output only. If present, the location of miscellaneous control files which can be used as part of job setup and handling. If not present, control files might be placed in the same location as driver_output_uri.
 * @member {String} driverControlFilesUri
 */
Job.prototype['driverControlFilesUri'] = undefined;

/**
 * Output only. A URI pointing to the location of the stdout of the job's driver program.
 * @member {String} driverOutputResourceUri
 */
Job.prototype['driverOutputResourceUri'] = undefined;

/**
 * @member {module:model/DriverSchedulingConfig} driverSchedulingConfig
 */
Job.prototype['driverSchedulingConfig'] = undefined;

/**
 * @member {module:model/FlinkJob} flinkJob
 */
Job.prototype['flinkJob'] = undefined;

/**
 * @member {module:model/HadoopJob} hadoopJob
 */
Job.prototype['hadoopJob'] = undefined;

/**
 * @member {module:model/HiveJob} hiveJob
 */
Job.prototype['hiveJob'] = undefined;

/**
 * Output only. A UUID that uniquely identifies a job within the project over time. This is in contrast to a user-settable reference.job_id that might be reused over time.
 * @member {String} jobUuid
 */
Job.prototype['jobUuid'] = undefined;

/**
 * Optional. The labels to associate with this job. Label keys must contain 1 to 63 characters, and must conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). Label values can be empty, but, if present, must contain 1 to 63 characters, and must conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). No more than 32 labels can be associated with a job.
 * @member {Object.<String, String>} labels
 */
Job.prototype['labels'] = undefined;

/**
 * @member {module:model/PigJob} pigJob
 */
Job.prototype['pigJob'] = undefined;

/**
 * @member {module:model/JobPlacement} placement
 */
Job.prototype['placement'] = undefined;

/**
 * @member {module:model/PrestoJob} prestoJob
 */
Job.prototype['prestoJob'] = undefined;

/**
 * @member {module:model/PySparkJob} pysparkJob
 */
Job.prototype['pysparkJob'] = undefined;

/**
 * @member {module:model/JobReference} reference
 */
Job.prototype['reference'] = undefined;

/**
 * @member {module:model/JobScheduling} scheduling
 */
Job.prototype['scheduling'] = undefined;

/**
 * @member {module:model/SparkJob} sparkJob
 */
Job.prototype['sparkJob'] = undefined;

/**
 * @member {module:model/SparkRJob} sparkRJob
 */
Job.prototype['sparkRJob'] = undefined;

/**
 * @member {module:model/SparkSqlJob} sparkSqlJob
 */
Job.prototype['sparkSqlJob'] = undefined;

/**
 * @member {module:model/JobStatus} status
 */
Job.prototype['status'] = undefined;

/**
 * Output only. The previous job status.
 * @member {Array.<module:model/JobStatus>} statusHistory
 */
Job.prototype['statusHistory'] = undefined;

/**
 * @member {module:model/TrinoJob} trinoJob
 */
Job.prototype['trinoJob'] = undefined;

/**
 * Output only. The collection of YARN applications spun up by this job.Beta Feature: This report is available for testing purposes only. It might be changed before final release.
 * @member {Array.<module:model/YarnApplication>} yarnApplications
 */
Job.prototype['yarnApplications'] = undefined;






export default Job;

