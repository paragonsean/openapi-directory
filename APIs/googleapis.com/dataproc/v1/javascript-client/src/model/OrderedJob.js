/**
 * Cloud Dataproc API
 * Manages Hadoop-based clusters and jobs on Google Cloud Platform.
 *
 * The version of the OpenAPI document: v1
 * 
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 *
 */

import ApiClient from '../ApiClient';
import FlinkJob from './FlinkJob';
import HadoopJob from './HadoopJob';
import HiveJob from './HiveJob';
import JobScheduling from './JobScheduling';
import PigJob from './PigJob';
import PrestoJob from './PrestoJob';
import PySparkJob from './PySparkJob';
import SparkJob from './SparkJob';
import SparkRJob from './SparkRJob';
import SparkSqlJob from './SparkSqlJob';
import TrinoJob from './TrinoJob';

/**
 * The OrderedJob model module.
 * @module model/OrderedJob
 * @version v1
 */
class OrderedJob {
    /**
     * Constructs a new <code>OrderedJob</code>.
     * A job executed by the workflow.
     * @alias module:model/OrderedJob
     */
    constructor() { 
        
        OrderedJob.initialize(this);
    }

    /**
     * Initializes the fields of this object.
     * This method is used by the constructors of any subclasses, in order to implement multiple inheritance (mix-ins).
     * Only for internal use.
     */
    static initialize(obj) { 
    }

    /**
     * Constructs a <code>OrderedJob</code> from a plain JavaScript object, optionally creating a new instance.
     * Copies all relevant properties from <code>data</code> to <code>obj</code> if supplied or a new instance if not.
     * @param {Object} data The plain JavaScript object bearing properties of interest.
     * @param {module:model/OrderedJob} obj Optional instance to populate.
     * @return {module:model/OrderedJob} The populated <code>OrderedJob</code> instance.
     */
    static constructFromObject(data, obj) {
        if (data) {
            obj = obj || new OrderedJob();

            if (data.hasOwnProperty('flinkJob')) {
                obj['flinkJob'] = FlinkJob.constructFromObject(data['flinkJob']);
            }
            if (data.hasOwnProperty('hadoopJob')) {
                obj['hadoopJob'] = HadoopJob.constructFromObject(data['hadoopJob']);
            }
            if (data.hasOwnProperty('hiveJob')) {
                obj['hiveJob'] = HiveJob.constructFromObject(data['hiveJob']);
            }
            if (data.hasOwnProperty('labels')) {
                obj['labels'] = ApiClient.convertToType(data['labels'], {'String': 'String'});
            }
            if (data.hasOwnProperty('pigJob')) {
                obj['pigJob'] = PigJob.constructFromObject(data['pigJob']);
            }
            if (data.hasOwnProperty('prerequisiteStepIds')) {
                obj['prerequisiteStepIds'] = ApiClient.convertToType(data['prerequisiteStepIds'], ['String']);
            }
            if (data.hasOwnProperty('prestoJob')) {
                obj['prestoJob'] = PrestoJob.constructFromObject(data['prestoJob']);
            }
            if (data.hasOwnProperty('pysparkJob')) {
                obj['pysparkJob'] = PySparkJob.constructFromObject(data['pysparkJob']);
            }
            if (data.hasOwnProperty('scheduling')) {
                obj['scheduling'] = JobScheduling.constructFromObject(data['scheduling']);
            }
            if (data.hasOwnProperty('sparkJob')) {
                obj['sparkJob'] = SparkJob.constructFromObject(data['sparkJob']);
            }
            if (data.hasOwnProperty('sparkRJob')) {
                obj['sparkRJob'] = SparkRJob.constructFromObject(data['sparkRJob']);
            }
            if (data.hasOwnProperty('sparkSqlJob')) {
                obj['sparkSqlJob'] = SparkSqlJob.constructFromObject(data['sparkSqlJob']);
            }
            if (data.hasOwnProperty('stepId')) {
                obj['stepId'] = ApiClient.convertToType(data['stepId'], 'String');
            }
            if (data.hasOwnProperty('trinoJob')) {
                obj['trinoJob'] = TrinoJob.constructFromObject(data['trinoJob']);
            }
        }
        return obj;
    }

    /**
     * Validates the JSON data with respect to <code>OrderedJob</code>.
     * @param {Object} data The plain JavaScript object bearing properties of interest.
     * @return {boolean} to indicate whether the JSON data is valid with respect to <code>OrderedJob</code>.
     */
    static validateJSON(data) {
        // validate the optional field `flinkJob`
        if (data['flinkJob']) { // data not null
          FlinkJob.validateJSON(data['flinkJob']);
        }
        // validate the optional field `hadoopJob`
        if (data['hadoopJob']) { // data not null
          HadoopJob.validateJSON(data['hadoopJob']);
        }
        // validate the optional field `hiveJob`
        if (data['hiveJob']) { // data not null
          HiveJob.validateJSON(data['hiveJob']);
        }
        // validate the optional field `pigJob`
        if (data['pigJob']) { // data not null
          PigJob.validateJSON(data['pigJob']);
        }
        // ensure the json data is an array
        if (!Array.isArray(data['prerequisiteStepIds'])) {
            throw new Error("Expected the field `prerequisiteStepIds` to be an array in the JSON data but got " + data['prerequisiteStepIds']);
        }
        // validate the optional field `prestoJob`
        if (data['prestoJob']) { // data not null
          PrestoJob.validateJSON(data['prestoJob']);
        }
        // validate the optional field `pysparkJob`
        if (data['pysparkJob']) { // data not null
          PySparkJob.validateJSON(data['pysparkJob']);
        }
        // validate the optional field `scheduling`
        if (data['scheduling']) { // data not null
          JobScheduling.validateJSON(data['scheduling']);
        }
        // validate the optional field `sparkJob`
        if (data['sparkJob']) { // data not null
          SparkJob.validateJSON(data['sparkJob']);
        }
        // validate the optional field `sparkRJob`
        if (data['sparkRJob']) { // data not null
          SparkRJob.validateJSON(data['sparkRJob']);
        }
        // validate the optional field `sparkSqlJob`
        if (data['sparkSqlJob']) { // data not null
          SparkSqlJob.validateJSON(data['sparkSqlJob']);
        }
        // ensure the json data is a string
        if (data['stepId'] && !(typeof data['stepId'] === 'string' || data['stepId'] instanceof String)) {
            throw new Error("Expected the field `stepId` to be a primitive type in the JSON string but got " + data['stepId']);
        }
        // validate the optional field `trinoJob`
        if (data['trinoJob']) { // data not null
          TrinoJob.validateJSON(data['trinoJob']);
        }

        return true;
    }


}



/**
 * @member {module:model/FlinkJob} flinkJob
 */
OrderedJob.prototype['flinkJob'] = undefined;

/**
 * @member {module:model/HadoopJob} hadoopJob
 */
OrderedJob.prototype['hadoopJob'] = undefined;

/**
 * @member {module:model/HiveJob} hiveJob
 */
OrderedJob.prototype['hiveJob'] = undefined;

/**
 * Optional. The labels to associate with this job.Label keys must be between 1 and 63 characters long, and must conform to the following regular expression: \\p{Ll}\\p{Lo}{0,62}Label values must be between 1 and 63 characters long, and must conform to the following regular expression: \\p{Ll}\\p{Lo}\\p{N}_-{0,63}No more than 32 labels can be associated with a given job.
 * @member {Object.<String, String>} labels
 */
OrderedJob.prototype['labels'] = undefined;

/**
 * @member {module:model/PigJob} pigJob
 */
OrderedJob.prototype['pigJob'] = undefined;

/**
 * Optional. The optional list of prerequisite job step_ids. If not specified, the job will start at the beginning of workflow.
 * @member {Array.<String>} prerequisiteStepIds
 */
OrderedJob.prototype['prerequisiteStepIds'] = undefined;

/**
 * @member {module:model/PrestoJob} prestoJob
 */
OrderedJob.prototype['prestoJob'] = undefined;

/**
 * @member {module:model/PySparkJob} pysparkJob
 */
OrderedJob.prototype['pysparkJob'] = undefined;

/**
 * @member {module:model/JobScheduling} scheduling
 */
OrderedJob.prototype['scheduling'] = undefined;

/**
 * @member {module:model/SparkJob} sparkJob
 */
OrderedJob.prototype['sparkJob'] = undefined;

/**
 * @member {module:model/SparkRJob} sparkRJob
 */
OrderedJob.prototype['sparkRJob'] = undefined;

/**
 * @member {module:model/SparkSqlJob} sparkSqlJob
 */
OrderedJob.prototype['sparkSqlJob'] = undefined;

/**
 * Required. The step id. The id must be unique among all jobs within the template.The step id is used as prefix for job id, as job goog-dataproc-workflow-step-id label, and in prerequisiteStepIds field from other steps.The id must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of between 3 and 50 characters.
 * @member {String} stepId
 */
OrderedJob.prototype['stepId'] = undefined;

/**
 * @member {module:model/TrinoJob} trinoJob
 */
OrderedJob.prototype['trinoJob'] = undefined;






export default OrderedJob;

