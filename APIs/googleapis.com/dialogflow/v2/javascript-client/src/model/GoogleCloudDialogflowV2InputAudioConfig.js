/**
 * Dialogflow API
 * Builds conversational interfaces (for example, chatbots, and voice-powered apps and devices).
 *
 * The version of the OpenAPI document: v2
 * 
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 *
 */

import ApiClient from '../ApiClient';
import GoogleCloudDialogflowV2SpeechContext from './GoogleCloudDialogflowV2SpeechContext';

/**
 * The GoogleCloudDialogflowV2InputAudioConfig model module.
 * @module model/GoogleCloudDialogflowV2InputAudioConfig
 * @version v2
 */
class GoogleCloudDialogflowV2InputAudioConfig {
    /**
     * Constructs a new <code>GoogleCloudDialogflowV2InputAudioConfig</code>.
     * Instructs the speech recognizer how to process the audio content.
     * @alias module:model/GoogleCloudDialogflowV2InputAudioConfig
     */
    constructor() { 
        
        GoogleCloudDialogflowV2InputAudioConfig.initialize(this);
    }

    /**
     * Initializes the fields of this object.
     * This method is used by the constructors of any subclasses, in order to implement multiple inheritance (mix-ins).
     * Only for internal use.
     */
    static initialize(obj) { 
    }

    /**
     * Constructs a <code>GoogleCloudDialogflowV2InputAudioConfig</code> from a plain JavaScript object, optionally creating a new instance.
     * Copies all relevant properties from <code>data</code> to <code>obj</code> if supplied or a new instance if not.
     * @param {Object} data The plain JavaScript object bearing properties of interest.
     * @param {module:model/GoogleCloudDialogflowV2InputAudioConfig} obj Optional instance to populate.
     * @return {module:model/GoogleCloudDialogflowV2InputAudioConfig} The populated <code>GoogleCloudDialogflowV2InputAudioConfig</code> instance.
     */
    static constructFromObject(data, obj) {
        if (data) {
            obj = obj || new GoogleCloudDialogflowV2InputAudioConfig();

            if (data.hasOwnProperty('audioEncoding')) {
                obj['audioEncoding'] = ApiClient.convertToType(data['audioEncoding'], 'String');
            }
            if (data.hasOwnProperty('disableNoSpeechRecognizedEvent')) {
                obj['disableNoSpeechRecognizedEvent'] = ApiClient.convertToType(data['disableNoSpeechRecognizedEvent'], 'Boolean');
            }
            if (data.hasOwnProperty('enableAutomaticPunctuation')) {
                obj['enableAutomaticPunctuation'] = ApiClient.convertToType(data['enableAutomaticPunctuation'], 'Boolean');
            }
            if (data.hasOwnProperty('enableWordInfo')) {
                obj['enableWordInfo'] = ApiClient.convertToType(data['enableWordInfo'], 'Boolean');
            }
            if (data.hasOwnProperty('languageCode')) {
                obj['languageCode'] = ApiClient.convertToType(data['languageCode'], 'String');
            }
            if (data.hasOwnProperty('model')) {
                obj['model'] = ApiClient.convertToType(data['model'], 'String');
            }
            if (data.hasOwnProperty('modelVariant')) {
                obj['modelVariant'] = ApiClient.convertToType(data['modelVariant'], 'String');
            }
            if (data.hasOwnProperty('optOutConformerModelMigration')) {
                obj['optOutConformerModelMigration'] = ApiClient.convertToType(data['optOutConformerModelMigration'], 'Boolean');
            }
            if (data.hasOwnProperty('phraseHints')) {
                obj['phraseHints'] = ApiClient.convertToType(data['phraseHints'], ['String']);
            }
            if (data.hasOwnProperty('sampleRateHertz')) {
                obj['sampleRateHertz'] = ApiClient.convertToType(data['sampleRateHertz'], 'Number');
            }
            if (data.hasOwnProperty('singleUtterance')) {
                obj['singleUtterance'] = ApiClient.convertToType(data['singleUtterance'], 'Boolean');
            }
            if (data.hasOwnProperty('speechContexts')) {
                obj['speechContexts'] = ApiClient.convertToType(data['speechContexts'], [GoogleCloudDialogflowV2SpeechContext]);
            }
        }
        return obj;
    }

    /**
     * Validates the JSON data with respect to <code>GoogleCloudDialogflowV2InputAudioConfig</code>.
     * @param {Object} data The plain JavaScript object bearing properties of interest.
     * @return {boolean} to indicate whether the JSON data is valid with respect to <code>GoogleCloudDialogflowV2InputAudioConfig</code>.
     */
    static validateJSON(data) {
        // ensure the json data is a string
        if (data['audioEncoding'] && !(typeof data['audioEncoding'] === 'string' || data['audioEncoding'] instanceof String)) {
            throw new Error("Expected the field `audioEncoding` to be a primitive type in the JSON string but got " + data['audioEncoding']);
        }
        // ensure the json data is a string
        if (data['languageCode'] && !(typeof data['languageCode'] === 'string' || data['languageCode'] instanceof String)) {
            throw new Error("Expected the field `languageCode` to be a primitive type in the JSON string but got " + data['languageCode']);
        }
        // ensure the json data is a string
        if (data['model'] && !(typeof data['model'] === 'string' || data['model'] instanceof String)) {
            throw new Error("Expected the field `model` to be a primitive type in the JSON string but got " + data['model']);
        }
        // ensure the json data is a string
        if (data['modelVariant'] && !(typeof data['modelVariant'] === 'string' || data['modelVariant'] instanceof String)) {
            throw new Error("Expected the field `modelVariant` to be a primitive type in the JSON string but got " + data['modelVariant']);
        }
        // ensure the json data is an array
        if (!Array.isArray(data['phraseHints'])) {
            throw new Error("Expected the field `phraseHints` to be an array in the JSON data but got " + data['phraseHints']);
        }
        if (data['speechContexts']) { // data not null
            // ensure the json data is an array
            if (!Array.isArray(data['speechContexts'])) {
                throw new Error("Expected the field `speechContexts` to be an array in the JSON data but got " + data['speechContexts']);
            }
            // validate the optional field `speechContexts` (array)
            for (const item of data['speechContexts']) {
                GoogleCloudDialogflowV2SpeechContext.validateJSON(item);
            };
        }

        return true;
    }


}



/**
 * Required. Audio encoding of the audio content to process.
 * @member {module:model/GoogleCloudDialogflowV2InputAudioConfig.AudioEncodingEnum} audioEncoding
 */
GoogleCloudDialogflowV2InputAudioConfig.prototype['audioEncoding'] = undefined;

/**
 * Only used in Participants.AnalyzeContent and Participants.StreamingAnalyzeContent. If `false` and recognition doesn't return any result, trigger `NO_SPEECH_RECOGNIZED` event to Dialogflow agent.
 * @member {Boolean} disableNoSpeechRecognizedEvent
 */
GoogleCloudDialogflowV2InputAudioConfig.prototype['disableNoSpeechRecognizedEvent'] = undefined;

/**
 * Enable automatic punctuation option at the speech backend.
 * @member {Boolean} enableAutomaticPunctuation
 */
GoogleCloudDialogflowV2InputAudioConfig.prototype['enableAutomaticPunctuation'] = undefined;

/**
 * If `true`, Dialogflow returns SpeechWordInfo in StreamingRecognitionResult with information about the recognized speech words, e.g. start and end time offsets. If false or unspecified, Speech doesn't return any word-level information.
 * @member {Boolean} enableWordInfo
 */
GoogleCloudDialogflowV2InputAudioConfig.prototype['enableWordInfo'] = undefined;

/**
 * Required. The language of the supplied audio. Dialogflow does not do translations. See [Language Support](https://cloud.google.com/dialogflow/docs/reference/language) for a list of the currently supported language codes. Note that queries in the same session do not necessarily need to specify the same language.
 * @member {String} languageCode
 */
GoogleCloudDialogflowV2InputAudioConfig.prototype['languageCode'] = undefined;

/**
 * Optional. Which Speech model to select for the given request. For more information, see [Speech models](https://cloud.google.com/dialogflow/es/docs/speech-models).
 * @member {String} model
 */
GoogleCloudDialogflowV2InputAudioConfig.prototype['model'] = undefined;

/**
 * Which variant of the Speech model to use.
 * @member {module:model/GoogleCloudDialogflowV2InputAudioConfig.ModelVariantEnum} modelVariant
 */
GoogleCloudDialogflowV2InputAudioConfig.prototype['modelVariant'] = undefined;

/**
 * If `true`, the request will opt out for STT conformer model migration. This field will be deprecated once force migration takes place in June 2024. Please refer to [Dialogflow ES Speech model migration](https://cloud.google.com/dialogflow/es/docs/speech-model-migration).
 * @member {Boolean} optOutConformerModelMigration
 */
GoogleCloudDialogflowV2InputAudioConfig.prototype['optOutConformerModelMigration'] = undefined;

/**
 * A list of strings containing words and phrases that the speech recognizer should recognize with higher likelihood. See [the Cloud Speech documentation](https://cloud.google.com/speech-to-text/docs/basics#phrase-hints) for more details. This field is deprecated. Please use [`speech_contexts`]() instead. If you specify both [`phrase_hints`]() and [`speech_contexts`](), Dialogflow will treat the [`phrase_hints`]() as a single additional [`SpeechContext`]().
 * @member {Array.<String>} phraseHints
 */
GoogleCloudDialogflowV2InputAudioConfig.prototype['phraseHints'] = undefined;

/**
 * Required. Sample rate (in Hertz) of the audio content sent in the query. Refer to [Cloud Speech API documentation](https://cloud.google.com/speech-to-text/docs/basics) for more details.
 * @member {Number} sampleRateHertz
 */
GoogleCloudDialogflowV2InputAudioConfig.prototype['sampleRateHertz'] = undefined;

/**
 * If `false` (default), recognition does not cease until the client closes the stream. If `true`, the recognizer will detect a single spoken utterance in input audio. Recognition ceases when it detects the audio's voice has stopped or paused. In this case, once a detected intent is received, the client should close the stream and start a new request with a new stream as needed. Note: This setting is relevant only for streaming methods. Note: When specified, InputAudioConfig.single_utterance takes precedence over StreamingDetectIntentRequest.single_utterance.
 * @member {Boolean} singleUtterance
 */
GoogleCloudDialogflowV2InputAudioConfig.prototype['singleUtterance'] = undefined;

/**
 * Context information to assist speech recognition. See [the Cloud Speech documentation](https://cloud.google.com/speech-to-text/docs/basics#phrase-hints) for more details.
 * @member {Array.<module:model/GoogleCloudDialogflowV2SpeechContext>} speechContexts
 */
GoogleCloudDialogflowV2InputAudioConfig.prototype['speechContexts'] = undefined;





/**
 * Allowed values for the <code>audioEncoding</code> property.
 * @enum {String}
 * @readonly
 */
GoogleCloudDialogflowV2InputAudioConfig['AudioEncodingEnum'] = {

    /**
     * value: "AUDIO_ENCODING_UNSPECIFIED"
     * @const
     */
    "UNSPECIFIED": "AUDIO_ENCODING_UNSPECIFIED",

    /**
     * value: "AUDIO_ENCODING_LINEAR_16"
     * @const
     */
    "LINEAR_16": "AUDIO_ENCODING_LINEAR_16",

    /**
     * value: "AUDIO_ENCODING_FLAC"
     * @const
     */
    "FLAC": "AUDIO_ENCODING_FLAC",

    /**
     * value: "AUDIO_ENCODING_MULAW"
     * @const
     */
    "MULAW": "AUDIO_ENCODING_MULAW",

    /**
     * value: "AUDIO_ENCODING_AMR"
     * @const
     */
    "AMR": "AUDIO_ENCODING_AMR",

    /**
     * value: "AUDIO_ENCODING_AMR_WB"
     * @const
     */
    "AMR_WB": "AUDIO_ENCODING_AMR_WB",

    /**
     * value: "AUDIO_ENCODING_OGG_OPUS"
     * @const
     */
    "OGG_OPUS": "AUDIO_ENCODING_OGG_OPUS",

    /**
     * value: "AUDIO_ENCODING_SPEEX_WITH_HEADER_BYTE"
     * @const
     */
    "SPEEX_WITH_HEADER_BYTE": "AUDIO_ENCODING_SPEEX_WITH_HEADER_BYTE"
};


/**
 * Allowed values for the <code>modelVariant</code> property.
 * @enum {String}
 * @readonly
 */
GoogleCloudDialogflowV2InputAudioConfig['ModelVariantEnum'] = {

    /**
     * value: "SPEECH_MODEL_VARIANT_UNSPECIFIED"
     * @const
     */
    "SPEECH_MODEL_VARIANT_UNSPECIFIED": "SPEECH_MODEL_VARIANT_UNSPECIFIED",

    /**
     * value: "USE_BEST_AVAILABLE"
     * @const
     */
    "USE_BEST_AVAILABLE": "USE_BEST_AVAILABLE",

    /**
     * value: "USE_STANDARD"
     * @const
     */
    "USE_STANDARD": "USE_STANDARD",

    /**
     * value: "USE_ENHANCED"
     * @const
     */
    "USE_ENHANCED": "USE_ENHANCED"
};



export default GoogleCloudDialogflowV2InputAudioConfig;

