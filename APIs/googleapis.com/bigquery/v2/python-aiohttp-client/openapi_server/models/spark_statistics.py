# coding: utf-8

from datetime import date, datetime

from typing import List, Dict, Type

from openapi_server.models.base_model import Model
from openapi_server.models.spark_logging_info import SparkLoggingInfo
from openapi_server import util


class SparkStatistics(Model):
    """NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).

    Do not edit the class manually.
    """

    def __init__(self, endpoints: Dict[str, str]=None, gcs_staging_bucket: str=None, kms_key_name: str=None, logging_info: SparkLoggingInfo=None, spark_job_id: str=None, spark_job_location: str=None):
        """SparkStatistics - a model defined in OpenAPI

        :param endpoints: The endpoints of this SparkStatistics.
        :param gcs_staging_bucket: The gcs_staging_bucket of this SparkStatistics.
        :param kms_key_name: The kms_key_name of this SparkStatistics.
        :param logging_info: The logging_info of this SparkStatistics.
        :param spark_job_id: The spark_job_id of this SparkStatistics.
        :param spark_job_location: The spark_job_location of this SparkStatistics.
        """
        self.openapi_types = {
            'endpoints': Dict[str, str],
            'gcs_staging_bucket': str,
            'kms_key_name': str,
            'logging_info': SparkLoggingInfo,
            'spark_job_id': str,
            'spark_job_location': str
        }

        self.attribute_map = {
            'endpoints': 'endpoints',
            'gcs_staging_bucket': 'gcsStagingBucket',
            'kms_key_name': 'kmsKeyName',
            'logging_info': 'loggingInfo',
            'spark_job_id': 'sparkJobId',
            'spark_job_location': 'sparkJobLocation'
        }

        self._endpoints = endpoints
        self._gcs_staging_bucket = gcs_staging_bucket
        self._kms_key_name = kms_key_name
        self._logging_info = logging_info
        self._spark_job_id = spark_job_id
        self._spark_job_location = spark_job_location

    @classmethod
    def from_dict(cls, dikt: dict) -> 'SparkStatistics':
        """Returns the dict as a model

        :param dikt: A dict.
        :return: The SparkStatistics of this SparkStatistics.
        """
        return util.deserialize_model(dikt, cls)

    @property
    def endpoints(self):
        """Gets the endpoints of this SparkStatistics.

        Output only. Endpoints returned from Dataproc. Key list: - history_server_endpoint: A link to Spark job UI.

        :return: The endpoints of this SparkStatistics.
        :rtype: Dict[str, str]
        """
        return self._endpoints

    @endpoints.setter
    def endpoints(self, endpoints):
        """Sets the endpoints of this SparkStatistics.

        Output only. Endpoints returned from Dataproc. Key list: - history_server_endpoint: A link to Spark job UI.

        :param endpoints: The endpoints of this SparkStatistics.
        :type endpoints: Dict[str, str]
        """

        self._endpoints = endpoints

    @property
    def gcs_staging_bucket(self):
        """Gets the gcs_staging_bucket of this SparkStatistics.

        Output only. The Google Cloud Storage bucket that is used as the default filesystem by the Spark application. This fields is only filled when the Spark procedure uses the INVOKER security mode. It is inferred from the system variable @@spark_proc_properties.staging_bucket if it is provided. Otherwise, BigQuery creates a default staging bucket for the job and returns the bucket name in this field. Example: * `gs://[bucket_name]`

        :return: The gcs_staging_bucket of this SparkStatistics.
        :rtype: str
        """
        return self._gcs_staging_bucket

    @gcs_staging_bucket.setter
    def gcs_staging_bucket(self, gcs_staging_bucket):
        """Sets the gcs_staging_bucket of this SparkStatistics.

        Output only. The Google Cloud Storage bucket that is used as the default filesystem by the Spark application. This fields is only filled when the Spark procedure uses the INVOKER security mode. It is inferred from the system variable @@spark_proc_properties.staging_bucket if it is provided. Otherwise, BigQuery creates a default staging bucket for the job and returns the bucket name in this field. Example: * `gs://[bucket_name]`

        :param gcs_staging_bucket: The gcs_staging_bucket of this SparkStatistics.
        :type gcs_staging_bucket: str
        """

        self._gcs_staging_bucket = gcs_staging_bucket

    @property
    def kms_key_name(self):
        """Gets the kms_key_name of this SparkStatistics.

        Output only. The Cloud KMS encryption key that is used to protect the resources created by the Spark job. If the Spark procedure uses DEFINER security mode, the Cloud KMS key is inferred from the Spark connection associated with the procedure if it is provided. Otherwise the key is inferred from the default key of the Spark connection's project if the CMEK organization policy is enforced. If the Spark procedure uses INVOKER security mode, the Cloud KMS encryption key is inferred from the system variable @@spark_proc_properties.kms_key_name if it is provided. Otherwise, the key is inferred fromt he default key of the BigQuery job's project if the CMEK organization policy is enforced. Example: * `projects/[kms_project_id]/locations/[region]/keyRings/[key_region]/cryptoKeys/[key]`

        :return: The kms_key_name of this SparkStatistics.
        :rtype: str
        """
        return self._kms_key_name

    @kms_key_name.setter
    def kms_key_name(self, kms_key_name):
        """Sets the kms_key_name of this SparkStatistics.

        Output only. The Cloud KMS encryption key that is used to protect the resources created by the Spark job. If the Spark procedure uses DEFINER security mode, the Cloud KMS key is inferred from the Spark connection associated with the procedure if it is provided. Otherwise the key is inferred from the default key of the Spark connection's project if the CMEK organization policy is enforced. If the Spark procedure uses INVOKER security mode, the Cloud KMS encryption key is inferred from the system variable @@spark_proc_properties.kms_key_name if it is provided. Otherwise, the key is inferred fromt he default key of the BigQuery job's project if the CMEK organization policy is enforced. Example: * `projects/[kms_project_id]/locations/[region]/keyRings/[key_region]/cryptoKeys/[key]`

        :param kms_key_name: The kms_key_name of this SparkStatistics.
        :type kms_key_name: str
        """

        self._kms_key_name = kms_key_name

    @property
    def logging_info(self):
        """Gets the logging_info of this SparkStatistics.


        :return: The logging_info of this SparkStatistics.
        :rtype: SparkLoggingInfo
        """
        return self._logging_info

    @logging_info.setter
    def logging_info(self, logging_info):
        """Sets the logging_info of this SparkStatistics.


        :param logging_info: The logging_info of this SparkStatistics.
        :type logging_info: SparkLoggingInfo
        """

        self._logging_info = logging_info

    @property
    def spark_job_id(self):
        """Gets the spark_job_id of this SparkStatistics.

        Output only. Spark job ID if a Spark job is created successfully.

        :return: The spark_job_id of this SparkStatistics.
        :rtype: str
        """
        return self._spark_job_id

    @spark_job_id.setter
    def spark_job_id(self, spark_job_id):
        """Sets the spark_job_id of this SparkStatistics.

        Output only. Spark job ID if a Spark job is created successfully.

        :param spark_job_id: The spark_job_id of this SparkStatistics.
        :type spark_job_id: str
        """

        self._spark_job_id = spark_job_id

    @property
    def spark_job_location(self):
        """Gets the spark_job_location of this SparkStatistics.

        Output only. Location where the Spark job is executed. A location is selected by BigQueury for jobs configured to run in a multi-region.

        :return: The spark_job_location of this SparkStatistics.
        :rtype: str
        """
        return self._spark_job_location

    @spark_job_location.setter
    def spark_job_location(self, spark_job_location):
        """Sets the spark_job_location of this SparkStatistics.

        Output only. Location where the Spark job is executed. A location is selected by BigQueury for jobs configured to run in a multi-region.

        :param spark_job_location: The spark_job_location of this SparkStatistics.
        :type spark_job_location: str
        """

        self._spark_job_location = spark_job_location
