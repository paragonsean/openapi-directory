# coding: utf-8

from datetime import date, datetime

from typing import List, Dict, Type

from openapi_server.models.base_model import Model
from openapi_server.models.google_type_date import GoogleTypeDate
from openapi_server import util


class GoogleCloudDiscoveryengineV1betaBigQuerySource(Model):
    """NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).

    Do not edit the class manually.
    """

    def __init__(self, data_schema: str=None, dataset_id: str=None, gcs_staging_dir: str=None, partition_date: GoogleTypeDate=None, project_id: str=None, table_id: str=None):
        """GoogleCloudDiscoveryengineV1betaBigQuerySource - a model defined in OpenAPI

        :param data_schema: The data_schema of this GoogleCloudDiscoveryengineV1betaBigQuerySource.
        :param dataset_id: The dataset_id of this GoogleCloudDiscoveryengineV1betaBigQuerySource.
        :param gcs_staging_dir: The gcs_staging_dir of this GoogleCloudDiscoveryengineV1betaBigQuerySource.
        :param partition_date: The partition_date of this GoogleCloudDiscoveryengineV1betaBigQuerySource.
        :param project_id: The project_id of this GoogleCloudDiscoveryengineV1betaBigQuerySource.
        :param table_id: The table_id of this GoogleCloudDiscoveryengineV1betaBigQuerySource.
        """
        self.openapi_types = {
            'data_schema': str,
            'dataset_id': str,
            'gcs_staging_dir': str,
            'partition_date': GoogleTypeDate,
            'project_id': str,
            'table_id': str
        }

        self.attribute_map = {
            'data_schema': 'dataSchema',
            'dataset_id': 'datasetId',
            'gcs_staging_dir': 'gcsStagingDir',
            'partition_date': 'partitionDate',
            'project_id': 'projectId',
            'table_id': 'tableId'
        }

        self._data_schema = data_schema
        self._dataset_id = dataset_id
        self._gcs_staging_dir = gcs_staging_dir
        self._partition_date = partition_date
        self._project_id = project_id
        self._table_id = table_id

    @classmethod
    def from_dict(cls, dikt: dict) -> 'GoogleCloudDiscoveryengineV1betaBigQuerySource':
        """Returns the dict as a model

        :param dikt: A dict.
        :return: The GoogleCloudDiscoveryengineV1betaBigQuerySource of this GoogleCloudDiscoveryengineV1betaBigQuerySource.
        """
        return util.deserialize_model(dikt, cls)

    @property
    def data_schema(self):
        """Gets the data_schema of this GoogleCloudDiscoveryengineV1betaBigQuerySource.

        The schema to use when parsing the data from the source. Supported values for user event imports: * `user_event` (default): One UserEvent per row. Supported values for document imports: * `document` (default): One Document format per row. Each document must have a valid Document.id and one of Document.json_data or Document.struct_data. * `custom`: One custom data per row in arbitrary format that conforms to the defined Schema of the data store. This can only be used by Gen App Builder.

        :return: The data_schema of this GoogleCloudDiscoveryengineV1betaBigQuerySource.
        :rtype: str
        """
        return self._data_schema

    @data_schema.setter
    def data_schema(self, data_schema):
        """Sets the data_schema of this GoogleCloudDiscoveryengineV1betaBigQuerySource.

        The schema to use when parsing the data from the source. Supported values for user event imports: * `user_event` (default): One UserEvent per row. Supported values for document imports: * `document` (default): One Document format per row. Each document must have a valid Document.id and one of Document.json_data or Document.struct_data. * `custom`: One custom data per row in arbitrary format that conforms to the defined Schema of the data store. This can only be used by Gen App Builder.

        :param data_schema: The data_schema of this GoogleCloudDiscoveryengineV1betaBigQuerySource.
        :type data_schema: str
        """

        self._data_schema = data_schema

    @property
    def dataset_id(self):
        """Gets the dataset_id of this GoogleCloudDiscoveryengineV1betaBigQuerySource.

        Required. The BigQuery data set to copy the data from with a length limit of 1,024 characters.

        :return: The dataset_id of this GoogleCloudDiscoveryengineV1betaBigQuerySource.
        :rtype: str
        """
        return self._dataset_id

    @dataset_id.setter
    def dataset_id(self, dataset_id):
        """Sets the dataset_id of this GoogleCloudDiscoveryengineV1betaBigQuerySource.

        Required. The BigQuery data set to copy the data from with a length limit of 1,024 characters.

        :param dataset_id: The dataset_id of this GoogleCloudDiscoveryengineV1betaBigQuerySource.
        :type dataset_id: str
        """

        self._dataset_id = dataset_id

    @property
    def gcs_staging_dir(self):
        """Gets the gcs_staging_dir of this GoogleCloudDiscoveryengineV1betaBigQuerySource.

        Intermediate Cloud Storage directory used for the import with a length limit of 2,000 characters. Can be specified if one wants to have the BigQuery export to a specific Cloud Storage directory.

        :return: The gcs_staging_dir of this GoogleCloudDiscoveryengineV1betaBigQuerySource.
        :rtype: str
        """
        return self._gcs_staging_dir

    @gcs_staging_dir.setter
    def gcs_staging_dir(self, gcs_staging_dir):
        """Sets the gcs_staging_dir of this GoogleCloudDiscoveryengineV1betaBigQuerySource.

        Intermediate Cloud Storage directory used for the import with a length limit of 2,000 characters. Can be specified if one wants to have the BigQuery export to a specific Cloud Storage directory.

        :param gcs_staging_dir: The gcs_staging_dir of this GoogleCloudDiscoveryengineV1betaBigQuerySource.
        :type gcs_staging_dir: str
        """

        self._gcs_staging_dir = gcs_staging_dir

    @property
    def partition_date(self):
        """Gets the partition_date of this GoogleCloudDiscoveryengineV1betaBigQuerySource.


        :return: The partition_date of this GoogleCloudDiscoveryengineV1betaBigQuerySource.
        :rtype: GoogleTypeDate
        """
        return self._partition_date

    @partition_date.setter
    def partition_date(self, partition_date):
        """Sets the partition_date of this GoogleCloudDiscoveryengineV1betaBigQuerySource.


        :param partition_date: The partition_date of this GoogleCloudDiscoveryengineV1betaBigQuerySource.
        :type partition_date: GoogleTypeDate
        """

        self._partition_date = partition_date

    @property
    def project_id(self):
        """Gets the project_id of this GoogleCloudDiscoveryengineV1betaBigQuerySource.

        The project ID (can be project # or ID) that the BigQuery source is in with a length limit of 128 characters. If not specified, inherits the project ID from the parent request.

        :return: The project_id of this GoogleCloudDiscoveryengineV1betaBigQuerySource.
        :rtype: str
        """
        return self._project_id

    @project_id.setter
    def project_id(self, project_id):
        """Sets the project_id of this GoogleCloudDiscoveryengineV1betaBigQuerySource.

        The project ID (can be project # or ID) that the BigQuery source is in with a length limit of 128 characters. If not specified, inherits the project ID from the parent request.

        :param project_id: The project_id of this GoogleCloudDiscoveryengineV1betaBigQuerySource.
        :type project_id: str
        """

        self._project_id = project_id

    @property
    def table_id(self):
        """Gets the table_id of this GoogleCloudDiscoveryengineV1betaBigQuerySource.

        Required. The BigQuery table to copy the data from with a length limit of 1,024 characters.

        :return: The table_id of this GoogleCloudDiscoveryengineV1betaBigQuerySource.
        :rtype: str
        """
        return self._table_id

    @table_id.setter
    def table_id(self, table_id):
        """Sets the table_id of this GoogleCloudDiscoveryengineV1betaBigQuerySource.

        Required. The BigQuery table to copy the data from with a length limit of 1,024 characters.

        :param table_id: The table_id of this GoogleCloudDiscoveryengineV1betaBigQuerySource.
        :type table_id: str
        """

        self._table_id = table_id
