/*
 * Discovery Engine API
 * Discovery Engine API.
 *
 * The version of the OpenAPI document: v1alpha
 * 
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */


package org.openapitools.client.model;

import java.util.Objects;
import com.google.gson.TypeAdapter;
import com.google.gson.annotations.JsonAdapter;
import com.google.gson.annotations.SerializedName;
import com.google.gson.stream.JsonReader;
import com.google.gson.stream.JsonWriter;
import java.io.IOException;
import java.util.Arrays;
import org.openapitools.client.model.GoogleTypeDate;

import com.google.gson.Gson;
import com.google.gson.GsonBuilder;
import com.google.gson.JsonArray;
import com.google.gson.JsonDeserializationContext;
import com.google.gson.JsonDeserializer;
import com.google.gson.JsonElement;
import com.google.gson.JsonObject;
import com.google.gson.JsonParseException;
import com.google.gson.TypeAdapterFactory;
import com.google.gson.reflect.TypeToken;
import com.google.gson.TypeAdapter;
import com.google.gson.stream.JsonReader;
import com.google.gson.stream.JsonWriter;
import java.io.IOException;

import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;

import org.openapitools.client.JSON;

/**
 * BigQuery source import data from.
 */
@javax.annotation.Generated(value = "org.openapitools.codegen.languages.JavaClientCodegen", date = "2024-10-12T11:39:39.358631-04:00[America/New_York]", comments = "Generator version: 7.9.0")
public class GoogleCloudDiscoveryengineV1alphaBigQuerySource {
  public static final String SERIALIZED_NAME_DATA_SCHEMA = "dataSchema";
  @SerializedName(SERIALIZED_NAME_DATA_SCHEMA)
  private String dataSchema;

  public static final String SERIALIZED_NAME_DATASET_ID = "datasetId";
  @SerializedName(SERIALIZED_NAME_DATASET_ID)
  private String datasetId;

  public static final String SERIALIZED_NAME_GCS_STAGING_DIR = "gcsStagingDir";
  @SerializedName(SERIALIZED_NAME_GCS_STAGING_DIR)
  private String gcsStagingDir;

  public static final String SERIALIZED_NAME_PARTITION_DATE = "partitionDate";
  @SerializedName(SERIALIZED_NAME_PARTITION_DATE)
  private GoogleTypeDate partitionDate;

  public static final String SERIALIZED_NAME_PROJECT_ID = "projectId";
  @SerializedName(SERIALIZED_NAME_PROJECT_ID)
  private String projectId;

  public static final String SERIALIZED_NAME_TABLE_ID = "tableId";
  @SerializedName(SERIALIZED_NAME_TABLE_ID)
  private String tableId;

  public GoogleCloudDiscoveryengineV1alphaBigQuerySource() {
  }

  public GoogleCloudDiscoveryengineV1alphaBigQuerySource dataSchema(String dataSchema) {
    this.dataSchema = dataSchema;
    return this;
  }

  /**
   * The schema to use when parsing the data from the source. Supported values for user event imports: * &#x60;user_event&#x60; (default): One UserEvent per row. Supported values for document imports: * &#x60;document&#x60; (default): One Document format per row. Each document must have a valid Document.id and one of Document.json_data or Document.struct_data. * &#x60;custom&#x60;: One custom data per row in arbitrary format that conforms to the defined Schema of the data store. This can only be used by Gen App Builder.
   * @return dataSchema
   */
  @javax.annotation.Nullable
  public String getDataSchema() {
    return dataSchema;
  }

  public void setDataSchema(String dataSchema) {
    this.dataSchema = dataSchema;
  }


  public GoogleCloudDiscoveryengineV1alphaBigQuerySource datasetId(String datasetId) {
    this.datasetId = datasetId;
    return this;
  }

  /**
   * Required. The BigQuery data set to copy the data from with a length limit of 1,024 characters.
   * @return datasetId
   */
  @javax.annotation.Nullable
  public String getDatasetId() {
    return datasetId;
  }

  public void setDatasetId(String datasetId) {
    this.datasetId = datasetId;
  }


  public GoogleCloudDiscoveryengineV1alphaBigQuerySource gcsStagingDir(String gcsStagingDir) {
    this.gcsStagingDir = gcsStagingDir;
    return this;
  }

  /**
   * Intermediate Cloud Storage directory used for the import with a length limit of 2,000 characters. Can be specified if one wants to have the BigQuery export to a specific Cloud Storage directory.
   * @return gcsStagingDir
   */
  @javax.annotation.Nullable
  public String getGcsStagingDir() {
    return gcsStagingDir;
  }

  public void setGcsStagingDir(String gcsStagingDir) {
    this.gcsStagingDir = gcsStagingDir;
  }


  public GoogleCloudDiscoveryengineV1alphaBigQuerySource partitionDate(GoogleTypeDate partitionDate) {
    this.partitionDate = partitionDate;
    return this;
  }

  /**
   * Get partitionDate
   * @return partitionDate
   */
  @javax.annotation.Nullable
  public GoogleTypeDate getPartitionDate() {
    return partitionDate;
  }

  public void setPartitionDate(GoogleTypeDate partitionDate) {
    this.partitionDate = partitionDate;
  }


  public GoogleCloudDiscoveryengineV1alphaBigQuerySource projectId(String projectId) {
    this.projectId = projectId;
    return this;
  }

  /**
   * The project ID (can be project # or ID) that the BigQuery source is in with a length limit of 128 characters. If not specified, inherits the project ID from the parent request.
   * @return projectId
   */
  @javax.annotation.Nullable
  public String getProjectId() {
    return projectId;
  }

  public void setProjectId(String projectId) {
    this.projectId = projectId;
  }


  public GoogleCloudDiscoveryengineV1alphaBigQuerySource tableId(String tableId) {
    this.tableId = tableId;
    return this;
  }

  /**
   * Required. The BigQuery table to copy the data from with a length limit of 1,024 characters.
   * @return tableId
   */
  @javax.annotation.Nullable
  public String getTableId() {
    return tableId;
  }

  public void setTableId(String tableId) {
    this.tableId = tableId;
  }



  @Override
  public boolean equals(Object o) {
    if (this == o) {
      return true;
    }
    if (o == null || getClass() != o.getClass()) {
      return false;
    }
    GoogleCloudDiscoveryengineV1alphaBigQuerySource googleCloudDiscoveryengineV1alphaBigQuerySource = (GoogleCloudDiscoveryengineV1alphaBigQuerySource) o;
    return Objects.equals(this.dataSchema, googleCloudDiscoveryengineV1alphaBigQuerySource.dataSchema) &&
        Objects.equals(this.datasetId, googleCloudDiscoveryengineV1alphaBigQuerySource.datasetId) &&
        Objects.equals(this.gcsStagingDir, googleCloudDiscoveryengineV1alphaBigQuerySource.gcsStagingDir) &&
        Objects.equals(this.partitionDate, googleCloudDiscoveryengineV1alphaBigQuerySource.partitionDate) &&
        Objects.equals(this.projectId, googleCloudDiscoveryengineV1alphaBigQuerySource.projectId) &&
        Objects.equals(this.tableId, googleCloudDiscoveryengineV1alphaBigQuerySource.tableId);
  }

  @Override
  public int hashCode() {
    return Objects.hash(dataSchema, datasetId, gcsStagingDir, partitionDate, projectId, tableId);
  }

  @Override
  public String toString() {
    StringBuilder sb = new StringBuilder();
    sb.append("class GoogleCloudDiscoveryengineV1alphaBigQuerySource {\n");
    sb.append("    dataSchema: ").append(toIndentedString(dataSchema)).append("\n");
    sb.append("    datasetId: ").append(toIndentedString(datasetId)).append("\n");
    sb.append("    gcsStagingDir: ").append(toIndentedString(gcsStagingDir)).append("\n");
    sb.append("    partitionDate: ").append(toIndentedString(partitionDate)).append("\n");
    sb.append("    projectId: ").append(toIndentedString(projectId)).append("\n");
    sb.append("    tableId: ").append(toIndentedString(tableId)).append("\n");
    sb.append("}");
    return sb.toString();
  }

  /**
   * Convert the given object to string with each line indented by 4 spaces
   * (except the first line).
   */
  private String toIndentedString(Object o) {
    if (o == null) {
      return "null";
    }
    return o.toString().replace("\n", "\n    ");
  }


  public static HashSet<String> openapiFields;
  public static HashSet<String> openapiRequiredFields;

  static {
    // a set of all properties/fields (JSON key names)
    openapiFields = new HashSet<String>();
    openapiFields.add("dataSchema");
    openapiFields.add("datasetId");
    openapiFields.add("gcsStagingDir");
    openapiFields.add("partitionDate");
    openapiFields.add("projectId");
    openapiFields.add("tableId");

    // a set of required properties/fields (JSON key names)
    openapiRequiredFields = new HashSet<String>();
  }

  /**
   * Validates the JSON Element and throws an exception if issues found
   *
   * @param jsonElement JSON Element
   * @throws IOException if the JSON Element is invalid with respect to GoogleCloudDiscoveryengineV1alphaBigQuerySource
   */
  public static void validateJsonElement(JsonElement jsonElement) throws IOException {
      if (jsonElement == null) {
        if (!GoogleCloudDiscoveryengineV1alphaBigQuerySource.openapiRequiredFields.isEmpty()) { // has required fields but JSON element is null
          throw new IllegalArgumentException(String.format("The required field(s) %s in GoogleCloudDiscoveryengineV1alphaBigQuerySource is not found in the empty JSON string", GoogleCloudDiscoveryengineV1alphaBigQuerySource.openapiRequiredFields.toString()));
        }
      }

      Set<Map.Entry<String, JsonElement>> entries = jsonElement.getAsJsonObject().entrySet();
      // check to see if the JSON string contains additional fields
      for (Map.Entry<String, JsonElement> entry : entries) {
        if (!GoogleCloudDiscoveryengineV1alphaBigQuerySource.openapiFields.contains(entry.getKey())) {
          throw new IllegalArgumentException(String.format("The field `%s` in the JSON string is not defined in the `GoogleCloudDiscoveryengineV1alphaBigQuerySource` properties. JSON: %s", entry.getKey(), jsonElement.toString()));
        }
      }
        JsonObject jsonObj = jsonElement.getAsJsonObject();
      if ((jsonObj.get("dataSchema") != null && !jsonObj.get("dataSchema").isJsonNull()) && !jsonObj.get("dataSchema").isJsonPrimitive()) {
        throw new IllegalArgumentException(String.format("Expected the field `dataSchema` to be a primitive type in the JSON string but got `%s`", jsonObj.get("dataSchema").toString()));
      }
      if ((jsonObj.get("datasetId") != null && !jsonObj.get("datasetId").isJsonNull()) && !jsonObj.get("datasetId").isJsonPrimitive()) {
        throw new IllegalArgumentException(String.format("Expected the field `datasetId` to be a primitive type in the JSON string but got `%s`", jsonObj.get("datasetId").toString()));
      }
      if ((jsonObj.get("gcsStagingDir") != null && !jsonObj.get("gcsStagingDir").isJsonNull()) && !jsonObj.get("gcsStagingDir").isJsonPrimitive()) {
        throw new IllegalArgumentException(String.format("Expected the field `gcsStagingDir` to be a primitive type in the JSON string but got `%s`", jsonObj.get("gcsStagingDir").toString()));
      }
      // validate the optional field `partitionDate`
      if (jsonObj.get("partitionDate") != null && !jsonObj.get("partitionDate").isJsonNull()) {
        GoogleTypeDate.validateJsonElement(jsonObj.get("partitionDate"));
      }
      if ((jsonObj.get("projectId") != null && !jsonObj.get("projectId").isJsonNull()) && !jsonObj.get("projectId").isJsonPrimitive()) {
        throw new IllegalArgumentException(String.format("Expected the field `projectId` to be a primitive type in the JSON string but got `%s`", jsonObj.get("projectId").toString()));
      }
      if ((jsonObj.get("tableId") != null && !jsonObj.get("tableId").isJsonNull()) && !jsonObj.get("tableId").isJsonPrimitive()) {
        throw new IllegalArgumentException(String.format("Expected the field `tableId` to be a primitive type in the JSON string but got `%s`", jsonObj.get("tableId").toString()));
      }
  }

  public static class CustomTypeAdapterFactory implements TypeAdapterFactory {
    @SuppressWarnings("unchecked")
    @Override
    public <T> TypeAdapter<T> create(Gson gson, TypeToken<T> type) {
       if (!GoogleCloudDiscoveryengineV1alphaBigQuerySource.class.isAssignableFrom(type.getRawType())) {
         return null; // this class only serializes 'GoogleCloudDiscoveryengineV1alphaBigQuerySource' and its subtypes
       }
       final TypeAdapter<JsonElement> elementAdapter = gson.getAdapter(JsonElement.class);
       final TypeAdapter<GoogleCloudDiscoveryengineV1alphaBigQuerySource> thisAdapter
                        = gson.getDelegateAdapter(this, TypeToken.get(GoogleCloudDiscoveryengineV1alphaBigQuerySource.class));

       return (TypeAdapter<T>) new TypeAdapter<GoogleCloudDiscoveryengineV1alphaBigQuerySource>() {
           @Override
           public void write(JsonWriter out, GoogleCloudDiscoveryengineV1alphaBigQuerySource value) throws IOException {
             JsonObject obj = thisAdapter.toJsonTree(value).getAsJsonObject();
             elementAdapter.write(out, obj);
           }

           @Override
           public GoogleCloudDiscoveryengineV1alphaBigQuerySource read(JsonReader in) throws IOException {
             JsonElement jsonElement = elementAdapter.read(in);
             validateJsonElement(jsonElement);
             return thisAdapter.fromJsonTree(jsonElement);
           }

       }.nullSafe();
    }
  }

  /**
   * Create an instance of GoogleCloudDiscoveryengineV1alphaBigQuerySource given an JSON string
   *
   * @param jsonString JSON string
   * @return An instance of GoogleCloudDiscoveryengineV1alphaBigQuerySource
   * @throws IOException if the JSON string is invalid with respect to GoogleCloudDiscoveryengineV1alphaBigQuerySource
   */
  public static GoogleCloudDiscoveryengineV1alphaBigQuerySource fromJson(String jsonString) throws IOException {
    return JSON.getGson().fromJson(jsonString, GoogleCloudDiscoveryengineV1alphaBigQuerySource.class);
  }

  /**
   * Convert an instance of GoogleCloudDiscoveryengineV1alphaBigQuerySource to an JSON string
   *
   * @return JSON string
   */
  public String toJson() {
    return JSON.getGson().toJson(this);
  }
}

