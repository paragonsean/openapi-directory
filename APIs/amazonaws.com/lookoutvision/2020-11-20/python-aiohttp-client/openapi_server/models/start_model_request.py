# coding: utf-8

from datetime import date, datetime

from typing import List, Dict, Type

from openapi_server.models.base_model import Model
from openapi_server import util


class StartModelRequest(Model):
    """NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).

    Do not edit the class manually.
    """

    def __init__(self, min_inference_units: int=None, max_inference_units: int=None):
        """StartModelRequest - a model defined in OpenAPI

        :param min_inference_units: The min_inference_units of this StartModelRequest.
        :param max_inference_units: The max_inference_units of this StartModelRequest.
        """
        self.openapi_types = {
            'min_inference_units': int,
            'max_inference_units': int
        }

        self.attribute_map = {
            'min_inference_units': 'MinInferenceUnits',
            'max_inference_units': 'MaxInferenceUnits'
        }

        self._min_inference_units = min_inference_units
        self._max_inference_units = max_inference_units

    @classmethod
    def from_dict(cls, dikt: dict) -> 'StartModelRequest':
        """Returns the dict as a model

        :param dikt: A dict.
        :return: The StartModel_request of this StartModelRequest.
        """
        return util.deserialize_model(dikt, cls)

    @property
    def min_inference_units(self):
        """Gets the min_inference_units of this StartModelRequest.

        The minimum number of inference units to use. A single inference unit represents 1 hour of processing. Use a higher number to increase the TPS throughput of your model. You are charged for the number of inference units that you use. 

        :return: The min_inference_units of this StartModelRequest.
        :rtype: int
        """
        return self._min_inference_units

    @min_inference_units.setter
    def min_inference_units(self, min_inference_units):
        """Sets the min_inference_units of this StartModelRequest.

        The minimum number of inference units to use. A single inference unit represents 1 hour of processing. Use a higher number to increase the TPS throughput of your model. You are charged for the number of inference units that you use. 

        :param min_inference_units: The min_inference_units of this StartModelRequest.
        :type min_inference_units: int
        """
        if min_inference_units is None:
            raise ValueError("Invalid value for `min_inference_units`, must not be `None`")
        if min_inference_units is not None and min_inference_units < 1:
            raise ValueError("Invalid value for `min_inference_units`, must be a value greater than or equal to `1`")

        self._min_inference_units = min_inference_units

    @property
    def max_inference_units(self):
        """Gets the max_inference_units of this StartModelRequest.

        The maximum number of inference units to use for auto-scaling the model. If you don't specify a value, Amazon Lookout for Vision doesn't auto-scale the model.

        :return: The max_inference_units of this StartModelRequest.
        :rtype: int
        """
        return self._max_inference_units

    @max_inference_units.setter
    def max_inference_units(self, max_inference_units):
        """Sets the max_inference_units of this StartModelRequest.

        The maximum number of inference units to use for auto-scaling the model. If you don't specify a value, Amazon Lookout for Vision doesn't auto-scale the model.

        :param max_inference_units: The max_inference_units of this StartModelRequest.
        :type max_inference_units: int
        """
        if max_inference_units is not None and max_inference_units < 1:
            raise ValueError("Invalid value for `max_inference_units`, must be a value greater than or equal to `1`")

        self._max_inference_units = max_inference_units
