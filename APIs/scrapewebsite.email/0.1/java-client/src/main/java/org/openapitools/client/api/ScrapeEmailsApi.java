/*
 * Scrape Website Email API
 * ScrapeWebsiteEmail is a service that exposes an api to fetch e-mails from a website.
 *
 * The version of the OpenAPI document: 0.1
 * 
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */


package org.openapitools.client.api;

import org.openapitools.client.ApiCallback;
import org.openapitools.client.ApiClient;
import org.openapitools.client.ApiException;
import org.openapitools.client.ApiResponse;
import org.openapitools.client.Configuration;
import org.openapitools.client.Pair;
import org.openapitools.client.ProgressRequestBody;
import org.openapitools.client.ProgressResponseBody;

import com.google.gson.reflect.TypeToken;

import java.io.IOException;



import java.lang.reflect.Type;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

public class ScrapeEmailsApi {
    private ApiClient localVarApiClient;
    private int localHostIndex;
    private String localCustomBaseUrl;

    public ScrapeEmailsApi() {
        this(Configuration.getDefaultApiClient());
    }

    public ScrapeEmailsApi(ApiClient apiClient) {
        this.localVarApiClient = apiClient;
    }

    public ApiClient getApiClient() {
        return localVarApiClient;
    }

    public void setApiClient(ApiClient apiClient) {
        this.localVarApiClient = apiClient;
    }

    public int getHostIndex() {
        return localHostIndex;
    }

    public void setHostIndex(int hostIndex) {
        this.localHostIndex = hostIndex;
    }

    public String getCustomBaseUrl() {
        return localCustomBaseUrl;
    }

    public void setCustomBaseUrl(String customBaseUrl) {
        this.localCustomBaseUrl = customBaseUrl;
    }

    /**
     * Build call for gETV1ScrapeEmailsFormat
     * @param website &lt;p&gt;The website (ie. www.soundflair.com)&lt;/p&gt;  (required)
     * @param mustInclude &lt;table&gt;   &lt;tbody&gt;     &lt;tr&gt;       &lt;td&gt;Optional. The word(s) that the webpage must include (otherwise it will skip scraping that page). Good if you want to scrape only contact pages. Takes regex (ie. about&lt;/td&gt;       &lt;td&gt;contact).&lt;/td&gt;     &lt;/tr&gt;   &lt;/tbody&gt; &lt;/table&gt;  (optional)
     * @param _callback Callback for upload/download progress
     * @return Call to execute
     * @throws ApiException If fail to serialize the request body object
     * @http.response.details
     <table summary="Response Details" border="1">
        <tr><td> Status Code </td><td> Description </td><td> Response Headers </td></tr>
        <tr><td> 200 </td><td> No response was specified </td><td>  -  </td></tr>
     </table>
     */
    public okhttp3.Call gETV1ScrapeEmailsFormatCall(String website, String mustInclude, final ApiCallback _callback) throws ApiException {
        String basePath = null;
        // Operation Servers
        String[] localBasePaths = new String[] {  };

        // Determine Base Path to Use
        if (localCustomBaseUrl != null){
            basePath = localCustomBaseUrl;
        } else if ( localBasePaths.length > 0 ) {
            basePath = localBasePaths[localHostIndex];
        } else {
            basePath = null;
        }

        Object localVarPostBody = null;

        // create path and map variables
        String localVarPath = "/v1/scrape_emails.json";

        List<Pair> localVarQueryParams = new ArrayList<Pair>();
        List<Pair> localVarCollectionQueryParams = new ArrayList<Pair>();
        Map<String, String> localVarHeaderParams = new HashMap<String, String>();
        Map<String, String> localVarCookieParams = new HashMap<String, String>();
        Map<String, Object> localVarFormParams = new HashMap<String, Object>();

        if (website != null) {
            localVarQueryParams.addAll(localVarApiClient.parameterToPair("website", website));
        }

        if (mustInclude != null) {
            localVarQueryParams.addAll(localVarApiClient.parameterToPair("must_include", mustInclude));
        }

        final String[] localVarAccepts = {
        };
        final String localVarAccept = localVarApiClient.selectHeaderAccept(localVarAccepts);
        if (localVarAccept != null) {
            localVarHeaderParams.put("Accept", localVarAccept);
        }

        final String[] localVarContentTypes = {
        };
        final String localVarContentType = localVarApiClient.selectHeaderContentType(localVarContentTypes);
        if (localVarContentType != null) {
            localVarHeaderParams.put("Content-Type", localVarContentType);
        }

        String[] localVarAuthNames = new String[] {  };
        return localVarApiClient.buildCall(basePath, localVarPath, "GET", localVarQueryParams, localVarCollectionQueryParams, localVarPostBody, localVarHeaderParams, localVarCookieParams, localVarFormParams, localVarAuthNames, _callback);
    }

    @SuppressWarnings("rawtypes")
    private okhttp3.Call gETV1ScrapeEmailsFormatValidateBeforeCall(String website, String mustInclude, final ApiCallback _callback) throws ApiException {
        // verify the required parameter 'website' is set
        if (website == null) {
            throw new ApiException("Missing the required parameter 'website' when calling gETV1ScrapeEmailsFormat(Async)");
        }

        return gETV1ScrapeEmailsFormatCall(website, mustInclude, _callback);

    }

    /**
     * Returns a list of emails scraped by priority (ie. e-mails appear on top level pages are first). Please note that subsequent calls to the same url will be fetched from the &lt;b&gt;cache&lt;/b&gt; (14 day flush). &lt;br/&gt;&lt;br/&gt;Will also parse patterns such as hello[at]site.com, hello[at]site[dot]com, hello(at)site.com, hello(at)site(dot)com, hello @ site.com, hello @ site . com. &lt;br/&gt;&lt;br/&gt;Please do note we cannot parse sites that require a login (for now), so for some Facebook pages it is not possible at the moment to fetch the e-mail.&lt;br/&gt;&lt;br/&gt;Finally, please note that the api will fetch links for up to 2 minutes. After that time it will start analysing the pages which have been scraped. &lt;b&gt;Therefore&lt;/b&gt; please ensure that your client has a timeout of at least &lt;b&gt;150 seconds&lt;/b&gt; (2 mins to fetch and the rest to parse). &lt;br/&gt;&lt;br/&gt;&lt;b&gt;Please note&lt;/b&gt; that due to the fact that the api goes out to fetch the pages, the server allows only 1 concurrent request/ip. Requests which are made while the 1 request is still processing will result in a 429 code.&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Please note&lt;/b&gt; that as of May 25, 2014, the main mechanism of tracking usage will be done via Mashape. You can get the free calls by signing up with the FREE plan.&lt;br/&gt;&lt;br/&gt;Please visit &lt;a href&#x3D;&#39;https://www.mashape.com/tommytcchan/scrape-website-email&#39;&gt;https://www.mashape.com/tommytcchan/scrape-website-email&lt;/a&gt;.&lt;br/&gt;&lt;br/&gt;&lt;b&gt;There is now a limit of 5 requests per day using this sample interface.&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;
     * 
     * @param website &lt;p&gt;The website (ie. www.soundflair.com)&lt;/p&gt;  (required)
     * @param mustInclude &lt;table&gt;   &lt;tbody&gt;     &lt;tr&gt;       &lt;td&gt;Optional. The word(s) that the webpage must include (otherwise it will skip scraping that page). Good if you want to scrape only contact pages. Takes regex (ie. about&lt;/td&gt;       &lt;td&gt;contact).&lt;/td&gt;     &lt;/tr&gt;   &lt;/tbody&gt; &lt;/table&gt;  (optional)
     * @throws ApiException If fail to call the API, e.g. server error or cannot deserialize the response body
     * @http.response.details
     <table summary="Response Details" border="1">
        <tr><td> Status Code </td><td> Description </td><td> Response Headers </td></tr>
        <tr><td> 200 </td><td> No response was specified </td><td>  -  </td></tr>
     </table>
     */
    public void gETV1ScrapeEmailsFormat(String website, String mustInclude) throws ApiException {
        gETV1ScrapeEmailsFormatWithHttpInfo(website, mustInclude);
    }

    /**
     * Returns a list of emails scraped by priority (ie. e-mails appear on top level pages are first). Please note that subsequent calls to the same url will be fetched from the &lt;b&gt;cache&lt;/b&gt; (14 day flush). &lt;br/&gt;&lt;br/&gt;Will also parse patterns such as hello[at]site.com, hello[at]site[dot]com, hello(at)site.com, hello(at)site(dot)com, hello @ site.com, hello @ site . com. &lt;br/&gt;&lt;br/&gt;Please do note we cannot parse sites that require a login (for now), so for some Facebook pages it is not possible at the moment to fetch the e-mail.&lt;br/&gt;&lt;br/&gt;Finally, please note that the api will fetch links for up to 2 minutes. After that time it will start analysing the pages which have been scraped. &lt;b&gt;Therefore&lt;/b&gt; please ensure that your client has a timeout of at least &lt;b&gt;150 seconds&lt;/b&gt; (2 mins to fetch and the rest to parse). &lt;br/&gt;&lt;br/&gt;&lt;b&gt;Please note&lt;/b&gt; that due to the fact that the api goes out to fetch the pages, the server allows only 1 concurrent request/ip. Requests which are made while the 1 request is still processing will result in a 429 code.&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Please note&lt;/b&gt; that as of May 25, 2014, the main mechanism of tracking usage will be done via Mashape. You can get the free calls by signing up with the FREE plan.&lt;br/&gt;&lt;br/&gt;Please visit &lt;a href&#x3D;&#39;https://www.mashape.com/tommytcchan/scrape-website-email&#39;&gt;https://www.mashape.com/tommytcchan/scrape-website-email&lt;/a&gt;.&lt;br/&gt;&lt;br/&gt;&lt;b&gt;There is now a limit of 5 requests per day using this sample interface.&lt;/b&gt;&lt;br/&gt;&lt;br/&gt;
     * 
     * @param website &lt;p&gt;The website (ie. www.soundflair.com)&lt;/p&gt;  (required)
     * @param mustInclude &lt;table&gt;   &lt;tbody&gt;     &lt;tr&gt;       &lt;td&gt;Optional. The word(s) that the webpage must include (otherwise it will skip scraping that page). Good if you want to scrape only contact pages. Takes regex (ie. about&lt;/td&gt;       &lt;td&gt;contact).&lt;/td&gt;     &lt;/tr&gt;   &lt;/tbody&gt; &lt;/table&gt;  (optional)
     * @return ApiResponse&lt;Void&gt;
     * @throws ApiException If fail to call the API, e.g. server error or cannot deserialize the response body
     * @http.response.details
     <table summary="Response Details" border="1">
        <tr><td> Status Code </td><td> Description </td><td> Response Headers </td></tr>
        <tr><td> 200 </td><td> No response was specified </td><td>  -  </td></tr>
     </table>
     */
    public ApiResponse<Void> gETV1ScrapeEmailsFormatWithHttpInfo(String website, String mustInclude) throws ApiException {
        okhttp3.Call localVarCall = gETV1ScrapeEmailsFormatValidateBeforeCall(website, mustInclude, null);
        return localVarApiClient.execute(localVarCall);
    }

    /**
     * Returns a list of emails scraped by priority (ie. e-mails appear on top level pages are first). Please note that subsequent calls to the same url will be fetched from the &lt;b&gt;cache&lt;/b&gt; (14 day flush). &lt;br/&gt;&lt;br/&gt;Will also parse patterns such as hello[at]site.com, hello[at]site[dot]com, hello(at)site.com, hello(at)site(dot)com, hello @ site.com, hello @ site . com. &lt;br/&gt;&lt;br/&gt;Please do note we cannot parse sites that require a login (for now), so for some Facebook pages it is not possible at the moment to fetch the e-mail.&lt;br/&gt;&lt;br/&gt;Finally, please note that the api will fetch links for up to 2 minutes. After that time it will start analysing the pages which have been scraped. &lt;b&gt;Therefore&lt;/b&gt; please ensure that your client has a timeout of at least &lt;b&gt;150 seconds&lt;/b&gt; (2 mins to fetch and the rest to parse). &lt;br/&gt;&lt;br/&gt;&lt;b&gt;Please note&lt;/b&gt; that due to the fact that the api goes out to fetch the pages, the server allows only 1 concurrent request/ip. Requests which are made while the 1 request is still processing will result in a 429 code.&lt;br/&gt;&lt;br/&gt;&lt;b&gt;Please note&lt;/b&gt; that as of May 25, 2014, the main mechanism of tracking usage will be done via Mashape. You can get the free calls by signing up with the FREE plan.&lt;br/&gt;&lt;br/&gt;Please visit &lt;a href&#x3D;&#39;https://www.mashape.com/tommytcchan/scrape-website-email&#39;&gt;https://www.mashape.com/tommytcchan/scrape-website-email&lt;/a&gt;.&lt;br/&gt;&lt;br/&gt;&lt;b&gt;There is now a limit of 5 requests per day using this sample interface.&lt;/b&gt;&lt;br/&gt;&lt;br/&gt; (asynchronously)
     * 
     * @param website &lt;p&gt;The website (ie. www.soundflair.com)&lt;/p&gt;  (required)
     * @param mustInclude &lt;table&gt;   &lt;tbody&gt;     &lt;tr&gt;       &lt;td&gt;Optional. The word(s) that the webpage must include (otherwise it will skip scraping that page). Good if you want to scrape only contact pages. Takes regex (ie. about&lt;/td&gt;       &lt;td&gt;contact).&lt;/td&gt;     &lt;/tr&gt;   &lt;/tbody&gt; &lt;/table&gt;  (optional)
     * @param _callback The callback to be executed when the API call finishes
     * @return The request call
     * @throws ApiException If fail to process the API call, e.g. serializing the request body object
     * @http.response.details
     <table summary="Response Details" border="1">
        <tr><td> Status Code </td><td> Description </td><td> Response Headers </td></tr>
        <tr><td> 200 </td><td> No response was specified </td><td>  -  </td></tr>
     </table>
     */
    public okhttp3.Call gETV1ScrapeEmailsFormatAsync(String website, String mustInclude, final ApiCallback<Void> _callback) throws ApiException {

        okhttp3.Call localVarCall = gETV1ScrapeEmailsFormatValidateBeforeCall(website, mustInclude, _callback);
        localVarApiClient.executeAsync(localVarCall, _callback);
        return localVarCall;
    }
}
